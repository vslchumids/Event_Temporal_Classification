{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Term Project: Event Temporal State Identification\n",
    "\n",
    "## CNN Model\n",
    "\n",
    "### John Chiang, Vincent Chu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Custom library for CNN model\n",
    "from nlp_cnn import NLPCNN\n",
    "\n",
    "# Custom library for data processing\n",
    "import societal_data_processor as sdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Data Loading, CNN Model Ops and CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=50\n",
      "CHECKPOINT_EVERY=500\n",
      "DATA_DIR=/home/vslchu/w266/project/data/eventstatus_eng/\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=1\n",
      "EMBEDDING_DIM=50\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=5\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================================\n",
    "# Parameters\n",
    "#\n",
    "# Adopted and modified from train.py of Danny Britz's cnn-text-classification-tf Github page\n",
    "# <https://github.com/dennybritz/cnn-text-classification-tf>\n",
    "#===========================================================================================================\n",
    "\n",
    "try:\n",
    "    # Data loading params\n",
    "    tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "    tf.flags.DEFINE_string(\"data_dir\", '/home/vslchu/w266/project/data/eventstatus_eng/', \"Directory for Annotated Societal Events Data\")\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    tf.flags.DEFINE_integer(\"embedding_dim\", 50, \"Dimensionality of character embedding\") \n",
    "    tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes\")\n",
    "    tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size\") \n",
    "    tf.flags.DEFINE_float(\"dropout_keep_prob\", 1, \"Dropout keep probability\") \n",
    "    tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda\")\n",
    "\n",
    "    # Training parameters\n",
    "    tf.flags.DEFINE_integer(\"batch_size\", 50, \"Batch Size\")\n",
    "    tf.flags.DEFINE_integer(\"num_epochs\", 5, \"Number of training epochs\")\n",
    "    tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps\")\n",
    "    tf.flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps\")\n",
    "    tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store\")\n",
    "    \n",
    "    # Misc Parameters\n",
    "    tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "    tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "except:\n",
    "    print \"Tf Flags already defined\"\n",
    "\n",
    "# Inspecting the parameters\n",
    "\n",
    "params = tf.flags.FLAGS\n",
    "params._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(params.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#===========================================================================================================\n",
    "# Functions\n",
    "#\n",
    "# Adopted and modified from train.py of Danny Britz's cnn-text-classification-tf Github page\n",
    "# <https://github.com/dennybritz/cnn-text-classification-tf>\n",
    "#===========================================================================================================\n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: load_text_data\n",
    "# Description  : Use functions from the societal_data_processor library to load and prepare annotated \n",
    "#                data from the EventStatus corpus. The annotations (i.e., the y's) are transformed to \n",
    "#                lists of binaries.\n",
    "# Parameters         :\n",
    "#   data_dir         : path of the directory wih the annotated files\n",
    "#   processor_ver    : version of the data processor to use: \n",
    "#                      1 - Chunk to multiple annotations\n",
    "#                      2 - Accumulated phrase to single annotation\n",
    "#                      3 - Phrase to single annotation\n",
    "#   remove_stopwords : Whether to remove the stopwords\n",
    "#   replace_num      : Whether to replace numbers with <NUM>\n",
    "#   remove_non_alpha : Whether to remove non-alphbetical strings\n",
    "#   to_lower         : Whether to convert all words to lowercase\n",
    "#   to_subwords      : Whether to futher break down the words into subwords\n",
    "# Return Values      :\n",
    "#   x_train          : Training data set with word chunks or phrases\n",
    "#   x_test           : Test data set with word chunks or phrases\n",
    "#   y_train          : List of binaries representing annotations (i.e., labels) corresponding to each \n",
    "#                      item in the training data set\n",
    "#   y_test           : List of binaries representing annotations (i.e., labels) corresponding to each \n",
    "#                      item in the test data set\n",
    "#   y_orig_train     : Original annotations (i.e., labels as strings) corresponding to each \n",
    "#                      item in the training data set\n",
    "#   y_orig_test      : Original annotations (i.e., labels as strings) corresponding to each \n",
    "#                      item in the test data set\n",
    "#   vocab_processor  : Vocabulary processor that maps the words chunks or phrases to a list of IDs \n",
    "############################################################################################################\n",
    "def load_text_data(data_dir, \n",
    "                   processor_ver, \n",
    "                   remove_stopwords = False, \n",
    "                   replace_num = False, \n",
    "                   remove_non_alpha = False, \n",
    "                   to_lower = False, \n",
    "                   to_subwords = False):\n",
    "    \n",
    "    # Load data from the annotated files\n",
    "    print(\"Loading data...\")\n",
    "    (original_chunks, clean_chunks, clean_chunk_sents, temporal_states, event_files) = \\\n",
    "    sdp.get_chunks_n_annotations(data_dir, \n",
    "                                 processor_ver, \n",
    "                                 remove_stopwords, \n",
    "                                 replace_num, \n",
    "                                 remove_non_alpha, \n",
    "                                 to_lower, \n",
    "                                 to_subwords)\n",
    "\n",
    "    # Tranform annotations into lists of binaries\n",
    "    y = sdp.transform_annotations_to_binary(temporal_states)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_chunk_length = max([len(x.split(\" \")) for x in clean_chunks])\n",
    "    print \"max_chunk_length = \", max_chunk_length\n",
    "\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_chunk_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(clean_chunks)))\n",
    "\n",
    "    (x_train, x_test) = sdp.split_train_test_data(x, params.dev_sample_percentage)\n",
    "    (y_train, y_test) = sdp.split_train_test_data(y, params.dev_sample_percentage)\n",
    "    (y_orig_train, y_orig_test) = sdp.split_train_test_data(temporal_states, params.dev_sample_percentage)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split on data (x): {:d}/{:d}\".format(len(x_train), len(x_test)))\n",
    "    print(\"Train/Dev split on labels (y): {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return (x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor)\n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: train_step\n",
    "# Description  : Implement a single training step on a batch from the training data set\n",
    "# Parameters   :\n",
    "#   sess       : Tensorflow session\n",
    "#   cnn        : Object of class NLPCNN, which has all training and test ops defined \n",
    "#   x_batch    : A batch of word chunks or phrases from the training data set\n",
    "#   y_batch    : List of binaries representing annotations (i.e., labals) corresponding to the x_batch \n",
    "#                from the training data set\n",
    "# Return Values:\n",
    "#   loss       : Mean cross-entropy loss\n",
    "#   accuracy   : Accuracy of predictions\n",
    "#   predictions: Predicted annotations (i.e., labels)\n",
    "############################################################################################################\n",
    "def train_step(sess, cnn, x_batch, y_batch):\n",
    "\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: params.dropout_keep_prob\n",
    "    }\n",
    "\n",
    "    _, step, loss, accuracy, predictions = \\\n",
    "        sess.run([cnn.train_op, \n",
    "                  cnn.global_step, \n",
    "                  cnn.loss, \n",
    "                  cnn.accuracy, \n",
    "                  cnn.predictions],\n",
    "                 feed_dict)\n",
    "        \n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    return (loss, accuracy, predictions)\n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: test_step\n",
    "# Description  : Evaluate the trained CNN model on the test data set\n",
    "# Parameters   :\n",
    "#   sess       : Tensorflow session\n",
    "#   cnn        : Object of class NLPCNN, which has all training and test ops defined \n",
    "#   x_batch    : A batch of word chunks or phrases from the test data set\n",
    "#   y_batch    : List of binaries representing annotations (i.e., labals) corresponding to the x_batch \n",
    "#                from the test data set\n",
    "# Return Values:\n",
    "#   loss       : Mean cross-entropy loss\n",
    "#   accuracy   : Accuracy of predictions\n",
    "#   predictions: Predicted annotations (i.e., labels)\n",
    "############################################################################################################\n",
    "def test_step(sess, cnn, x_batch, y_batch):\n",
    "\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    \n",
    "\n",
    "    step, loss, accuracy, predictions = \\\n",
    "        sess.run([cnn.global_step, \n",
    "                  cnn.loss, \n",
    "                  cnn.accuracy, \n",
    "                  cnn.predictions],\n",
    "                 feed_dict)\n",
    "        \n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    \n",
    "    return (loss, accuracy, predictions) \n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: run_cnn\n",
    "# Description  : Instantiate and run a CNN model, train the model on the training data set and evaluate \n",
    "#                its performance using the test data set\n",
    "# Parameters       :\n",
    "#   x_train        : Training data set with word chunks or phrases\n",
    "#   y_train        : Annotations (i.e., labels) corresponding to each item in the training data set\n",
    "#   x_test         : Test data set with word chunks or phrases\n",
    "#   y_test         : Annotations (i.e., labels) corresponding to each item in the test data set\n",
    "#   vocab_processor: Vocabulary processor that maps the words chunks or phrases to a list of IDs \n",
    "# Return Values    :\n",
    "#   test_preds     : List of lists of predicted labels from each evaluation (i.e., testing) step\n",
    "############################################################################################################\n",
    "def run_cnn(x_train, y_train, x_test, y_test, vocab_processor): \n",
    "    \n",
    "    train_preds = []\n",
    "    test_preds = []\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=params.allow_soft_placement,\n",
    "          log_device_placement=params.log_device_placement)\n",
    "\n",
    "        sess = tf.Session(config=session_conf)\n",
    "\n",
    "        with sess.as_default():\n",
    "            cnn = NLPCNN(sequence_length = x_train.shape[1],\n",
    "                         num_classes = y_train.shape[1],\n",
    "                         vocab_size = len(vocab_processor.vocabulary_),\n",
    "                         embedding_size = params.embedding_dim,\n",
    "                         filter_sizes = list(map(int, params.filter_sizes.split(\",\"))),\n",
    "                         num_filters = params.num_filters,\n",
    "                         l2_reg_lambda = params.l2_reg_lambda)\n",
    "            \n",
    "            cnn.build_core_graph()\n",
    "            cnn.build_train_test_graph()\n",
    "            \n",
    "            print \"cnn.out_dir = \", cnn.out_dir\n",
    "            \n",
    "            checkpoint_dir = os.path.abspath(os.path.join(cnn.out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep = cnn.num_checkpoints)            \n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())                     \n",
    "\n",
    "            # Generate batches\n",
    "            batches = sdp.batch_iter(\n",
    "                list(zip(x_train, y_train)), params.batch_size, params.num_epochs)\n",
    "\n",
    "            batch_count = 0\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                loss, accuracy, predictions = train_step(sess, cnn, x_batch, y_batch)\n",
    "\n",
    "                current_step = tf.train.global_step(sess, cnn.global_step)                \n",
    "                print \"current_step: \", current_step\n",
    "\n",
    "                for i in range(len(predictions)):\n",
    "                    train_preds.append(predictions[i])\n",
    "\n",
    "                if current_step % params.evaluate_every == 0:\n",
    "                    print \"\\nPredicting annotation for test data:\"\n",
    "                    loss, accuracy, predictions = test_step(sess, cnn, x_test, y_test)\n",
    "                    test_preds.append(list(predictions))\n",
    "                    print\n",
    "\n",
    "                if current_step % params.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step = current_step)\n",
    "                    print \"Saved model checkpoint to {}\\n\".format(path)\n",
    "\n",
    "                batch_count += 1\n",
    "                \n",
    "            print \"\\nRan %d batches during training and created %d rounds of predictions\" % (batch_count, len(test_preds))\n",
    "\n",
    "    return test_preds\n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: eval_preds\n",
    "# Description  : Evaluate predictions from the test steps against the real annotations\n",
    "# Parameters       :\n",
    "#   test_preds_list: list of predictions from various evaluation checkpoint during the training cycle\n",
    "#   test_labels    : Labels (annotations) for the test data set\n",
    "# Return Values    :\n",
    "#   test_pred_eval : List of tuples of metrics (i.e., F1 Score, Precision and Recall) from each evaluation \n",
    "#                    (i.e., testing) step\n",
    "############################################################################################################\n",
    "def eval_preds(test_preds_list, test_labels):\n",
    "\n",
    "    #reload(sdp)\n",
    "    test_pred_annotations = []\n",
    "    test_pred_eval = []\n",
    "    \n",
    "    for i in range(len(test_preds_list)):\n",
    "        temp_test_pred_annotations = sdp.transform_digits_to_annotations(test_preds_list[i])\n",
    "        test_pred_annotations.append(temp_test_pred_annotations)\n",
    "                \n",
    "        ### Evaluate Performance of model        \n",
    "        f1 = f1_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        precision = precision_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        recall = recall_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        \n",
    "        print \"\\nPerformance Evaluation of CNN Model (i = %d):\" % i\n",
    "        print \"F1 Score = %f\" % f1\n",
    "        print \"Precision Score = %f\" % precision\n",
    "        print \"Recall Score = %f\" % recall \n",
    "\n",
    "        test_pred_eval.append((f1, precision, recall))\n",
    "        \n",
    "    return test_pred_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  643\n",
      "Vocabulary Size: 8522\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0303_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0303_UTC\n",
      "2017-08-23T03:03:23.909296: step 1, loss 4.29361, acc 0.02\n",
      "current_step:  1\n",
      "2017-08-23T03:03:24.088741: step 2, loss 3.21933, acc 0.02\n",
      "current_step:  2\n",
      "2017-08-23T03:03:24.270868: step 3, loss 2.18547, acc 0.3\n",
      "current_step:  3\n",
      "2017-08-23T03:03:24.454777: step 4, loss 1.7048, acc 0.3\n",
      "current_step:  4\n",
      "2017-08-23T03:03:24.638798: step 5, loss 1.51942, acc 0.38\n",
      "current_step:  5\n",
      "2017-08-23T03:03:24.819940: step 6, loss 1.40831, acc 0.4\n",
      "current_step:  6\n",
      "2017-08-23T03:03:25.008410: step 7, loss 1.54705, acc 0.52\n",
      "current_step:  7\n",
      "2017-08-23T03:03:25.184522: step 8, loss 1.28973, acc 0.54\n",
      "current_step:  8\n",
      "2017-08-23T03:03:25.372190: step 9, loss 1.91805, acc 0.42\n",
      "current_step:  9\n",
      "2017-08-23T03:03:25.546834: step 10, loss 1.61752, acc 0.44\n",
      "current_step:  10\n",
      "2017-08-23T03:03:25.758202: step 11, loss 1.41494, acc 0.44\n",
      "current_step:  11\n",
      "2017-08-23T03:03:26.034196: step 12, loss 1.27657, acc 0.5\n",
      "current_step:  12\n",
      "2017-08-23T03:03:26.306778: step 13, loss 1.44796, acc 0.44\n",
      "current_step:  13\n",
      "2017-08-23T03:03:26.581497: step 14, loss 1.72612, acc 0.38\n",
      "current_step:  14\n",
      "2017-08-23T03:03:26.854186: step 15, loss 1.26939, acc 0.4\n",
      "current_step:  15\n",
      "2017-08-23T03:03:27.127582: step 16, loss 1.43721, acc 0.44\n",
      "current_step:  16\n",
      "2017-08-23T03:03:27.394914: step 17, loss 1.45504, acc 0.3\n",
      "current_step:  17\n",
      "2017-08-23T03:03:27.665878: step 18, loss 1.21653, acc 0.42\n",
      "current_step:  18\n",
      "2017-08-23T03:03:27.934390: step 19, loss 1.69178, acc 0.38\n",
      "current_step:  19\n",
      "2017-08-23T03:03:28.204891: step 20, loss 1.25277, acc 0.52\n",
      "current_step:  20\n",
      "2017-08-23T03:03:28.471793: step 21, loss 1.07316, acc 0.56\n",
      "current_step:  21\n",
      "2017-08-23T03:03:28.740303: step 22, loss 1.30905, acc 0.48\n",
      "current_step:  22\n",
      "2017-08-23T03:03:29.008490: step 23, loss 1.30626, acc 0.52\n",
      "current_step:  23\n",
      "2017-08-23T03:03:29.274793: step 24, loss 2.00422, acc 0.4\n",
      "current_step:  24\n",
      "2017-08-23T03:03:29.543356: step 25, loss 1.47147, acc 0.48\n",
      "current_step:  25\n",
      "2017-08-23T03:03:29.819857: step 26, loss 1.3489, acc 0.46\n",
      "current_step:  26\n",
      "2017-08-23T03:03:30.092347: step 27, loss 1.24942, acc 0.44\n",
      "current_step:  27\n",
      "2017-08-23T03:03:30.364990: step 28, loss 1.55888, acc 0.5\n",
      "current_step:  28\n",
      "2017-08-23T03:03:30.642776: step 29, loss 1.06305, acc 0.56\n",
      "current_step:  29\n",
      "2017-08-23T03:03:30.922881: step 30, loss 1.16515, acc 0.42\n",
      "current_step:  30\n",
      "2017-08-23T03:03:31.198076: step 31, loss 1.46011, acc 0.48\n",
      "current_step:  31\n",
      "2017-08-23T03:03:31.473968: step 32, loss 1.26911, acc 0.54\n",
      "current_step:  32\n",
      "2017-08-23T03:03:31.755038: step 33, loss 1.46069, acc 0.46\n",
      "current_step:  33\n",
      "2017-08-23T03:03:32.034535: step 34, loss 1.08489, acc 0.56\n",
      "current_step:  34\n",
      "2017-08-23T03:03:32.311563: step 35, loss 1.029, acc 0.64\n",
      "current_step:  35\n",
      "2017-08-23T03:03:32.592645: step 36, loss 1.39817, acc 0.56\n",
      "current_step:  36\n",
      "2017-08-23T03:03:32.870278: step 37, loss 1.56639, acc 0.48\n",
      "current_step:  37\n",
      "2017-08-23T03:03:33.142760: step 38, loss 1.0133, acc 0.62\n",
      "current_step:  38\n",
      "2017-08-23T03:03:33.414653: step 39, loss 1.45458, acc 0.48\n",
      "current_step:  39\n",
      "2017-08-23T03:03:33.689771: step 40, loss 1.42289, acc 0.54\n",
      "current_step:  40\n",
      "2017-08-23T03:03:33.963414: step 41, loss 1.2121, acc 0.56\n",
      "current_step:  41\n",
      "2017-08-23T03:03:34.237213: step 42, loss 1.13735, acc 0.6\n",
      "current_step:  42\n",
      "2017-08-23T03:03:34.511984: step 43, loss 1.04948, acc 0.6\n",
      "current_step:  43\n",
      "2017-08-23T03:03:34.788409: step 44, loss 1.25008, acc 0.54\n",
      "current_step:  44\n",
      "2017-08-23T03:03:35.063190: step 45, loss 1.03071, acc 0.58\n",
      "current_step:  45\n",
      "2017-08-23T03:03:35.342672: step 46, loss 0.902109, acc 0.62\n",
      "current_step:  46\n",
      "2017-08-23T03:03:35.622183: step 47, loss 1.3412, acc 0.64\n",
      "current_step:  47\n",
      "2017-08-23T03:03:35.897381: step 48, loss 1.01687, acc 0.64\n",
      "current_step:  48\n",
      "2017-08-23T03:03:36.176742: step 49, loss 1.32132, acc 0.54\n",
      "current_step:  49\n",
      "2017-08-23T03:03:36.449395: step 50, loss 1.21548, acc 0.56\n",
      "current_step:  50\n",
      "2017-08-23T03:03:36.721319: step 51, loss 1.23802, acc 0.5\n",
      "current_step:  51\n",
      "2017-08-23T03:03:36.994962: step 52, loss 1.41563, acc 0.56\n",
      "current_step:  52\n",
      "2017-08-23T03:03:37.270785: step 53, loss 1.10384, acc 0.66\n",
      "current_step:  53\n",
      "2017-08-23T03:03:37.545429: step 54, loss 1.44526, acc 0.56\n",
      "current_step:  54\n",
      "2017-08-23T03:03:37.819683: step 55, loss 1.38662, acc 0.54\n",
      "current_step:  55\n",
      "2017-08-23T03:03:38.094903: step 56, loss 1.31414, acc 0.68\n",
      "current_step:  56\n",
      "2017-08-23T03:03:38.368452: step 57, loss 1.05279, acc 0.6\n",
      "current_step:  57\n",
      "2017-08-23T03:03:38.647477: step 58, loss 1.09236, acc 0.58\n",
      "current_step:  58\n",
      "2017-08-23T03:03:38.929591: step 59, loss 1.17848, acc 0.6\n",
      "current_step:  59\n",
      "2017-08-23T03:03:39.211902: step 60, loss 1.3615, acc 0.54\n",
      "current_step:  60\n",
      "2017-08-23T03:03:39.494229: step 61, loss 0.887808, acc 0.62\n",
      "current_step:  61\n",
      "2017-08-23T03:03:39.765874: step 62, loss 1.34414, acc 0.44\n",
      "current_step:  62\n",
      "2017-08-23T03:03:40.038907: step 63, loss 0.93904, acc 0.64\n",
      "current_step:  63\n",
      "2017-08-23T03:03:40.303829: step 64, loss 0.864155, acc 0.7\n",
      "current_step:  64\n",
      "2017-08-23T03:03:40.569870: step 65, loss 1.19168, acc 0.54\n",
      "current_step:  65\n",
      "2017-08-23T03:03:40.838004: step 66, loss 1.32471, acc 0.52\n",
      "current_step:  66\n",
      "2017-08-23T03:03:41.104326: step 67, loss 1.02402, acc 0.66\n",
      "current_step:  67\n",
      "2017-08-23T03:03:41.370897: step 68, loss 0.843597, acc 0.72\n",
      "current_step:  68\n",
      "2017-08-23T03:03:41.638502: step 69, loss 1.05829, acc 0.7\n",
      "current_step:  69\n",
      "2017-08-23T03:03:41.905099: step 70, loss 1.2539, acc 0.56\n",
      "current_step:  70\n",
      "2017-08-23T03:03:42.172590: step 71, loss 0.956885, acc 0.68\n",
      "current_step:  71\n",
      "2017-08-23T03:03:42.441750: step 72, loss 1.26865, acc 0.52\n",
      "current_step:  72\n",
      "2017-08-23T03:03:42.708427: step 73, loss 0.997846, acc 0.7\n",
      "current_step:  73\n",
      "2017-08-23T03:03:42.975885: step 74, loss 0.920149, acc 0.68\n",
      "current_step:  74\n",
      "2017-08-23T03:03:43.243570: step 75, loss 0.92826, acc 0.7\n",
      "current_step:  75\n",
      "2017-08-23T03:03:43.514305: step 76, loss 1.14054, acc 0.64\n",
      "current_step:  76\n",
      "2017-08-23T03:03:43.782000: step 77, loss 0.952542, acc 0.64\n",
      "current_step:  77\n",
      "2017-08-23T03:03:44.048384: step 78, loss 1.08897, acc 0.52\n",
      "current_step:  78\n",
      "2017-08-23T03:03:44.317259: step 79, loss 1.39435, acc 0.54\n",
      "current_step:  79\n",
      "2017-08-23T03:03:44.583612: step 80, loss 0.794407, acc 0.72\n",
      "current_step:  80\n",
      "2017-08-23T03:03:44.852804: step 81, loss 1.06918, acc 0.72\n",
      "current_step:  81\n",
      "2017-08-23T03:03:45.119050: step 82, loss 1.0349, acc 0.6\n",
      "current_step:  82\n",
      "2017-08-23T03:03:45.385254: step 83, loss 1.04782, acc 0.62\n",
      "current_step:  83\n",
      "2017-08-23T03:03:45.651460: step 84, loss 1.39456, acc 0.58\n",
      "current_step:  84\n",
      "2017-08-23T03:03:45.917407: step 85, loss 0.865563, acc 0.68\n",
      "current_step:  85\n",
      "2017-08-23T03:03:46.184085: step 86, loss 1.19427, acc 0.56\n",
      "current_step:  86\n",
      "2017-08-23T03:03:46.451835: step 87, loss 1.08197, acc 0.66\n",
      "current_step:  87\n",
      "2017-08-23T03:03:46.719075: step 88, loss 1.05201, acc 0.64\n",
      "current_step:  88\n",
      "2017-08-23T03:03:46.987542: step 89, loss 1.02608, acc 0.68\n",
      "current_step:  89\n",
      "2017-08-23T03:03:47.255950: step 90, loss 0.996518, acc 0.7\n",
      "current_step:  90\n",
      "2017-08-23T03:03:47.522211: step 91, loss 1.00016, acc 0.6\n",
      "current_step:  91\n",
      "2017-08-23T03:03:47.789718: step 92, loss 0.885821, acc 0.66\n",
      "current_step:  92\n",
      "2017-08-23T03:03:48.059498: step 93, loss 1.18647, acc 0.58\n",
      "current_step:  93\n",
      "2017-08-23T03:03:48.326904: step 94, loss 0.84738, acc 0.64\n",
      "current_step:  94\n",
      "2017-08-23T03:03:48.594817: step 95, loss 0.735727, acc 0.8\n",
      "current_step:  95\n",
      "2017-08-23T03:03:48.865064: step 96, loss 1.19087, acc 0.56\n",
      "current_step:  96\n",
      "2017-08-23T03:03:49.135832: step 97, loss 1.28481, acc 0.62\n",
      "current_step:  97\n",
      "2017-08-23T03:03:49.406793: step 98, loss 1.28218, acc 0.62\n",
      "current_step:  98\n",
      "2017-08-23T03:03:49.678851: step 99, loss 0.97531, acc 0.6\n",
      "current_step:  99\n",
      "2017-08-23T03:03:49.949745: step 100, loss 0.836865, acc 0.72\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:03:50.978985: step 100, loss 1.01389, acc 0.660142\n",
      "\n",
      "2017-08-23T03:03:51.247426: step 101, loss 1.04078, acc 0.58\n",
      "current_step:  101\n",
      "2017-08-23T03:03:51.300496: step 102, loss 0.805243, acc 0.666667\n",
      "current_step:  102\n",
      "2017-08-23T03:03:51.568134: step 103, loss 0.77896, acc 0.72\n",
      "current_step:  103\n",
      "2017-08-23T03:03:51.836219: step 104, loss 0.631102, acc 0.78\n",
      "current_step:  104\n",
      "2017-08-23T03:03:52.105333: step 105, loss 0.826145, acc 0.74\n",
      "current_step:  105\n",
      "2017-08-23T03:03:52.373063: step 106, loss 0.736771, acc 0.76\n",
      "current_step:  106\n",
      "2017-08-23T03:03:52.643297: step 107, loss 0.666658, acc 0.78\n",
      "current_step:  107\n",
      "2017-08-23T03:03:52.909071: step 108, loss 1.11496, acc 0.64\n",
      "current_step:  108\n",
      "2017-08-23T03:03:53.174566: step 109, loss 0.935253, acc 0.58\n",
      "current_step:  109\n",
      "2017-08-23T03:03:53.439497: step 110, loss 0.84992, acc 0.7\n",
      "current_step:  110\n",
      "2017-08-23T03:03:53.709546: step 111, loss 0.805243, acc 0.76\n",
      "current_step:  111\n",
      "2017-08-23T03:03:53.983995: step 112, loss 0.859788, acc 0.68\n",
      "current_step:  112\n",
      "2017-08-23T03:03:54.256685: step 113, loss 0.806527, acc 0.76\n",
      "current_step:  113\n",
      "2017-08-23T03:03:54.529587: step 114, loss 0.871431, acc 0.8\n",
      "current_step:  114\n",
      "2017-08-23T03:03:54.798318: step 115, loss 0.660051, acc 0.86\n",
      "current_step:  115\n",
      "2017-08-23T03:03:55.065983: step 116, loss 0.827851, acc 0.68\n",
      "current_step:  116\n",
      "2017-08-23T03:03:55.334385: step 117, loss 0.901225, acc 0.74\n",
      "current_step:  117\n",
      "2017-08-23T03:03:55.602086: step 118, loss 0.898222, acc 0.76\n",
      "current_step:  118\n",
      "2017-08-23T03:03:55.873005: step 119, loss 0.677769, acc 0.74\n",
      "current_step:  119\n",
      "2017-08-23T03:03:56.146230: step 120, loss 0.987496, acc 0.66\n",
      "current_step:  120\n",
      "2017-08-23T03:03:56.416862: step 121, loss 0.745282, acc 0.74\n",
      "current_step:  121\n",
      "2017-08-23T03:03:56.686250: step 122, loss 0.685969, acc 0.76\n",
      "current_step:  122\n",
      "2017-08-23T03:03:56.956395: step 123, loss 0.739879, acc 0.76\n",
      "current_step:  123\n",
      "2017-08-23T03:03:57.224884: step 124, loss 0.732936, acc 0.8\n",
      "current_step:  124\n",
      "2017-08-23T03:03:57.492737: step 125, loss 0.956492, acc 0.68\n",
      "current_step:  125\n",
      "2017-08-23T03:03:57.760349: step 126, loss 1.03704, acc 0.62\n",
      "current_step:  126\n",
      "2017-08-23T03:03:58.030433: step 127, loss 0.789736, acc 0.76\n",
      "current_step:  127\n",
      "2017-08-23T03:03:58.300436: step 128, loss 0.57842, acc 0.82\n",
      "current_step:  128\n",
      "2017-08-23T03:03:58.574808: step 129, loss 0.966801, acc 0.74\n",
      "current_step:  129\n",
      "2017-08-23T03:03:58.847864: step 130, loss 0.744756, acc 0.82\n",
      "current_step:  130\n",
      "2017-08-23T03:03:59.121900: step 131, loss 0.63836, acc 0.84\n",
      "current_step:  131\n",
      "2017-08-23T03:03:59.396138: step 132, loss 0.714117, acc 0.82\n",
      "current_step:  132\n",
      "2017-08-23T03:03:59.671788: step 133, loss 0.753761, acc 0.76\n",
      "current_step:  133\n",
      "2017-08-23T03:03:59.946073: step 134, loss 0.670557, acc 0.74\n",
      "current_step:  134\n",
      "2017-08-23T03:04:00.219362: step 135, loss 0.534404, acc 0.82\n",
      "current_step:  135\n",
      "2017-08-23T03:04:00.486487: step 136, loss 0.708869, acc 0.74\n",
      "current_step:  136\n",
      "2017-08-23T03:04:00.754743: step 137, loss 0.857706, acc 0.72\n",
      "current_step:  137\n",
      "2017-08-23T03:04:01.023241: step 138, loss 0.869384, acc 0.58\n",
      "current_step:  138\n",
      "2017-08-23T03:04:01.292617: step 139, loss 0.673645, acc 0.78\n",
      "current_step:  139\n",
      "2017-08-23T03:04:01.568219: step 140, loss 0.65542, acc 0.76\n",
      "current_step:  140\n",
      "2017-08-23T03:04:01.837648: step 141, loss 0.703339, acc 0.76\n",
      "current_step:  141\n",
      "2017-08-23T03:04:02.109802: step 142, loss 0.756143, acc 0.76\n",
      "current_step:  142\n",
      "2017-08-23T03:04:02.378733: step 143, loss 0.961462, acc 0.68\n",
      "current_step:  143\n",
      "2017-08-23T03:04:02.645121: step 144, loss 0.801297, acc 0.72\n",
      "current_step:  144\n",
      "2017-08-23T03:04:02.914448: step 145, loss 0.94252, acc 0.68\n",
      "current_step:  145\n",
      "2017-08-23T03:04:03.182555: step 146, loss 0.596275, acc 0.8\n",
      "current_step:  146\n",
      "2017-08-23T03:04:03.449576: step 147, loss 0.760806, acc 0.7\n",
      "current_step:  147\n",
      "2017-08-23T03:04:03.720052: step 148, loss 0.814995, acc 0.78\n",
      "current_step:  148\n",
      "2017-08-23T03:04:03.988132: step 149, loss 0.725501, acc 0.8\n",
      "current_step:  149\n",
      "2017-08-23T03:04:04.256304: step 150, loss 0.695566, acc 0.8\n",
      "current_step:  150\n",
      "2017-08-23T03:04:04.522809: step 151, loss 0.703321, acc 0.76\n",
      "current_step:  151\n",
      "2017-08-23T03:04:04.794251: step 152, loss 0.746934, acc 0.74\n",
      "current_step:  152\n",
      "2017-08-23T03:04:05.066503: step 153, loss 0.805601, acc 0.64\n",
      "current_step:  153\n",
      "2017-08-23T03:04:05.336066: step 154, loss 0.903551, acc 0.7\n",
      "current_step:  154\n",
      "2017-08-23T03:04:05.603266: step 155, loss 0.567929, acc 0.82\n",
      "current_step:  155\n",
      "2017-08-23T03:04:05.873616: step 156, loss 0.658242, acc 0.8\n",
      "current_step:  156\n",
      "2017-08-23T03:04:06.142939: step 157, loss 0.765851, acc 0.76\n",
      "current_step:  157\n",
      "2017-08-23T03:04:06.413541: step 158, loss 0.832041, acc 0.7\n",
      "current_step:  158\n",
      "2017-08-23T03:04:06.681160: step 159, loss 1.00727, acc 0.6\n",
      "current_step:  159\n",
      "2017-08-23T03:04:06.952824: step 160, loss 0.923685, acc 0.68\n",
      "current_step:  160\n",
      "2017-08-23T03:04:07.220204: step 161, loss 0.664276, acc 0.82\n",
      "current_step:  161\n",
      "2017-08-23T03:04:07.490555: step 162, loss 0.901473, acc 0.7\n",
      "current_step:  162\n",
      "2017-08-23T03:04:07.761353: step 163, loss 0.877602, acc 0.68\n",
      "current_step:  163\n",
      "2017-08-23T03:04:08.033184: step 164, loss 0.941014, acc 0.72\n",
      "current_step:  164\n",
      "2017-08-23T03:04:08.304370: step 165, loss 0.659045, acc 0.8\n",
      "current_step:  165\n",
      "2017-08-23T03:04:08.573212: step 166, loss 0.714788, acc 0.7\n",
      "current_step:  166\n",
      "2017-08-23T03:04:08.841816: step 167, loss 0.720312, acc 0.64\n",
      "current_step:  167\n",
      "2017-08-23T03:04:09.109349: step 168, loss 0.65166, acc 0.72\n",
      "current_step:  168\n",
      "2017-08-23T03:04:09.376294: step 169, loss 0.656544, acc 0.78\n",
      "current_step:  169\n",
      "2017-08-23T03:04:09.641689: step 170, loss 0.70272, acc 0.76\n",
      "current_step:  170\n",
      "2017-08-23T03:04:09.912547: step 171, loss 0.986141, acc 0.64\n",
      "current_step:  171\n",
      "2017-08-23T03:04:10.180106: step 172, loss 0.738273, acc 0.68\n",
      "current_step:  172\n",
      "2017-08-23T03:04:10.454045: step 173, loss 0.885521, acc 0.76\n",
      "current_step:  173\n",
      "2017-08-23T03:04:10.721383: step 174, loss 0.893686, acc 0.74\n",
      "current_step:  174\n",
      "2017-08-23T03:04:10.991465: step 175, loss 0.778038, acc 0.8\n",
      "current_step:  175\n",
      "2017-08-23T03:04:11.261390: step 176, loss 0.875428, acc 0.7\n",
      "current_step:  176\n",
      "2017-08-23T03:04:11.529457: step 177, loss 0.965685, acc 0.72\n",
      "current_step:  177\n",
      "2017-08-23T03:04:11.799256: step 178, loss 0.805825, acc 0.8\n",
      "current_step:  178\n",
      "2017-08-23T03:04:12.069477: step 179, loss 0.768283, acc 0.74\n",
      "current_step:  179\n",
      "2017-08-23T03:04:12.337639: step 180, loss 0.853202, acc 0.64\n",
      "current_step:  180\n",
      "2017-08-23T03:04:12.608607: step 181, loss 0.755916, acc 0.7\n",
      "current_step:  181\n",
      "2017-08-23T03:04:12.877757: step 182, loss 0.739692, acc 0.72\n",
      "current_step:  182\n",
      "2017-08-23T03:04:13.148228: step 183, loss 0.658653, acc 0.74\n",
      "current_step:  183\n",
      "2017-08-23T03:04:13.419386: step 184, loss 0.604703, acc 0.74\n",
      "current_step:  184\n",
      "2017-08-23T03:04:13.689165: step 185, loss 0.648024, acc 0.78\n",
      "current_step:  185\n",
      "2017-08-23T03:04:13.958564: step 186, loss 0.777283, acc 0.7\n",
      "current_step:  186\n",
      "2017-08-23T03:04:14.231046: step 187, loss 0.857265, acc 0.74\n",
      "current_step:  187\n",
      "2017-08-23T03:04:14.499127: step 188, loss 0.775144, acc 0.7\n",
      "current_step:  188\n",
      "2017-08-23T03:04:14.767762: step 189, loss 0.716103, acc 0.74\n",
      "current_step:  189\n",
      "2017-08-23T03:04:15.036655: step 190, loss 0.653591, acc 0.82\n",
      "current_step:  190\n",
      "2017-08-23T03:04:15.305859: step 191, loss 0.89884, acc 0.62\n",
      "current_step:  191\n",
      "2017-08-23T03:04:15.573525: step 192, loss 0.681353, acc 0.76\n",
      "current_step:  192\n",
      "2017-08-23T03:04:15.841749: step 193, loss 0.833038, acc 0.66\n",
      "current_step:  193\n",
      "2017-08-23T03:04:16.109848: step 194, loss 0.830661, acc 0.66\n",
      "current_step:  194\n",
      "2017-08-23T03:04:16.378222: step 195, loss 0.664197, acc 0.84\n",
      "current_step:  195\n",
      "2017-08-23T03:04:16.646588: step 196, loss 0.731366, acc 0.76\n",
      "current_step:  196\n",
      "2017-08-23T03:04:16.915995: step 197, loss 0.618235, acc 0.8\n",
      "current_step:  197\n",
      "2017-08-23T03:04:17.187860: step 198, loss 0.919009, acc 0.64\n",
      "current_step:  198\n",
      "2017-08-23T03:04:17.457991: step 199, loss 0.781046, acc 0.7\n",
      "current_step:  199\n",
      "2017-08-23T03:04:17.727072: step 200, loss 0.884037, acc 0.72\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:04:18.732637: step 200, loss 0.902962, acc 0.702847\n",
      "\n",
      "2017-08-23T03:04:19.001268: step 201, loss 0.988748, acc 0.64\n",
      "current_step:  201\n",
      "2017-08-23T03:04:19.267996: step 202, loss 0.810474, acc 0.74\n",
      "current_step:  202\n",
      "2017-08-23T03:04:19.537231: step 203, loss 0.995767, acc 0.72\n",
      "current_step:  203\n",
      "2017-08-23T03:04:19.590363: step 204, loss 0.472878, acc 0.888889\n",
      "current_step:  204\n",
      "2017-08-23T03:04:19.857554: step 205, loss 0.848544, acc 0.68\n",
      "current_step:  205\n",
      "2017-08-23T03:04:20.124054: step 206, loss 0.549328, acc 0.84\n",
      "current_step:  206\n",
      "2017-08-23T03:04:20.393825: step 207, loss 0.712221, acc 0.7\n",
      "current_step:  207\n",
      "2017-08-23T03:04:20.660468: step 208, loss 0.698262, acc 0.78\n",
      "current_step:  208\n",
      "2017-08-23T03:04:20.929573: step 209, loss 0.508577, acc 0.9\n",
      "current_step:  209\n",
      "2017-08-23T03:04:21.201288: step 210, loss 0.733372, acc 0.76\n",
      "current_step:  210\n",
      "2017-08-23T03:04:21.471685: step 211, loss 0.520802, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-23T03:04:21.738873: step 212, loss 0.56743, acc 0.78\n",
      "current_step:  212\n",
      "2017-08-23T03:04:22.007324: step 213, loss 0.687529, acc 0.78\n",
      "current_step:  213\n",
      "2017-08-23T03:04:22.272827: step 214, loss 0.496928, acc 0.84\n",
      "current_step:  214\n",
      "2017-08-23T03:04:22.541060: step 215, loss 0.594916, acc 0.8\n",
      "current_step:  215\n",
      "2017-08-23T03:04:22.809089: step 216, loss 0.537943, acc 0.86\n",
      "current_step:  216\n",
      "2017-08-23T03:04:23.077120: step 217, loss 0.537077, acc 0.88\n",
      "current_step:  217\n",
      "2017-08-23T03:04:23.343442: step 218, loss 0.807429, acc 0.7\n",
      "current_step:  218\n",
      "2017-08-23T03:04:23.613423: step 219, loss 0.563561, acc 0.84\n",
      "current_step:  219\n",
      "2017-08-23T03:04:23.883636: step 220, loss 0.573803, acc 0.84\n",
      "current_step:  220\n",
      "2017-08-23T03:04:24.153874: step 221, loss 0.543042, acc 0.82\n",
      "current_step:  221\n",
      "2017-08-23T03:04:24.426110: step 222, loss 0.649459, acc 0.84\n",
      "current_step:  222\n",
      "2017-08-23T03:04:24.699844: step 223, loss 0.517898, acc 0.88\n",
      "current_step:  223\n",
      "2017-08-23T03:04:24.971370: step 224, loss 0.549374, acc 0.82\n",
      "current_step:  224\n",
      "2017-08-23T03:04:25.240848: step 225, loss 0.656411, acc 0.78\n",
      "current_step:  225\n",
      "2017-08-23T03:04:25.510591: step 226, loss 0.605089, acc 0.8\n",
      "current_step:  226\n",
      "2017-08-23T03:04:25.781233: step 227, loss 0.691534, acc 0.8\n",
      "current_step:  227\n",
      "2017-08-23T03:04:26.049269: step 228, loss 0.480744, acc 0.8\n",
      "current_step:  228\n",
      "2017-08-23T03:04:26.318390: step 229, loss 0.436723, acc 0.9\n",
      "current_step:  229\n",
      "2017-08-23T03:04:26.585515: step 230, loss 0.569265, acc 0.9\n",
      "current_step:  230\n",
      "2017-08-23T03:04:26.853107: step 231, loss 0.532303, acc 0.84\n",
      "current_step:  231\n",
      "2017-08-23T03:04:27.122510: step 232, loss 0.64662, acc 0.8\n",
      "current_step:  232\n",
      "2017-08-23T03:04:27.391841: step 233, loss 0.646353, acc 0.8\n",
      "current_step:  233\n",
      "2017-08-23T03:04:27.664131: step 234, loss 0.532199, acc 0.8\n",
      "current_step:  234\n",
      "2017-08-23T03:04:27.931320: step 235, loss 0.608909, acc 0.8\n",
      "current_step:  235\n",
      "2017-08-23T03:04:28.199800: step 236, loss 0.531935, acc 0.82\n",
      "current_step:  236\n",
      "2017-08-23T03:04:28.476272: step 237, loss 0.459359, acc 0.86\n",
      "current_step:  237\n",
      "2017-08-23T03:04:28.747476: step 238, loss 0.496146, acc 0.9\n",
      "current_step:  238\n",
      "2017-08-23T03:04:29.016126: step 239, loss 0.580802, acc 0.78\n",
      "current_step:  239\n",
      "2017-08-23T03:04:29.282779: step 240, loss 0.528769, acc 0.82\n",
      "current_step:  240\n",
      "2017-08-23T03:04:29.549393: step 241, loss 0.605611, acc 0.78\n",
      "current_step:  241\n",
      "2017-08-23T03:04:29.820954: step 242, loss 0.444791, acc 0.86\n",
      "current_step:  242\n",
      "2017-08-23T03:04:30.091616: step 243, loss 0.408466, acc 0.84\n",
      "current_step:  243\n",
      "2017-08-23T03:04:30.359607: step 244, loss 0.487753, acc 0.84\n",
      "current_step:  244\n",
      "2017-08-23T03:04:30.626093: step 245, loss 0.867577, acc 0.68\n",
      "current_step:  245\n",
      "2017-08-23T03:04:30.893586: step 246, loss 0.524278, acc 0.86\n",
      "current_step:  246\n",
      "2017-08-23T03:04:31.164570: step 247, loss 0.428275, acc 0.86\n",
      "current_step:  247\n",
      "2017-08-23T03:04:31.432348: step 248, loss 0.595947, acc 0.8\n",
      "current_step:  248\n",
      "2017-08-23T03:04:31.701815: step 249, loss 0.594075, acc 0.82\n",
      "current_step:  249\n",
      "2017-08-23T03:04:31.970331: step 250, loss 0.627713, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-23T03:04:32.238324: step 251, loss 0.54623, acc 0.82\n",
      "current_step:  251\n",
      "2017-08-23T03:04:32.506121: step 252, loss 0.581414, acc 0.78\n",
      "current_step:  252\n",
      "2017-08-23T03:04:32.772979: step 253, loss 0.538896, acc 0.8\n",
      "current_step:  253\n",
      "2017-08-23T03:04:33.042347: step 254, loss 0.570769, acc 0.82\n",
      "current_step:  254\n",
      "2017-08-23T03:04:33.312357: step 255, loss 0.662866, acc 0.82\n",
      "current_step:  255\n",
      "2017-08-23T03:04:33.578808: step 256, loss 0.424489, acc 0.86\n",
      "current_step:  256\n",
      "2017-08-23T03:04:33.845850: step 257, loss 0.462227, acc 0.88\n",
      "current_step:  257\n",
      "2017-08-23T03:04:34.113831: step 258, loss 0.694241, acc 0.82\n",
      "current_step:  258\n",
      "2017-08-23T03:04:34.378768: step 259, loss 0.596621, acc 0.8\n",
      "current_step:  259\n",
      "2017-08-23T03:04:34.647911: step 260, loss 0.595423, acc 0.8\n",
      "current_step:  260\n",
      "2017-08-23T03:04:34.915744: step 261, loss 0.496537, acc 0.86\n",
      "current_step:  261\n",
      "2017-08-23T03:04:35.183938: step 262, loss 0.650118, acc 0.8\n",
      "current_step:  262\n",
      "2017-08-23T03:04:35.453004: step 263, loss 0.523464, acc 0.88\n",
      "current_step:  263\n",
      "2017-08-23T03:04:35.720580: step 264, loss 0.54559, acc 0.8\n",
      "current_step:  264\n",
      "2017-08-23T03:04:35.987306: step 265, loss 0.71345, acc 0.7\n",
      "current_step:  265\n",
      "2017-08-23T03:04:36.253899: step 266, loss 0.449206, acc 0.88\n",
      "current_step:  266\n",
      "2017-08-23T03:04:36.520320: step 267, loss 0.593762, acc 0.82\n",
      "current_step:  267\n",
      "2017-08-23T03:04:36.787264: step 268, loss 0.530839, acc 0.86\n",
      "current_step:  268\n",
      "2017-08-23T03:04:37.054672: step 269, loss 0.546191, acc 0.84\n",
      "current_step:  269\n",
      "2017-08-23T03:04:37.320777: step 270, loss 0.637475, acc 0.84\n",
      "current_step:  270\n",
      "2017-08-23T03:04:37.586676: step 271, loss 0.434464, acc 0.86\n",
      "current_step:  271\n",
      "2017-08-23T03:04:37.852050: step 272, loss 0.417191, acc 0.9\n",
      "current_step:  272\n",
      "2017-08-23T03:04:38.120188: step 273, loss 0.59673, acc 0.74\n",
      "current_step:  273\n",
      "2017-08-23T03:04:38.388509: step 274, loss 0.493079, acc 0.88\n",
      "current_step:  274\n",
      "2017-08-23T03:04:38.654732: step 275, loss 0.672929, acc 0.8\n",
      "current_step:  275\n",
      "2017-08-23T03:04:38.925256: step 276, loss 0.563999, acc 0.8\n",
      "current_step:  276\n",
      "2017-08-23T03:04:39.192992: step 277, loss 0.644382, acc 0.72\n",
      "current_step:  277\n",
      "2017-08-23T03:04:39.460148: step 278, loss 0.651725, acc 0.78\n",
      "current_step:  278\n",
      "2017-08-23T03:04:39.728065: step 279, loss 0.603299, acc 0.8\n",
      "current_step:  279\n",
      "2017-08-23T03:04:39.997860: step 280, loss 0.71678, acc 0.8\n",
      "current_step:  280\n",
      "2017-08-23T03:04:40.267027: step 281, loss 0.536363, acc 0.84\n",
      "current_step:  281\n",
      "2017-08-23T03:04:40.537102: step 282, loss 0.480712, acc 0.86\n",
      "current_step:  282\n",
      "2017-08-23T03:04:40.806251: step 283, loss 0.665158, acc 0.76\n",
      "current_step:  283\n",
      "2017-08-23T03:04:41.075258: step 284, loss 0.620501, acc 0.76\n",
      "current_step:  284\n",
      "2017-08-23T03:04:41.342639: step 285, loss 0.606993, acc 0.76\n",
      "current_step:  285\n",
      "2017-08-23T03:04:41.610258: step 286, loss 0.663213, acc 0.78\n",
      "current_step:  286\n",
      "2017-08-23T03:04:41.878773: step 287, loss 0.51852, acc 0.78\n",
      "current_step:  287\n",
      "2017-08-23T03:04:42.147542: step 288, loss 0.459608, acc 0.86\n",
      "current_step:  288\n",
      "2017-08-23T03:04:42.415087: step 289, loss 0.603981, acc 0.82\n",
      "current_step:  289\n",
      "2017-08-23T03:04:42.682303: step 290, loss 0.569179, acc 0.8\n",
      "current_step:  290\n",
      "2017-08-23T03:04:42.950130: step 291, loss 0.550845, acc 0.88\n",
      "current_step:  291\n",
      "2017-08-23T03:04:43.218469: step 292, loss 0.480508, acc 0.86\n",
      "current_step:  292\n",
      "2017-08-23T03:04:43.484491: step 293, loss 0.699395, acc 0.76\n",
      "current_step:  293\n",
      "2017-08-23T03:04:43.750504: step 294, loss 0.633237, acc 0.84\n",
      "current_step:  294\n",
      "2017-08-23T03:04:44.026149: step 295, loss 0.546128, acc 0.86\n",
      "current_step:  295\n",
      "2017-08-23T03:04:44.294525: step 296, loss 0.479322, acc 0.88\n",
      "current_step:  296\n",
      "2017-08-23T03:04:44.562922: step 297, loss 0.550565, acc 0.88\n",
      "current_step:  297\n",
      "2017-08-23T03:04:44.832146: step 298, loss 0.762558, acc 0.72\n",
      "current_step:  298\n",
      "2017-08-23T03:04:45.099644: step 299, loss 0.433839, acc 0.9\n",
      "current_step:  299\n",
      "2017-08-23T03:04:45.367531: step 300, loss 0.670967, acc 0.8\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:04:46.390386: step 300, loss 0.860801, acc 0.709964\n",
      "\n",
      "2017-08-23T03:04:46.660421: step 301, loss 0.572285, acc 0.82\n",
      "current_step:  301\n",
      "2017-08-23T03:04:46.928386: step 302, loss 0.577919, acc 0.76\n",
      "current_step:  302\n",
      "2017-08-23T03:04:47.198616: step 303, loss 0.618748, acc 0.84\n",
      "current_step:  303\n",
      "2017-08-23T03:04:47.469292: step 304, loss 0.545598, acc 0.82\n",
      "current_step:  304\n",
      "2017-08-23T03:04:47.737033: step 305, loss 0.541072, acc 0.9\n",
      "current_step:  305\n",
      "2017-08-23T03:04:47.790606: step 306, loss 0.536964, acc 0.888889\n",
      "current_step:  306\n",
      "2017-08-23T03:04:48.062021: step 307, loss 0.331997, acc 0.96\n",
      "current_step:  307\n",
      "2017-08-23T03:04:48.330823: step 308, loss 0.463433, acc 0.86\n",
      "current_step:  308\n",
      "2017-08-23T03:04:48.597138: step 309, loss 0.48242, acc 0.88\n",
      "current_step:  309\n",
      "2017-08-23T03:04:48.864424: step 310, loss 0.396565, acc 0.92\n",
      "current_step:  310\n",
      "2017-08-23T03:04:49.130285: step 311, loss 0.282523, acc 0.96\n",
      "current_step:  311\n",
      "2017-08-23T03:04:49.397455: step 312, loss 0.376377, acc 0.92\n",
      "current_step:  312\n",
      "2017-08-23T03:04:49.662766: step 313, loss 0.457313, acc 0.9\n",
      "current_step:  313\n",
      "2017-08-23T03:04:49.929578: step 314, loss 0.378266, acc 0.94\n",
      "current_step:  314\n",
      "2017-08-23T03:04:50.195806: step 315, loss 0.628661, acc 0.8\n",
      "current_step:  315\n",
      "2017-08-23T03:04:50.464438: step 316, loss 0.518253, acc 0.88\n",
      "current_step:  316\n",
      "2017-08-23T03:04:50.730683: step 317, loss 0.504483, acc 0.88\n",
      "current_step:  317\n",
      "2017-08-23T03:04:50.996155: step 318, loss 0.360905, acc 0.94\n",
      "current_step:  318\n",
      "2017-08-23T03:04:51.263461: step 319, loss 0.41164, acc 0.92\n",
      "current_step:  319\n",
      "2017-08-23T03:04:51.529794: step 320, loss 0.452097, acc 0.88\n",
      "current_step:  320\n",
      "2017-08-23T03:04:51.798790: step 321, loss 0.515596, acc 0.88\n",
      "current_step:  321\n",
      "2017-08-23T03:04:52.066045: step 322, loss 0.598715, acc 0.82\n",
      "current_step:  322\n",
      "2017-08-23T03:04:52.333056: step 323, loss 0.471267, acc 0.84\n",
      "current_step:  323\n",
      "2017-08-23T03:04:52.599105: step 324, loss 0.521338, acc 0.88\n",
      "current_step:  324\n",
      "2017-08-23T03:04:52.866253: step 325, loss 0.373181, acc 0.96\n",
      "current_step:  325\n",
      "2017-08-23T03:04:53.132826: step 326, loss 0.334794, acc 0.92\n",
      "current_step:  326\n",
      "2017-08-23T03:04:53.396981: step 327, loss 0.367654, acc 0.92\n",
      "current_step:  327\n",
      "2017-08-23T03:04:53.664262: step 328, loss 0.483528, acc 0.88\n",
      "current_step:  328\n",
      "2017-08-23T03:04:53.930663: step 329, loss 0.475286, acc 0.84\n",
      "current_step:  329\n",
      "2017-08-23T03:04:54.197386: step 330, loss 0.384809, acc 0.92\n",
      "current_step:  330\n",
      "2017-08-23T03:04:54.463135: step 331, loss 0.412209, acc 0.86\n",
      "current_step:  331\n",
      "2017-08-23T03:04:54.731312: step 332, loss 0.447646, acc 0.88\n",
      "current_step:  332\n",
      "2017-08-23T03:04:54.998159: step 333, loss 0.419654, acc 0.92\n",
      "current_step:  333\n",
      "2017-08-23T03:04:55.265060: step 334, loss 0.483331, acc 0.9\n",
      "current_step:  334\n",
      "2017-08-23T03:04:55.530461: step 335, loss 0.449922, acc 0.88\n",
      "current_step:  335\n",
      "2017-08-23T03:04:55.798991: step 336, loss 0.412308, acc 0.9\n",
      "current_step:  336\n",
      "2017-08-23T03:04:56.066065: step 337, loss 0.555136, acc 0.86\n",
      "current_step:  337\n",
      "2017-08-23T03:04:56.333347: step 338, loss 0.48558, acc 0.88\n",
      "current_step:  338\n",
      "2017-08-23T03:04:56.598151: step 339, loss 0.395834, acc 0.9\n",
      "current_step:  339\n",
      "2017-08-23T03:04:56.866934: step 340, loss 0.383408, acc 0.86\n",
      "current_step:  340\n",
      "2017-08-23T03:04:57.134060: step 341, loss 0.454206, acc 0.8\n",
      "current_step:  341\n",
      "2017-08-23T03:04:57.399202: step 342, loss 0.505161, acc 0.84\n",
      "current_step:  342\n",
      "2017-08-23T03:04:57.666703: step 343, loss 0.549593, acc 0.86\n",
      "current_step:  343\n",
      "2017-08-23T03:04:57.933334: step 344, loss 0.284299, acc 0.98\n",
      "current_step:  344\n",
      "2017-08-23T03:04:58.202504: step 345, loss 0.38961, acc 0.92\n",
      "current_step:  345\n",
      "2017-08-23T03:04:58.469738: step 346, loss 0.451213, acc 0.9\n",
      "current_step:  346\n",
      "2017-08-23T03:04:58.736846: step 347, loss 0.417948, acc 0.88\n",
      "current_step:  347\n",
      "2017-08-23T03:04:59.003253: step 348, loss 0.374018, acc 0.92\n",
      "current_step:  348\n",
      "2017-08-23T03:04:59.271097: step 349, loss 0.470916, acc 0.86\n",
      "current_step:  349\n",
      "2017-08-23T03:04:59.538418: step 350, loss 0.640227, acc 0.78\n",
      "current_step:  350\n",
      "2017-08-23T03:04:59.811340: step 351, loss 0.431162, acc 0.9\n",
      "current_step:  351\n",
      "2017-08-23T03:05:00.080114: step 352, loss 0.412709, acc 0.88\n",
      "current_step:  352\n",
      "2017-08-23T03:05:00.346861: step 353, loss 0.471, acc 0.88\n",
      "current_step:  353\n",
      "2017-08-23T03:05:00.612932: step 354, loss 0.455459, acc 0.9\n",
      "current_step:  354\n",
      "2017-08-23T03:05:00.882822: step 355, loss 0.453116, acc 0.88\n",
      "current_step:  355\n",
      "2017-08-23T03:05:01.152120: step 356, loss 0.393792, acc 0.92\n",
      "current_step:  356\n",
      "2017-08-23T03:05:01.416827: step 357, loss 0.469168, acc 0.88\n",
      "current_step:  357\n",
      "2017-08-23T03:05:01.684028: step 358, loss 0.544878, acc 0.84\n",
      "current_step:  358\n",
      "2017-08-23T03:05:01.952104: step 359, loss 0.374051, acc 0.92\n",
      "current_step:  359\n",
      "2017-08-23T03:05:02.219559: step 360, loss 0.459901, acc 0.84\n",
      "current_step:  360\n",
      "2017-08-23T03:05:02.485946: step 361, loss 0.520034, acc 0.86\n",
      "current_step:  361\n",
      "2017-08-23T03:05:02.753636: step 362, loss 0.490556, acc 0.84\n",
      "current_step:  362\n",
      "2017-08-23T03:05:03.020509: step 363, loss 0.377588, acc 0.98\n",
      "current_step:  363\n",
      "2017-08-23T03:05:03.289431: step 364, loss 0.465263, acc 0.88\n",
      "current_step:  364\n",
      "2017-08-23T03:05:03.556503: step 365, loss 0.540657, acc 0.88\n",
      "current_step:  365\n",
      "2017-08-23T03:05:03.822874: step 366, loss 0.459005, acc 0.9\n",
      "current_step:  366\n",
      "2017-08-23T03:05:04.089045: step 367, loss 0.426439, acc 0.88\n",
      "current_step:  367\n",
      "2017-08-23T03:05:04.355378: step 368, loss 0.427368, acc 0.92\n",
      "current_step:  368\n",
      "2017-08-23T03:05:04.623931: step 369, loss 0.404315, acc 0.88\n",
      "current_step:  369\n",
      "2017-08-23T03:05:04.893994: step 370, loss 0.472862, acc 0.92\n",
      "current_step:  370\n",
      "2017-08-23T03:05:05.160923: step 371, loss 0.449878, acc 0.86\n",
      "current_step:  371\n",
      "2017-08-23T03:05:05.424772: step 372, loss 0.610041, acc 0.76\n",
      "current_step:  372\n",
      "2017-08-23T03:05:05.692180: step 373, loss 0.382997, acc 0.88\n",
      "current_step:  373\n",
      "2017-08-23T03:05:05.959151: step 374, loss 0.49013, acc 0.86\n",
      "current_step:  374\n",
      "2017-08-23T03:05:06.225678: step 375, loss 0.417591, acc 0.88\n",
      "current_step:  375\n",
      "2017-08-23T03:05:06.491624: step 376, loss 0.271333, acc 0.96\n",
      "current_step:  376\n",
      "2017-08-23T03:05:06.756659: step 377, loss 0.550673, acc 0.84\n",
      "current_step:  377\n",
      "2017-08-23T03:05:07.023959: step 378, loss 0.58525, acc 0.82\n",
      "current_step:  378\n",
      "2017-08-23T03:05:07.290616: step 379, loss 0.612356, acc 0.8\n",
      "current_step:  379\n",
      "2017-08-23T03:05:07.555480: step 380, loss 0.54762, acc 0.84\n",
      "current_step:  380\n",
      "2017-08-23T03:05:07.825315: step 381, loss 0.479133, acc 0.86\n",
      "current_step:  381\n",
      "2017-08-23T03:05:08.091564: step 382, loss 0.592275, acc 0.82\n",
      "current_step:  382\n",
      "2017-08-23T03:05:08.357640: step 383, loss 0.286323, acc 0.94\n",
      "current_step:  383\n",
      "2017-08-23T03:05:08.624001: step 384, loss 0.482766, acc 0.84\n",
      "current_step:  384\n",
      "2017-08-23T03:05:08.890504: step 385, loss 0.315357, acc 0.94\n",
      "current_step:  385\n",
      "2017-08-23T03:05:09.155462: step 386, loss 0.625161, acc 0.8\n",
      "current_step:  386\n",
      "2017-08-23T03:05:09.422795: step 387, loss 0.545775, acc 0.86\n",
      "current_step:  387\n",
      "2017-08-23T03:05:09.690876: step 388, loss 0.347519, acc 0.96\n",
      "current_step:  388\n",
      "2017-08-23T03:05:09.957986: step 389, loss 0.343293, acc 0.94\n",
      "current_step:  389\n",
      "2017-08-23T03:05:10.225140: step 390, loss 0.503984, acc 0.88\n",
      "current_step:  390\n",
      "2017-08-23T03:05:10.494180: step 391, loss 0.502887, acc 0.84\n",
      "current_step:  391\n",
      "2017-08-23T03:05:10.763423: step 392, loss 0.545224, acc 0.86\n",
      "current_step:  392\n",
      "2017-08-23T03:05:11.031558: step 393, loss 0.298937, acc 0.92\n",
      "current_step:  393\n",
      "2017-08-23T03:05:11.299356: step 394, loss 0.550193, acc 0.8\n",
      "current_step:  394\n",
      "2017-08-23T03:05:11.567340: step 395, loss 0.343832, acc 0.9\n",
      "current_step:  395\n",
      "2017-08-23T03:05:11.833095: step 396, loss 0.306928, acc 0.94\n",
      "current_step:  396\n",
      "2017-08-23T03:05:12.100186: step 397, loss 0.380932, acc 0.92\n",
      "current_step:  397\n",
      "2017-08-23T03:05:12.367457: step 398, loss 0.482827, acc 0.84\n",
      "current_step:  398\n",
      "2017-08-23T03:05:12.632861: step 399, loss 0.378286, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-23T03:05:12.901580: step 400, loss 0.422609, acc 0.84\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:05:13.894066: step 400, loss 0.853318, acc 0.699288\n",
      "\n",
      "2017-08-23T03:05:14.163136: step 401, loss 0.33046, acc 0.92\n",
      "current_step:  401\n",
      "2017-08-23T03:05:14.428600: step 402, loss 0.399168, acc 0.9\n",
      "current_step:  402\n",
      "2017-08-23T03:05:14.694103: step 403, loss 0.436513, acc 0.86\n",
      "current_step:  403\n",
      "2017-08-23T03:05:14.962175: step 404, loss 0.411865, acc 0.88\n",
      "current_step:  404\n",
      "2017-08-23T03:05:15.230052: step 405, loss 0.297106, acc 0.92\n",
      "current_step:  405\n",
      "2017-08-23T03:05:15.497765: step 406, loss 0.443697, acc 0.86\n",
      "current_step:  406\n",
      "2017-08-23T03:05:15.763523: step 407, loss 0.391882, acc 0.92\n",
      "current_step:  407\n",
      "2017-08-23T03:05:15.815982: step 408, loss 0.387532, acc 0.777778\n",
      "current_step:  408\n",
      "2017-08-23T03:05:16.087710: step 409, loss 0.368244, acc 0.86\n",
      "current_step:  409\n",
      "2017-08-23T03:05:16.355114: step 410, loss 0.302292, acc 0.94\n",
      "current_step:  410\n",
      "2017-08-23T03:05:16.621247: step 411, loss 0.34146, acc 0.96\n",
      "current_step:  411\n",
      "2017-08-23T03:05:16.888703: step 412, loss 0.33918, acc 0.96\n",
      "current_step:  412\n",
      "2017-08-23T03:05:17.157193: step 413, loss 0.220064, acc 0.98\n",
      "current_step:  413\n",
      "2017-08-23T03:05:17.424390: step 414, loss 0.324779, acc 0.94\n",
      "current_step:  414\n",
      "2017-08-23T03:05:17.692151: step 415, loss 0.292394, acc 0.92\n",
      "current_step:  415\n",
      "2017-08-23T03:05:17.958885: step 416, loss 0.333808, acc 0.98\n",
      "current_step:  416\n",
      "2017-08-23T03:05:18.227146: step 417, loss 0.386754, acc 0.86\n",
      "current_step:  417\n",
      "2017-08-23T03:05:18.495154: step 418, loss 0.356326, acc 0.96\n",
      "current_step:  418\n",
      "2017-08-23T03:05:18.762203: step 419, loss 0.277432, acc 0.94\n",
      "current_step:  419\n",
      "2017-08-23T03:05:19.031582: step 420, loss 0.294781, acc 0.94\n",
      "current_step:  420\n",
      "2017-08-23T03:05:19.299157: step 421, loss 0.22112, acc 1\n",
      "current_step:  421\n",
      "2017-08-23T03:05:19.567098: step 422, loss 0.258501, acc 0.96\n",
      "current_step:  422\n",
      "2017-08-23T03:05:19.834167: step 423, loss 0.344599, acc 0.92\n",
      "current_step:  423\n",
      "2017-08-23T03:05:20.101097: step 424, loss 0.468506, acc 0.84\n",
      "current_step:  424\n",
      "2017-08-23T03:05:20.368054: step 425, loss 0.336048, acc 0.92\n",
      "current_step:  425\n",
      "2017-08-23T03:05:20.634431: step 426, loss 0.310197, acc 0.96\n",
      "current_step:  426\n",
      "2017-08-23T03:05:20.902453: step 427, loss 0.263916, acc 0.98\n",
      "current_step:  427\n",
      "2017-08-23T03:05:21.168547: step 428, loss 0.278809, acc 0.94\n",
      "current_step:  428\n",
      "2017-08-23T03:05:21.435375: step 429, loss 0.327958, acc 0.92\n",
      "current_step:  429\n",
      "2017-08-23T03:05:21.702934: step 430, loss 0.277838, acc 0.96\n",
      "current_step:  430\n",
      "2017-08-23T03:05:21.970641: step 431, loss 0.317555, acc 0.96\n",
      "current_step:  431\n",
      "2017-08-23T03:05:22.238604: step 432, loss 0.283152, acc 0.96\n",
      "current_step:  432\n",
      "2017-08-23T03:05:22.505523: step 433, loss 0.414815, acc 0.88\n",
      "current_step:  433\n",
      "2017-08-23T03:05:22.770394: step 434, loss 0.444017, acc 0.88\n",
      "current_step:  434\n",
      "2017-08-23T03:05:23.040058: step 435, loss 0.458172, acc 0.88\n",
      "current_step:  435\n",
      "2017-08-23T03:05:23.308263: step 436, loss 0.381157, acc 0.92\n",
      "current_step:  436\n",
      "2017-08-23T03:05:23.574750: step 437, loss 0.359339, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-23T03:05:23.845179: step 438, loss 0.354333, acc 0.9\n",
      "current_step:  438\n",
      "2017-08-23T03:05:24.115976: step 439, loss 0.242284, acc 0.98\n",
      "current_step:  439\n",
      "2017-08-23T03:05:24.384186: step 440, loss 0.432031, acc 0.88\n",
      "current_step:  440\n",
      "2017-08-23T03:05:24.652424: step 441, loss 0.4054, acc 0.84\n",
      "current_step:  441\n",
      "2017-08-23T03:05:24.920435: step 442, loss 0.409905, acc 0.92\n",
      "current_step:  442\n",
      "2017-08-23T03:05:25.188593: step 443, loss 0.289062, acc 0.96\n",
      "current_step:  443\n",
      "2017-08-23T03:05:25.455414: step 444, loss 0.335986, acc 0.9\n",
      "current_step:  444\n",
      "2017-08-23T03:05:25.722683: step 445, loss 0.229367, acc 1\n",
      "current_step:  445\n",
      "2017-08-23T03:05:25.993222: step 446, loss 0.352575, acc 0.92\n",
      "current_step:  446\n",
      "2017-08-23T03:05:26.259730: step 447, loss 0.344618, acc 0.94\n",
      "current_step:  447\n",
      "2017-08-23T03:05:26.526288: step 448, loss 0.411342, acc 0.9\n",
      "current_step:  448\n",
      "2017-08-23T03:05:26.795298: step 449, loss 0.436747, acc 0.84\n",
      "current_step:  449\n",
      "2017-08-23T03:05:27.064218: step 450, loss 0.334545, acc 0.94\n",
      "current_step:  450\n",
      "2017-08-23T03:05:27.331610: step 451, loss 0.325353, acc 0.94\n",
      "current_step:  451\n",
      "2017-08-23T03:05:27.597652: step 452, loss 0.300343, acc 0.94\n",
      "current_step:  452\n",
      "2017-08-23T03:05:27.867553: step 453, loss 0.340732, acc 0.9\n",
      "current_step:  453\n",
      "2017-08-23T03:05:28.134474: step 454, loss 0.403427, acc 0.86\n",
      "current_step:  454\n",
      "2017-08-23T03:05:28.401462: step 455, loss 0.365468, acc 0.92\n",
      "current_step:  455\n",
      "2017-08-23T03:05:28.667622: step 456, loss 0.387411, acc 0.88\n",
      "current_step:  456\n",
      "2017-08-23T03:05:28.936133: step 457, loss 0.403732, acc 0.88\n",
      "current_step:  457\n",
      "2017-08-23T03:05:29.203313: step 458, loss 0.433521, acc 0.86\n",
      "current_step:  458\n",
      "2017-08-23T03:05:29.470974: step 459, loss 0.347551, acc 0.9\n",
      "current_step:  459\n",
      "2017-08-23T03:05:29.738764: step 460, loss 0.230053, acc 0.96\n",
      "current_step:  460\n",
      "2017-08-23T03:05:30.005990: step 461, loss 0.208785, acc 1\n",
      "current_step:  461\n",
      "2017-08-23T03:05:30.273232: step 462, loss 0.293017, acc 0.94\n",
      "current_step:  462\n",
      "2017-08-23T03:05:30.539373: step 463, loss 0.290438, acc 0.92\n",
      "current_step:  463\n",
      "2017-08-23T03:05:30.807936: step 464, loss 0.354983, acc 0.9\n",
      "current_step:  464\n",
      "2017-08-23T03:05:31.076377: step 465, loss 0.253704, acc 0.94\n",
      "current_step:  465\n",
      "2017-08-23T03:05:31.341667: step 466, loss 0.442632, acc 0.82\n",
      "current_step:  466\n",
      "2017-08-23T03:05:31.608795: step 467, loss 0.339485, acc 0.92\n",
      "current_step:  467\n",
      "2017-08-23T03:05:31.875670: step 468, loss 0.411689, acc 0.88\n",
      "current_step:  468\n",
      "2017-08-23T03:05:32.141836: step 469, loss 0.495369, acc 0.82\n",
      "current_step:  469\n",
      "2017-08-23T03:05:32.408718: step 470, loss 0.411031, acc 0.92\n",
      "current_step:  470\n",
      "2017-08-23T03:05:32.673977: step 471, loss 0.510991, acc 0.88\n",
      "current_step:  471\n",
      "2017-08-23T03:05:32.941306: step 472, loss 0.3654, acc 0.88\n",
      "current_step:  472\n",
      "2017-08-23T03:05:33.209886: step 473, loss 0.380284, acc 0.94\n",
      "current_step:  473\n",
      "2017-08-23T03:05:33.477141: step 474, loss 0.502611, acc 0.9\n",
      "current_step:  474\n",
      "2017-08-23T03:05:33.743730: step 475, loss 0.443943, acc 0.92\n",
      "current_step:  475\n",
      "2017-08-23T03:05:34.009775: step 476, loss 0.364466, acc 0.96\n",
      "current_step:  476\n",
      "2017-08-23T03:05:34.278332: step 477, loss 0.304533, acc 0.96\n",
      "current_step:  477\n",
      "2017-08-23T03:05:34.543684: step 478, loss 0.346352, acc 0.92\n",
      "current_step:  478\n",
      "2017-08-23T03:05:34.808679: step 479, loss 0.357326, acc 0.9\n",
      "current_step:  479\n",
      "2017-08-23T03:05:35.078261: step 480, loss 0.406439, acc 0.86\n",
      "current_step:  480\n",
      "2017-08-23T03:05:35.344023: step 481, loss 0.264151, acc 0.92\n",
      "current_step:  481\n",
      "2017-08-23T03:05:35.611466: step 482, loss 0.44187, acc 0.8\n",
      "current_step:  482\n",
      "2017-08-23T03:05:35.887549: step 483, loss 0.389385, acc 0.9\n",
      "current_step:  483\n",
      "2017-08-23T03:05:36.155089: step 484, loss 0.428135, acc 0.84\n",
      "current_step:  484\n",
      "2017-08-23T03:05:36.423779: step 485, loss 0.327891, acc 0.9\n",
      "current_step:  485\n",
      "2017-08-23T03:05:36.692486: step 486, loss 0.319655, acc 0.94\n",
      "current_step:  486\n",
      "2017-08-23T03:05:36.958782: step 487, loss 0.327517, acc 0.94\n",
      "current_step:  487\n",
      "2017-08-23T03:05:37.225301: step 488, loss 0.316412, acc 0.92\n",
      "current_step:  488\n",
      "2017-08-23T03:05:37.490763: step 489, loss 0.235792, acc 0.98\n",
      "current_step:  489\n",
      "2017-08-23T03:05:37.758893: step 490, loss 0.416775, acc 0.9\n",
      "current_step:  490\n",
      "2017-08-23T03:05:38.027036: step 491, loss 0.35542, acc 0.9\n",
      "current_step:  491\n",
      "2017-08-23T03:05:38.296208: step 492, loss 0.344898, acc 0.94\n",
      "current_step:  492\n",
      "2017-08-23T03:05:38.563115: step 493, loss 0.425585, acc 0.9\n",
      "current_step:  493\n",
      "2017-08-23T03:05:38.829991: step 494, loss 0.366407, acc 0.9\n",
      "current_step:  494\n",
      "2017-08-23T03:05:39.098202: step 495, loss 0.382279, acc 0.88\n",
      "current_step:  495\n",
      "2017-08-23T03:05:39.363368: step 496, loss 0.462792, acc 0.88\n",
      "current_step:  496\n",
      "2017-08-23T03:05:39.629590: step 497, loss 0.304767, acc 0.96\n",
      "current_step:  497\n",
      "2017-08-23T03:05:39.896177: step 498, loss 0.321576, acc 0.96\n",
      "current_step:  498\n",
      "2017-08-23T03:05:40.164749: step 499, loss 0.212295, acc 1\n",
      "current_step:  499\n",
      "2017-08-23T03:05:40.432623: step 500, loss 0.29752, acc 0.92\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:05:41.434929: step 500, loss 0.848821, acc 0.683274\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0303_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:05:41.791096: step 501, loss 0.413154, acc 0.9\n",
      "current_step:  501\n",
      "2017-08-23T03:05:42.057116: step 502, loss 0.310674, acc 0.94\n",
      "current_step:  502\n",
      "2017-08-23T03:05:42.323945: step 503, loss 0.277281, acc 0.96\n",
      "current_step:  503\n",
      "2017-08-23T03:05:42.590338: step 504, loss 0.480352, acc 0.88\n",
      "current_step:  504\n",
      "2017-08-23T03:05:42.856278: step 505, loss 0.241926, acc 0.94\n",
      "current_step:  505\n",
      "2017-08-23T03:05:43.122928: step 506, loss 0.341653, acc 0.92\n",
      "current_step:  506\n",
      "2017-08-23T03:05:43.388706: step 507, loss 0.473219, acc 0.86\n",
      "current_step:  507\n",
      "2017-08-23T03:05:43.653431: step 508, loss 0.21449, acc 0.98\n",
      "current_step:  508\n",
      "2017-08-23T03:05:43.920378: step 509, loss 0.394787, acc 0.86\n",
      "current_step:  509\n",
      "2017-08-23T03:05:43.973312: step 510, loss 0.300059, acc 1\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.614656\n",
      "Precision Score = 0.586953\n",
      "Recall Score = 0.654804\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.656010\n",
      "Precision Score = 0.635344\n",
      "Recall Score = 0.695730\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.651974\n",
      "Precision Score = 0.644191\n",
      "Recall Score = 0.701068\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.658145\n",
      "Precision Score = 0.644402\n",
      "Recall Score = 0.693950\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.646581\n",
      "Precision Score = 0.628722\n",
      "Recall Score = 0.677936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vslchu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vslchu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v1 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 1, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  411\n",
      "Vocabulary Size: 6361\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0306_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0306_UTC\n",
      "2017-08-23T03:06:08.864738: step 1, loss 2.72359, acc 0.02\n",
      "current_step:  1\n",
      "2017-08-23T03:06:08.985534: step 2, loss 2.10008, acc 0.12\n",
      "current_step:  2\n",
      "2017-08-23T03:06:09.095633: step 3, loss 1.67343, acc 0.34\n",
      "current_step:  3\n",
      "2017-08-23T03:06:09.211802: step 4, loss 1.36412, acc 0.44\n",
      "current_step:  4\n",
      "2017-08-23T03:06:09.326955: step 5, loss 1.47887, acc 0.44\n",
      "current_step:  5\n",
      "2017-08-23T03:06:09.438638: step 6, loss 1.59233, acc 0.38\n",
      "current_step:  6\n",
      "2017-08-23T03:06:09.547550: step 7, loss 1.22439, acc 0.56\n",
      "current_step:  7\n",
      "2017-08-23T03:06:09.658616: step 8, loss 1.47965, acc 0.54\n",
      "current_step:  8\n",
      "2017-08-23T03:06:09.777671: step 9, loss 1.37254, acc 0.46\n",
      "current_step:  9\n",
      "2017-08-23T03:06:09.884492: step 10, loss 1.34474, acc 0.44\n",
      "current_step:  10\n",
      "2017-08-23T03:06:09.992103: step 11, loss 1.4203, acc 0.52\n",
      "current_step:  11\n",
      "2017-08-23T03:06:10.100081: step 12, loss 1.33549, acc 0.46\n",
      "current_step:  12\n",
      "2017-08-23T03:06:10.212318: step 13, loss 1.57968, acc 0.38\n",
      "current_step:  13\n",
      "2017-08-23T03:06:10.323965: step 14, loss 1.76311, acc 0.36\n",
      "current_step:  14\n",
      "2017-08-23T03:06:10.431012: step 15, loss 1.60549, acc 0.42\n",
      "current_step:  15\n",
      "2017-08-23T03:06:10.535386: step 16, loss 1.63291, acc 0.48\n",
      "current_step:  16\n",
      "2017-08-23T03:06:10.653318: step 17, loss 1.32407, acc 0.42\n",
      "current_step:  17\n",
      "2017-08-23T03:06:10.799616: step 18, loss 1.19715, acc 0.52\n",
      "current_step:  18\n",
      "2017-08-23T03:06:10.974977: step 19, loss 0.969863, acc 0.64\n",
      "current_step:  19\n",
      "2017-08-23T03:06:11.148014: step 20, loss 1.16171, acc 0.6\n",
      "current_step:  20\n",
      "2017-08-23T03:06:11.319988: step 21, loss 0.932547, acc 0.66\n",
      "current_step:  21\n",
      "2017-08-23T03:06:11.494123: step 22, loss 1.69009, acc 0.4\n",
      "current_step:  22\n",
      "2017-08-23T03:06:11.667141: step 23, loss 1.42389, acc 0.58\n",
      "current_step:  23\n",
      "2017-08-23T03:06:11.841577: step 24, loss 1.16821, acc 0.6\n",
      "current_step:  24\n",
      "2017-08-23T03:06:12.015414: step 25, loss 1.23012, acc 0.52\n",
      "current_step:  25\n",
      "2017-08-23T03:06:12.186718: step 26, loss 1.33562, acc 0.5\n",
      "current_step:  26\n",
      "2017-08-23T03:06:12.358130: step 27, loss 1.20543, acc 0.56\n",
      "current_step:  27\n",
      "2017-08-23T03:06:12.530810: step 28, loss 1.35805, acc 0.6\n",
      "current_step:  28\n",
      "2017-08-23T03:06:12.702804: step 29, loss 1.21796, acc 0.52\n",
      "current_step:  29\n",
      "2017-08-23T03:06:12.878201: step 30, loss 1.20688, acc 0.62\n",
      "current_step:  30\n",
      "2017-08-23T03:06:13.051525: step 31, loss 1.07134, acc 0.58\n",
      "current_step:  31\n",
      "2017-08-23T03:06:13.226939: step 32, loss 1.19521, acc 0.6\n",
      "current_step:  32\n",
      "2017-08-23T03:06:13.400299: step 33, loss 1.04212, acc 0.56\n",
      "current_step:  33\n",
      "2017-08-23T03:06:13.573979: step 34, loss 1.35272, acc 0.48\n",
      "current_step:  34\n",
      "2017-08-23T03:06:13.746904: step 35, loss 0.961812, acc 0.54\n",
      "current_step:  35\n",
      "2017-08-23T03:06:13.919251: step 36, loss 0.954443, acc 0.68\n",
      "current_step:  36\n",
      "2017-08-23T03:06:14.092219: step 37, loss 1.16933, acc 0.58\n",
      "current_step:  37\n",
      "2017-08-23T03:06:14.263942: step 38, loss 1.34988, acc 0.54\n",
      "current_step:  38\n",
      "2017-08-23T03:06:14.437004: step 39, loss 1.27278, acc 0.58\n",
      "current_step:  39\n",
      "2017-08-23T03:06:14.607757: step 40, loss 1.61789, acc 0.54\n",
      "current_step:  40\n",
      "2017-08-23T03:06:14.778983: step 41, loss 1.27498, acc 0.6\n",
      "current_step:  41\n",
      "2017-08-23T03:06:14.950872: step 42, loss 0.994644, acc 0.62\n",
      "current_step:  42\n",
      "2017-08-23T03:06:15.121585: step 43, loss 1.20143, acc 0.6\n",
      "current_step:  43\n",
      "2017-08-23T03:06:15.293573: step 44, loss 1.36463, acc 0.52\n",
      "current_step:  44\n",
      "2017-08-23T03:06:15.464532: step 45, loss 1.31005, acc 0.54\n",
      "current_step:  45\n",
      "2017-08-23T03:06:15.636237: step 46, loss 1.12074, acc 0.64\n",
      "current_step:  46\n",
      "2017-08-23T03:06:15.807363: step 47, loss 1.05686, acc 0.62\n",
      "current_step:  47\n",
      "2017-08-23T03:06:15.978392: step 48, loss 1.10901, acc 0.64\n",
      "current_step:  48\n",
      "2017-08-23T03:06:16.148581: step 49, loss 1.01788, acc 0.66\n",
      "current_step:  49\n",
      "2017-08-23T03:06:16.320767: step 50, loss 0.887076, acc 0.74\n",
      "current_step:  50\n",
      "2017-08-23T03:06:16.492842: step 51, loss 1.39969, acc 0.5\n",
      "current_step:  51\n",
      "2017-08-23T03:06:16.663470: step 52, loss 1.0595, acc 0.62\n",
      "current_step:  52\n",
      "2017-08-23T03:06:16.835907: step 53, loss 1.20584, acc 0.56\n",
      "current_step:  53\n",
      "2017-08-23T03:06:17.008745: step 54, loss 1.16207, acc 0.62\n",
      "current_step:  54\n",
      "2017-08-23T03:06:17.181841: step 55, loss 0.773562, acc 0.74\n",
      "current_step:  55\n",
      "2017-08-23T03:06:17.353602: step 56, loss 1.16819, acc 0.56\n",
      "current_step:  56\n",
      "2017-08-23T03:06:17.525988: step 57, loss 1.20015, acc 0.54\n",
      "current_step:  57\n",
      "2017-08-23T03:06:17.697080: step 58, loss 1.10241, acc 0.66\n",
      "current_step:  58\n",
      "2017-08-23T03:06:17.869873: step 59, loss 1.19131, acc 0.52\n",
      "current_step:  59\n",
      "2017-08-23T03:06:18.041600: step 60, loss 1.22548, acc 0.52\n",
      "current_step:  60\n",
      "2017-08-23T03:06:18.212951: step 61, loss 0.907779, acc 0.66\n",
      "current_step:  61\n",
      "2017-08-23T03:06:18.382551: step 62, loss 1.23764, acc 0.62\n",
      "current_step:  62\n",
      "2017-08-23T03:06:18.554590: step 63, loss 1.16382, acc 0.58\n",
      "current_step:  63\n",
      "2017-08-23T03:06:18.727005: step 64, loss 0.970975, acc 0.62\n",
      "current_step:  64\n",
      "2017-08-23T03:06:18.900614: step 65, loss 1.21073, acc 0.58\n",
      "current_step:  65\n",
      "2017-08-23T03:06:19.072163: step 66, loss 1.19853, acc 0.62\n",
      "current_step:  66\n",
      "2017-08-23T03:06:19.242644: step 67, loss 1.10783, acc 0.68\n",
      "current_step:  67\n",
      "2017-08-23T03:06:19.415730: step 68, loss 1.0552, acc 0.62\n",
      "current_step:  68\n",
      "2017-08-23T03:06:19.587119: step 69, loss 1.05908, acc 0.58\n",
      "current_step:  69\n",
      "2017-08-23T03:06:19.758546: step 70, loss 0.870531, acc 0.64\n",
      "current_step:  70\n",
      "2017-08-23T03:06:19.930844: step 71, loss 0.93346, acc 0.66\n",
      "current_step:  71\n",
      "2017-08-23T03:06:20.101728: step 72, loss 1.1251, acc 0.64\n",
      "current_step:  72\n",
      "2017-08-23T03:06:20.272765: step 73, loss 1.36418, acc 0.48\n",
      "current_step:  73\n",
      "2017-08-23T03:06:20.442851: step 74, loss 1.09554, acc 0.66\n",
      "current_step:  74\n",
      "2017-08-23T03:06:20.613961: step 75, loss 1.07734, acc 0.64\n",
      "current_step:  75\n",
      "2017-08-23T03:06:20.785076: step 76, loss 1.06371, acc 0.6\n",
      "current_step:  76\n",
      "2017-08-23T03:06:20.956548: step 77, loss 1.25164, acc 0.6\n",
      "current_step:  77\n",
      "2017-08-23T03:06:21.127922: step 78, loss 1.1234, acc 0.6\n",
      "current_step:  78\n",
      "2017-08-23T03:06:21.299455: step 79, loss 0.915377, acc 0.7\n",
      "current_step:  79\n",
      "2017-08-23T03:06:21.469898: step 80, loss 1.23645, acc 0.62\n",
      "current_step:  80\n",
      "2017-08-23T03:06:21.641554: step 81, loss 0.903799, acc 0.72\n",
      "current_step:  81\n",
      "2017-08-23T03:06:21.812633: step 82, loss 1.13907, acc 0.64\n",
      "current_step:  82\n",
      "2017-08-23T03:06:21.983816: step 83, loss 1.01725, acc 0.66\n",
      "current_step:  83\n",
      "2017-08-23T03:06:22.154921: step 84, loss 1.3505, acc 0.64\n",
      "current_step:  84\n",
      "2017-08-23T03:06:22.327599: step 85, loss 1.00886, acc 0.6\n",
      "current_step:  85\n",
      "2017-08-23T03:06:22.499362: step 86, loss 1.06943, acc 0.58\n",
      "current_step:  86\n",
      "2017-08-23T03:06:22.671656: step 87, loss 0.819102, acc 0.74\n",
      "current_step:  87\n",
      "2017-08-23T03:06:22.846538: step 88, loss 1.06985, acc 0.6\n",
      "current_step:  88\n",
      "2017-08-23T03:06:23.021942: step 89, loss 0.846655, acc 0.68\n",
      "current_step:  89\n",
      "2017-08-23T03:06:23.197206: step 90, loss 0.92982, acc 0.68\n",
      "current_step:  90\n",
      "2017-08-23T03:06:23.371915: step 91, loss 1.0414, acc 0.66\n",
      "current_step:  91\n",
      "2017-08-23T03:06:23.548613: step 92, loss 0.912829, acc 0.72\n",
      "current_step:  92\n",
      "2017-08-23T03:06:23.727277: step 93, loss 1.04466, acc 0.7\n",
      "current_step:  93\n",
      "2017-08-23T03:06:23.903333: step 94, loss 0.994062, acc 0.62\n",
      "current_step:  94\n",
      "2017-08-23T03:06:24.083401: step 95, loss 0.789089, acc 0.74\n",
      "current_step:  95\n",
      "2017-08-23T03:06:24.257599: step 96, loss 1.28068, acc 0.54\n",
      "current_step:  96\n",
      "2017-08-23T03:06:24.433024: step 97, loss 0.952388, acc 0.64\n",
      "current_step:  97\n",
      "2017-08-23T03:06:24.609984: step 98, loss 1.12929, acc 0.62\n",
      "current_step:  98\n",
      "2017-08-23T03:06:24.785692: step 99, loss 0.997934, acc 0.66\n",
      "current_step:  99\n",
      "2017-08-23T03:06:24.960696: step 100, loss 1.04044, acc 0.7\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:06:25.620642: step 100, loss 0.978178, acc 0.660142\n",
      "\n",
      "2017-08-23T03:06:25.795436: step 101, loss 1.02865, acc 0.64\n",
      "current_step:  101\n",
      "2017-08-23T03:06:25.832303: step 102, loss 1.01947, acc 0.666667\n",
      "current_step:  102\n",
      "2017-08-23T03:06:26.006609: step 103, loss 0.810295, acc 0.72\n",
      "current_step:  103\n",
      "2017-08-23T03:06:26.179743: step 104, loss 0.742338, acc 0.78\n",
      "current_step:  104\n",
      "2017-08-23T03:06:26.353583: step 105, loss 0.85355, acc 0.74\n",
      "current_step:  105\n",
      "2017-08-23T03:06:26.525777: step 106, loss 0.726156, acc 0.76\n",
      "current_step:  106\n",
      "2017-08-23T03:06:26.697086: step 107, loss 0.882422, acc 0.66\n",
      "current_step:  107\n",
      "2017-08-23T03:06:26.870757: step 108, loss 0.891385, acc 0.66\n",
      "current_step:  108\n",
      "2017-08-23T03:06:27.042926: step 109, loss 0.648247, acc 0.78\n",
      "current_step:  109\n",
      "2017-08-23T03:06:27.215436: step 110, loss 0.827274, acc 0.68\n",
      "current_step:  110\n",
      "2017-08-23T03:06:27.388224: step 111, loss 0.712739, acc 0.62\n",
      "current_step:  111\n",
      "2017-08-23T03:06:27.561640: step 112, loss 0.659535, acc 0.78\n",
      "current_step:  112\n",
      "2017-08-23T03:06:27.734644: step 113, loss 0.694898, acc 0.82\n",
      "current_step:  113\n",
      "2017-08-23T03:06:27.910306: step 114, loss 0.901954, acc 0.68\n",
      "current_step:  114\n",
      "2017-08-23T03:06:28.082741: step 115, loss 0.875512, acc 0.72\n",
      "current_step:  115\n",
      "2017-08-23T03:06:28.253416: step 116, loss 1.17821, acc 0.66\n",
      "current_step:  116\n",
      "2017-08-23T03:06:28.425773: step 117, loss 0.865084, acc 0.66\n",
      "current_step:  117\n",
      "2017-08-23T03:06:28.599282: step 118, loss 0.767383, acc 0.7\n",
      "current_step:  118\n",
      "2017-08-23T03:06:28.770577: step 119, loss 0.711255, acc 0.74\n",
      "current_step:  119\n",
      "2017-08-23T03:06:28.942832: step 120, loss 0.857579, acc 0.66\n",
      "current_step:  120\n",
      "2017-08-23T03:06:29.114799: step 121, loss 0.763651, acc 0.88\n",
      "current_step:  121\n",
      "2017-08-23T03:06:29.287741: step 122, loss 0.724621, acc 0.78\n",
      "current_step:  122\n",
      "2017-08-23T03:06:29.461159: step 123, loss 0.673819, acc 0.8\n",
      "current_step:  123\n",
      "2017-08-23T03:06:29.633915: step 124, loss 0.911626, acc 0.68\n",
      "current_step:  124\n",
      "2017-08-23T03:06:29.807111: step 125, loss 0.851581, acc 0.72\n",
      "current_step:  125\n",
      "2017-08-23T03:06:29.980710: step 126, loss 0.882742, acc 0.66\n",
      "current_step:  126\n",
      "2017-08-23T03:06:30.153509: step 127, loss 0.74622, acc 0.72\n",
      "current_step:  127\n",
      "2017-08-23T03:06:30.324713: step 128, loss 0.711185, acc 0.72\n",
      "current_step:  128\n",
      "2017-08-23T03:06:30.499016: step 129, loss 0.62655, acc 0.76\n",
      "current_step:  129\n",
      "2017-08-23T03:06:30.671903: step 130, loss 1.26431, acc 0.5\n",
      "current_step:  130\n",
      "2017-08-23T03:06:30.844707: step 131, loss 0.81857, acc 0.68\n",
      "current_step:  131\n",
      "2017-08-23T03:06:31.016689: step 132, loss 0.813357, acc 0.7\n",
      "current_step:  132\n",
      "2017-08-23T03:06:31.188461: step 133, loss 0.599994, acc 0.82\n",
      "current_step:  133\n",
      "2017-08-23T03:06:31.359538: step 134, loss 0.769189, acc 0.74\n",
      "current_step:  134\n",
      "2017-08-23T03:06:31.530494: step 135, loss 0.781443, acc 0.76\n",
      "current_step:  135\n",
      "2017-08-23T03:06:31.701300: step 136, loss 0.686308, acc 0.82\n",
      "current_step:  136\n",
      "2017-08-23T03:06:31.873337: step 137, loss 0.933977, acc 0.56\n",
      "current_step:  137\n",
      "2017-08-23T03:06:32.044200: step 138, loss 0.978354, acc 0.7\n",
      "current_step:  138\n",
      "2017-08-23T03:06:32.214457: step 139, loss 0.848963, acc 0.72\n",
      "current_step:  139\n",
      "2017-08-23T03:06:32.384972: step 140, loss 0.715303, acc 0.78\n",
      "current_step:  140\n",
      "2017-08-23T03:06:32.557098: step 141, loss 0.85476, acc 0.74\n",
      "current_step:  141\n",
      "2017-08-23T03:06:32.728221: step 142, loss 0.904518, acc 0.74\n",
      "current_step:  142\n",
      "2017-08-23T03:06:32.899797: step 143, loss 0.655959, acc 0.8\n",
      "current_step:  143\n",
      "2017-08-23T03:06:33.072226: step 144, loss 0.767023, acc 0.76\n",
      "current_step:  144\n",
      "2017-08-23T03:06:33.243089: step 145, loss 0.819676, acc 0.66\n",
      "current_step:  145\n",
      "2017-08-23T03:06:33.415279: step 146, loss 1.11326, acc 0.56\n",
      "current_step:  146\n",
      "2017-08-23T03:06:33.587318: step 147, loss 0.809955, acc 0.78\n",
      "current_step:  147\n",
      "2017-08-23T03:06:33.758973: step 148, loss 1.05694, acc 0.56\n",
      "current_step:  148\n",
      "2017-08-23T03:06:33.931325: step 149, loss 0.952354, acc 0.64\n",
      "current_step:  149\n",
      "2017-08-23T03:06:34.102911: step 150, loss 0.642845, acc 0.82\n",
      "current_step:  150\n",
      "2017-08-23T03:06:34.275572: step 151, loss 0.869376, acc 0.7\n",
      "current_step:  151\n",
      "2017-08-23T03:06:34.446712: step 152, loss 0.704029, acc 0.8\n",
      "current_step:  152\n",
      "2017-08-23T03:06:34.619862: step 153, loss 0.751362, acc 0.72\n",
      "current_step:  153\n",
      "2017-08-23T03:06:34.793222: step 154, loss 0.809154, acc 0.74\n",
      "current_step:  154\n",
      "2017-08-23T03:06:34.963371: step 155, loss 0.898033, acc 0.64\n",
      "current_step:  155\n",
      "2017-08-23T03:06:35.136372: step 156, loss 0.583569, acc 0.8\n",
      "current_step:  156\n",
      "2017-08-23T03:06:35.308244: step 157, loss 1.01892, acc 0.72\n",
      "current_step:  157\n",
      "2017-08-23T03:06:35.479006: step 158, loss 0.616934, acc 0.82\n",
      "current_step:  158\n",
      "2017-08-23T03:06:35.649840: step 159, loss 0.797934, acc 0.7\n",
      "current_step:  159\n",
      "2017-08-23T03:06:35.822193: step 160, loss 1.16525, acc 0.58\n",
      "current_step:  160\n",
      "2017-08-23T03:06:35.994878: step 161, loss 0.918809, acc 0.62\n",
      "current_step:  161\n",
      "2017-08-23T03:06:36.168072: step 162, loss 0.678667, acc 0.8\n",
      "current_step:  162\n",
      "2017-08-23T03:06:36.341725: step 163, loss 0.772711, acc 0.72\n",
      "current_step:  163\n",
      "2017-08-23T03:06:36.513094: step 164, loss 0.748456, acc 0.78\n",
      "current_step:  164\n",
      "2017-08-23T03:06:36.684969: step 165, loss 0.73637, acc 0.82\n",
      "current_step:  165\n",
      "2017-08-23T03:06:36.855182: step 166, loss 1.01751, acc 0.68\n",
      "current_step:  166\n",
      "2017-08-23T03:06:37.026823: step 167, loss 0.543365, acc 0.88\n",
      "current_step:  167\n",
      "2017-08-23T03:06:37.200685: step 168, loss 0.779336, acc 0.68\n",
      "current_step:  168\n",
      "2017-08-23T03:06:37.372575: step 169, loss 0.666666, acc 0.74\n",
      "current_step:  169\n",
      "2017-08-23T03:06:37.546214: step 170, loss 0.833136, acc 0.74\n",
      "current_step:  170\n",
      "2017-08-23T03:06:37.717915: step 171, loss 0.859854, acc 0.72\n",
      "current_step:  171\n",
      "2017-08-23T03:06:37.892148: step 172, loss 0.700441, acc 0.74\n",
      "current_step:  172\n",
      "2017-08-23T03:06:38.064434: step 173, loss 0.549901, acc 0.84\n",
      "current_step:  173\n",
      "2017-08-23T03:06:38.235499: step 174, loss 1.20608, acc 0.54\n",
      "current_step:  174\n",
      "2017-08-23T03:06:38.405082: step 175, loss 0.769408, acc 0.68\n",
      "current_step:  175\n",
      "2017-08-23T03:06:38.576445: step 176, loss 0.606243, acc 0.8\n",
      "current_step:  176\n",
      "2017-08-23T03:06:38.747124: step 177, loss 0.716415, acc 0.68\n",
      "current_step:  177\n",
      "2017-08-23T03:06:38.918471: step 178, loss 0.967807, acc 0.62\n",
      "current_step:  178\n",
      "2017-08-23T03:06:39.088533: step 179, loss 0.6491, acc 0.8\n",
      "current_step:  179\n",
      "2017-08-23T03:06:39.260376: step 180, loss 0.784946, acc 0.74\n",
      "current_step:  180\n",
      "2017-08-23T03:06:39.431621: step 181, loss 0.971693, acc 0.68\n",
      "current_step:  181\n",
      "2017-08-23T03:06:39.603205: step 182, loss 0.781601, acc 0.76\n",
      "current_step:  182\n",
      "2017-08-23T03:06:39.774092: step 183, loss 0.865485, acc 0.68\n",
      "current_step:  183\n",
      "2017-08-23T03:06:39.945642: step 184, loss 0.903703, acc 0.72\n",
      "current_step:  184\n",
      "2017-08-23T03:06:40.115494: step 185, loss 0.799147, acc 0.66\n",
      "current_step:  185\n",
      "2017-08-23T03:06:40.287583: step 186, loss 0.556374, acc 0.86\n",
      "current_step:  186\n",
      "2017-08-23T03:06:40.458887: step 187, loss 0.556689, acc 0.88\n",
      "current_step:  187\n",
      "2017-08-23T03:06:40.630155: step 188, loss 0.64405, acc 0.76\n",
      "current_step:  188\n",
      "2017-08-23T03:06:40.802650: step 189, loss 0.937778, acc 0.7\n",
      "current_step:  189\n",
      "2017-08-23T03:06:40.974654: step 190, loss 0.882258, acc 0.66\n",
      "current_step:  190\n",
      "2017-08-23T03:06:41.145161: step 191, loss 0.935125, acc 0.64\n",
      "current_step:  191\n",
      "2017-08-23T03:06:41.316500: step 192, loss 0.675123, acc 0.8\n",
      "current_step:  192\n",
      "2017-08-23T03:06:41.486966: step 193, loss 0.704289, acc 0.74\n",
      "current_step:  193\n",
      "2017-08-23T03:06:41.657539: step 194, loss 0.757766, acc 0.72\n",
      "current_step:  194\n",
      "2017-08-23T03:06:41.830638: step 195, loss 0.963538, acc 0.58\n",
      "current_step:  195\n",
      "2017-08-23T03:06:42.001499: step 196, loss 0.786825, acc 0.68\n",
      "current_step:  196\n",
      "2017-08-23T03:06:42.178476: step 197, loss 0.940693, acc 0.74\n",
      "current_step:  197\n",
      "2017-08-23T03:06:42.349574: step 198, loss 0.797278, acc 0.74\n",
      "current_step:  198\n",
      "2017-08-23T03:06:42.520004: step 199, loss 0.627784, acc 0.78\n",
      "current_step:  199\n",
      "2017-08-23T03:06:42.691365: step 200, loss 0.822713, acc 0.76\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:06:43.331457: step 200, loss 0.912553, acc 0.685053\n",
      "\n",
      "2017-08-23T03:06:43.503069: step 201, loss 0.770377, acc 0.72\n",
      "current_step:  201\n",
      "2017-08-23T03:06:43.676306: step 202, loss 0.828295, acc 0.7\n",
      "current_step:  202\n",
      "2017-08-23T03:06:43.849459: step 203, loss 0.710812, acc 0.76\n",
      "current_step:  203\n",
      "2017-08-23T03:06:43.885169: step 204, loss 0.580975, acc 0.777778\n",
      "current_step:  204\n",
      "2017-08-23T03:06:44.058613: step 205, loss 0.512427, acc 0.9\n",
      "current_step:  205\n",
      "2017-08-23T03:06:44.230281: step 206, loss 0.552475, acc 0.88\n",
      "current_step:  206\n",
      "2017-08-23T03:06:44.402609: step 207, loss 0.50366, acc 0.88\n",
      "current_step:  207\n",
      "2017-08-23T03:06:44.575108: step 208, loss 0.557068, acc 0.88\n",
      "current_step:  208\n",
      "2017-08-23T03:06:44.747352: step 209, loss 0.571442, acc 0.8\n",
      "current_step:  209\n",
      "2017-08-23T03:06:44.919934: step 210, loss 0.664009, acc 0.76\n",
      "current_step:  210\n",
      "2017-08-23T03:06:45.091231: step 211, loss 0.571725, acc 0.8\n",
      "current_step:  211\n",
      "2017-08-23T03:06:45.263255: step 212, loss 0.620605, acc 0.76\n",
      "current_step:  212\n",
      "2017-08-23T03:06:45.435491: step 213, loss 0.673148, acc 0.78\n",
      "current_step:  213\n",
      "2017-08-23T03:06:45.608994: step 214, loss 0.576619, acc 0.84\n",
      "current_step:  214\n",
      "2017-08-23T03:06:45.782865: step 215, loss 0.56794, acc 0.86\n",
      "current_step:  215\n",
      "2017-08-23T03:06:45.960252: step 216, loss 0.724248, acc 0.74\n",
      "current_step:  216\n",
      "2017-08-23T03:06:46.132495: step 217, loss 0.64316, acc 0.74\n",
      "current_step:  217\n",
      "2017-08-23T03:06:46.304061: step 218, loss 0.74874, acc 0.76\n",
      "current_step:  218\n",
      "2017-08-23T03:06:46.474928: step 219, loss 0.554534, acc 0.84\n",
      "current_step:  219\n",
      "2017-08-23T03:06:46.648581: step 220, loss 0.562126, acc 0.78\n",
      "current_step:  220\n",
      "2017-08-23T03:06:46.819872: step 221, loss 0.674378, acc 0.78\n",
      "current_step:  221\n",
      "2017-08-23T03:06:46.991253: step 222, loss 0.376933, acc 0.94\n",
      "current_step:  222\n",
      "2017-08-23T03:06:47.163969: step 223, loss 0.534229, acc 0.82\n",
      "current_step:  223\n",
      "2017-08-23T03:06:47.335738: step 224, loss 0.459549, acc 0.86\n",
      "current_step:  224\n",
      "2017-08-23T03:06:47.509177: step 225, loss 0.487816, acc 0.86\n",
      "current_step:  225\n",
      "2017-08-23T03:06:47.681946: step 226, loss 0.605492, acc 0.76\n",
      "current_step:  226\n",
      "2017-08-23T03:06:47.856538: step 227, loss 0.596391, acc 0.8\n",
      "current_step:  227\n",
      "2017-08-23T03:06:48.030408: step 228, loss 0.657301, acc 0.72\n",
      "current_step:  228\n",
      "2017-08-23T03:06:48.204728: step 229, loss 0.709431, acc 0.72\n",
      "current_step:  229\n",
      "2017-08-23T03:06:48.377466: step 230, loss 0.548351, acc 0.8\n",
      "current_step:  230\n",
      "2017-08-23T03:06:48.550839: step 231, loss 0.676907, acc 0.74\n",
      "current_step:  231\n",
      "2017-08-23T03:06:48.723271: step 232, loss 0.733674, acc 0.72\n",
      "current_step:  232\n",
      "2017-08-23T03:06:48.895410: step 233, loss 0.449605, acc 0.9\n",
      "current_step:  233\n",
      "2017-08-23T03:06:49.065995: step 234, loss 0.469895, acc 0.86\n",
      "current_step:  234\n",
      "2017-08-23T03:06:49.237550: step 235, loss 0.65743, acc 0.74\n",
      "current_step:  235\n",
      "2017-08-23T03:06:49.409573: step 236, loss 0.511008, acc 0.88\n",
      "current_step:  236\n",
      "2017-08-23T03:06:49.582762: step 237, loss 0.469738, acc 0.88\n",
      "current_step:  237\n",
      "2017-08-23T03:06:49.754457: step 238, loss 0.569163, acc 0.84\n",
      "current_step:  238\n",
      "2017-08-23T03:06:49.926091: step 239, loss 0.594678, acc 0.74\n",
      "current_step:  239\n",
      "2017-08-23T03:06:50.099298: step 240, loss 0.685186, acc 0.76\n",
      "current_step:  240\n",
      "2017-08-23T03:06:50.271764: step 241, loss 0.702434, acc 0.8\n",
      "current_step:  241\n",
      "2017-08-23T03:06:50.444679: step 242, loss 0.605802, acc 0.8\n",
      "current_step:  242\n",
      "2017-08-23T03:06:50.617173: step 243, loss 0.757578, acc 0.7\n",
      "current_step:  243\n",
      "2017-08-23T03:06:50.789092: step 244, loss 0.849099, acc 0.74\n",
      "current_step:  244\n",
      "2017-08-23T03:06:50.961388: step 245, loss 0.484583, acc 0.92\n",
      "current_step:  245\n",
      "2017-08-23T03:06:51.134463: step 246, loss 0.625192, acc 0.86\n",
      "current_step:  246\n",
      "2017-08-23T03:06:51.305451: step 247, loss 0.643697, acc 0.76\n",
      "current_step:  247\n",
      "2017-08-23T03:06:51.476974: step 248, loss 0.564846, acc 0.82\n",
      "current_step:  248\n",
      "2017-08-23T03:06:51.650800: step 249, loss 0.509698, acc 0.84\n",
      "current_step:  249\n",
      "2017-08-23T03:06:51.822280: step 250, loss 0.793029, acc 0.72\n",
      "current_step:  250\n",
      "2017-08-23T03:06:51.993984: step 251, loss 0.626307, acc 0.76\n",
      "current_step:  251\n",
      "2017-08-23T03:06:52.166525: step 252, loss 0.549679, acc 0.78\n",
      "current_step:  252\n",
      "2017-08-23T03:06:52.337720: step 253, loss 0.629817, acc 0.82\n",
      "current_step:  253\n",
      "2017-08-23T03:06:52.509962: step 254, loss 0.956697, acc 0.6\n",
      "current_step:  254\n",
      "2017-08-23T03:06:52.681361: step 255, loss 0.812055, acc 0.76\n",
      "current_step:  255\n",
      "2017-08-23T03:06:52.851746: step 256, loss 0.582271, acc 0.8\n",
      "current_step:  256\n",
      "2017-08-23T03:06:53.022268: step 257, loss 0.476908, acc 0.86\n",
      "current_step:  257\n",
      "2017-08-23T03:06:53.194970: step 258, loss 0.69471, acc 0.72\n",
      "current_step:  258\n",
      "2017-08-23T03:06:53.365243: step 259, loss 0.575805, acc 0.74\n",
      "current_step:  259\n",
      "2017-08-23T03:06:53.536454: step 260, loss 0.456049, acc 0.86\n",
      "current_step:  260\n",
      "2017-08-23T03:06:53.708788: step 261, loss 0.536396, acc 0.84\n",
      "current_step:  261\n",
      "2017-08-23T03:06:53.881451: step 262, loss 0.525744, acc 0.78\n",
      "current_step:  262\n",
      "2017-08-23T03:06:54.053324: step 263, loss 0.602479, acc 0.82\n",
      "current_step:  263\n",
      "2017-08-23T03:06:54.223696: step 264, loss 0.478088, acc 0.84\n",
      "current_step:  264\n",
      "2017-08-23T03:06:54.395803: step 265, loss 0.934715, acc 0.7\n",
      "current_step:  265\n",
      "2017-08-23T03:06:54.567468: step 266, loss 0.768778, acc 0.72\n",
      "current_step:  266\n",
      "2017-08-23T03:06:54.739979: step 267, loss 0.706903, acc 0.72\n",
      "current_step:  267\n",
      "2017-08-23T03:06:54.911549: step 268, loss 0.566944, acc 0.84\n",
      "current_step:  268\n",
      "2017-08-23T03:06:55.082221: step 269, loss 0.676936, acc 0.76\n",
      "current_step:  269\n",
      "2017-08-23T03:06:55.255607: step 270, loss 0.726941, acc 0.8\n",
      "current_step:  270\n",
      "2017-08-23T03:06:55.427483: step 271, loss 0.686711, acc 0.74\n",
      "current_step:  271\n",
      "2017-08-23T03:06:55.599398: step 272, loss 0.710436, acc 0.74\n",
      "current_step:  272\n",
      "2017-08-23T03:06:55.772079: step 273, loss 0.62569, acc 0.8\n",
      "current_step:  273\n",
      "2017-08-23T03:06:55.946078: step 274, loss 0.638502, acc 0.74\n",
      "current_step:  274\n",
      "2017-08-23T03:06:56.119997: step 275, loss 0.481412, acc 0.8\n",
      "current_step:  275\n",
      "2017-08-23T03:06:56.292563: step 276, loss 0.482513, acc 0.88\n",
      "current_step:  276\n",
      "2017-08-23T03:06:56.464534: step 277, loss 0.721286, acc 0.76\n",
      "current_step:  277\n",
      "2017-08-23T03:06:56.635583: step 278, loss 0.465634, acc 0.88\n",
      "current_step:  278\n",
      "2017-08-23T03:06:56.807011: step 279, loss 0.638865, acc 0.8\n",
      "current_step:  279\n",
      "2017-08-23T03:06:56.978979: step 280, loss 0.595149, acc 0.84\n",
      "current_step:  280\n",
      "2017-08-23T03:06:57.150760: step 281, loss 0.635458, acc 0.82\n",
      "current_step:  281\n",
      "2017-08-23T03:06:57.321834: step 282, loss 0.619339, acc 0.76\n",
      "current_step:  282\n",
      "2017-08-23T03:06:57.494036: step 283, loss 0.646811, acc 0.8\n",
      "current_step:  283\n",
      "2017-08-23T03:06:57.664743: step 284, loss 0.669601, acc 0.76\n",
      "current_step:  284\n",
      "2017-08-23T03:06:57.836143: step 285, loss 0.600514, acc 0.82\n",
      "current_step:  285\n",
      "2017-08-23T03:06:58.007498: step 286, loss 0.698572, acc 0.78\n",
      "current_step:  286\n",
      "2017-08-23T03:06:58.177801: step 287, loss 0.537908, acc 0.82\n",
      "current_step:  287\n",
      "2017-08-23T03:06:58.348479: step 288, loss 0.428572, acc 0.92\n",
      "current_step:  288\n",
      "2017-08-23T03:06:58.518159: step 289, loss 0.832989, acc 0.68\n",
      "current_step:  289\n",
      "2017-08-23T03:06:58.689154: step 290, loss 0.565993, acc 0.84\n",
      "current_step:  290\n",
      "2017-08-23T03:06:58.875349: step 291, loss 0.712394, acc 0.7\n",
      "current_step:  291\n",
      "2017-08-23T03:06:59.050595: step 292, loss 0.607538, acc 0.84\n",
      "current_step:  292\n",
      "2017-08-23T03:06:59.224862: step 293, loss 0.536041, acc 0.82\n",
      "current_step:  293\n",
      "2017-08-23T03:06:59.399153: step 294, loss 0.677918, acc 0.78\n",
      "current_step:  294\n",
      "2017-08-23T03:06:59.571754: step 295, loss 0.624242, acc 0.78\n",
      "current_step:  295\n",
      "2017-08-23T03:06:59.744184: step 296, loss 0.684332, acc 0.74\n",
      "current_step:  296\n",
      "2017-08-23T03:06:59.916517: step 297, loss 0.620258, acc 0.86\n",
      "current_step:  297\n",
      "2017-08-23T03:07:00.088504: step 298, loss 0.715968, acc 0.8\n",
      "current_step:  298\n",
      "2017-08-23T03:07:00.260669: step 299, loss 0.571597, acc 0.88\n",
      "current_step:  299\n",
      "2017-08-23T03:07:00.436121: step 300, loss 0.669381, acc 0.8\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:07:01.084732: step 300, loss 0.848179, acc 0.708185\n",
      "\n",
      "2017-08-23T03:07:01.256489: step 301, loss 0.537882, acc 0.84\n",
      "current_step:  301\n",
      "2017-08-23T03:07:01.427957: step 302, loss 0.761957, acc 0.7\n",
      "current_step:  302\n",
      "2017-08-23T03:07:01.600031: step 303, loss 0.499258, acc 0.88\n",
      "current_step:  303\n",
      "2017-08-23T03:07:01.771678: step 304, loss 0.633919, acc 0.84\n",
      "current_step:  304\n",
      "2017-08-23T03:07:01.945597: step 305, loss 0.48417, acc 0.84\n",
      "current_step:  305\n",
      "2017-08-23T03:07:01.982403: step 306, loss 0.705345, acc 0.777778\n",
      "current_step:  306\n",
      "2017-08-23T03:07:02.154922: step 307, loss 0.364925, acc 0.92\n",
      "current_step:  307\n",
      "2017-08-23T03:07:02.327050: step 308, loss 0.379614, acc 0.88\n",
      "current_step:  308\n",
      "2017-08-23T03:07:02.498528: step 309, loss 0.52163, acc 0.84\n",
      "current_step:  309\n",
      "2017-08-23T03:07:02.669771: step 310, loss 0.478214, acc 0.9\n",
      "current_step:  310\n",
      "2017-08-23T03:07:02.841215: step 311, loss 0.54999, acc 0.82\n",
      "current_step:  311\n",
      "2017-08-23T03:07:03.013369: step 312, loss 0.367832, acc 0.94\n",
      "current_step:  312\n",
      "2017-08-23T03:07:03.184063: step 313, loss 0.394075, acc 0.92\n",
      "current_step:  313\n",
      "2017-08-23T03:07:03.355143: step 314, loss 0.408061, acc 0.9\n",
      "current_step:  314\n",
      "2017-08-23T03:07:03.525667: step 315, loss 0.461409, acc 0.86\n",
      "current_step:  315\n",
      "2017-08-23T03:07:03.696401: step 316, loss 0.410755, acc 0.88\n",
      "current_step:  316\n",
      "2017-08-23T03:07:03.867952: step 317, loss 0.44937, acc 0.88\n",
      "current_step:  317\n",
      "2017-08-23T03:07:04.039437: step 318, loss 0.457589, acc 0.86\n",
      "current_step:  318\n",
      "2017-08-23T03:07:04.211660: step 319, loss 0.306512, acc 0.96\n",
      "current_step:  319\n",
      "2017-08-23T03:07:04.383174: step 320, loss 0.497275, acc 0.82\n",
      "current_step:  320\n",
      "2017-08-23T03:07:04.554411: step 321, loss 0.460049, acc 0.86\n",
      "current_step:  321\n",
      "2017-08-23T03:07:04.725573: step 322, loss 0.405304, acc 0.94\n",
      "current_step:  322\n",
      "2017-08-23T03:07:04.897444: step 323, loss 0.356084, acc 0.92\n",
      "current_step:  323\n",
      "2017-08-23T03:07:05.068551: step 324, loss 0.529984, acc 0.88\n",
      "current_step:  324\n",
      "2017-08-23T03:07:05.240384: step 325, loss 0.360691, acc 0.9\n",
      "current_step:  325\n",
      "2017-08-23T03:07:05.412738: step 326, loss 0.358946, acc 0.9\n",
      "current_step:  326\n",
      "2017-08-23T03:07:05.583495: step 327, loss 0.439549, acc 0.88\n",
      "current_step:  327\n",
      "2017-08-23T03:07:05.755117: step 328, loss 0.372453, acc 0.92\n",
      "current_step:  328\n",
      "2017-08-23T03:07:05.928576: step 329, loss 0.48071, acc 0.84\n",
      "current_step:  329\n",
      "2017-08-23T03:07:06.101337: step 330, loss 0.393506, acc 0.88\n",
      "current_step:  330\n",
      "2017-08-23T03:07:06.273540: step 331, loss 0.537275, acc 0.76\n",
      "current_step:  331\n",
      "2017-08-23T03:07:06.444687: step 332, loss 0.421141, acc 0.88\n",
      "current_step:  332\n",
      "2017-08-23T03:07:06.617009: step 333, loss 0.495122, acc 0.88\n",
      "current_step:  333\n",
      "2017-08-23T03:07:06.788042: step 334, loss 0.49815, acc 0.84\n",
      "current_step:  334\n",
      "2017-08-23T03:07:06.960349: step 335, loss 0.441229, acc 0.84\n",
      "current_step:  335\n",
      "2017-08-23T03:07:07.133051: step 336, loss 0.51454, acc 0.84\n",
      "current_step:  336\n",
      "2017-08-23T03:07:07.304272: step 337, loss 0.596473, acc 0.82\n",
      "current_step:  337\n",
      "2017-08-23T03:07:07.476828: step 338, loss 0.525596, acc 0.86\n",
      "current_step:  338\n",
      "2017-08-23T03:07:07.650218: step 339, loss 0.430052, acc 0.88\n",
      "current_step:  339\n",
      "2017-08-23T03:07:07.822916: step 340, loss 0.322044, acc 0.94\n",
      "current_step:  340\n",
      "2017-08-23T03:07:07.999536: step 341, loss 0.477091, acc 0.84\n",
      "current_step:  341\n",
      "2017-08-23T03:07:08.171517: step 342, loss 0.45364, acc 0.9\n",
      "current_step:  342\n",
      "2017-08-23T03:07:08.342433: step 343, loss 0.371041, acc 0.86\n",
      "current_step:  343\n",
      "2017-08-23T03:07:08.512402: step 344, loss 0.586714, acc 0.86\n",
      "current_step:  344\n",
      "2017-08-23T03:07:08.684622: step 345, loss 0.446883, acc 0.86\n",
      "current_step:  345\n",
      "2017-08-23T03:07:08.856467: step 346, loss 0.373339, acc 0.92\n",
      "current_step:  346\n",
      "2017-08-23T03:07:09.028877: step 347, loss 0.407767, acc 0.9\n",
      "current_step:  347\n",
      "2017-08-23T03:07:09.198799: step 348, loss 0.445702, acc 0.88\n",
      "current_step:  348\n",
      "2017-08-23T03:07:09.370343: step 349, loss 0.370143, acc 0.9\n",
      "current_step:  349\n",
      "2017-08-23T03:07:09.542277: step 350, loss 0.534311, acc 0.86\n",
      "current_step:  350\n",
      "2017-08-23T03:07:09.713605: step 351, loss 0.426037, acc 0.86\n",
      "current_step:  351\n",
      "2017-08-23T03:07:09.884298: step 352, loss 0.464515, acc 0.82\n",
      "current_step:  352\n",
      "2017-08-23T03:07:10.055637: step 353, loss 0.408137, acc 0.96\n",
      "current_step:  353\n",
      "2017-08-23T03:07:10.226636: step 354, loss 0.474554, acc 0.84\n",
      "current_step:  354\n",
      "2017-08-23T03:07:10.397498: step 355, loss 0.437525, acc 0.9\n",
      "current_step:  355\n",
      "2017-08-23T03:07:10.568216: step 356, loss 0.571025, acc 0.84\n",
      "current_step:  356\n",
      "2017-08-23T03:07:10.738941: step 357, loss 0.616332, acc 0.78\n",
      "current_step:  357\n",
      "2017-08-23T03:07:10.910188: step 358, loss 0.397754, acc 0.94\n",
      "current_step:  358\n",
      "2017-08-23T03:07:11.080012: step 359, loss 0.545109, acc 0.84\n",
      "current_step:  359\n",
      "2017-08-23T03:07:11.251011: step 360, loss 0.403742, acc 0.88\n",
      "current_step:  360\n",
      "2017-08-23T03:07:11.422542: step 361, loss 0.443123, acc 0.86\n",
      "current_step:  361\n",
      "2017-08-23T03:07:11.594234: step 362, loss 0.654164, acc 0.8\n",
      "current_step:  362\n",
      "2017-08-23T03:07:11.766946: step 363, loss 0.591145, acc 0.8\n",
      "current_step:  363\n",
      "2017-08-23T03:07:11.938060: step 364, loss 0.405023, acc 0.86\n",
      "current_step:  364\n",
      "2017-08-23T03:07:12.109019: step 365, loss 0.407668, acc 0.88\n",
      "current_step:  365\n",
      "2017-08-23T03:07:12.280836: step 366, loss 0.462392, acc 0.92\n",
      "current_step:  366\n",
      "2017-08-23T03:07:12.451933: step 367, loss 0.594992, acc 0.84\n",
      "current_step:  367\n",
      "2017-08-23T03:07:12.625212: step 368, loss 0.518576, acc 0.86\n",
      "current_step:  368\n",
      "2017-08-23T03:07:12.794724: step 369, loss 0.687562, acc 0.8\n",
      "current_step:  369\n",
      "2017-08-23T03:07:12.964942: step 370, loss 0.501396, acc 0.84\n",
      "current_step:  370\n",
      "2017-08-23T03:07:13.135982: step 371, loss 0.446512, acc 0.9\n",
      "current_step:  371\n",
      "2017-08-23T03:07:13.306648: step 372, loss 0.437225, acc 0.9\n",
      "current_step:  372\n",
      "2017-08-23T03:07:13.478088: step 373, loss 0.631838, acc 0.76\n",
      "current_step:  373\n",
      "2017-08-23T03:07:13.649202: step 374, loss 0.329033, acc 0.94\n",
      "current_step:  374\n",
      "2017-08-23T03:07:13.818940: step 375, loss 0.683792, acc 0.74\n",
      "current_step:  375\n",
      "2017-08-23T03:07:13.990515: step 376, loss 0.529657, acc 0.82\n",
      "current_step:  376\n",
      "2017-08-23T03:07:14.162213: step 377, loss 0.7182, acc 0.72\n",
      "current_step:  377\n",
      "2017-08-23T03:07:14.332762: step 378, loss 0.5706, acc 0.8\n",
      "current_step:  378\n",
      "2017-08-23T03:07:14.503625: step 379, loss 0.65034, acc 0.74\n",
      "current_step:  379\n",
      "2017-08-23T03:07:14.675798: step 380, loss 0.67758, acc 0.76\n",
      "current_step:  380\n",
      "2017-08-23T03:07:14.846197: step 381, loss 0.523324, acc 0.86\n",
      "current_step:  381\n",
      "2017-08-23T03:07:15.018706: step 382, loss 0.45562, acc 0.84\n",
      "current_step:  382\n",
      "2017-08-23T03:07:15.188434: step 383, loss 0.567947, acc 0.8\n",
      "current_step:  383\n",
      "2017-08-23T03:07:15.359822: step 384, loss 0.325682, acc 0.92\n",
      "current_step:  384\n",
      "2017-08-23T03:07:15.531865: step 385, loss 0.443031, acc 0.88\n",
      "current_step:  385\n",
      "2017-08-23T03:07:15.703809: step 386, loss 0.448043, acc 0.84\n",
      "current_step:  386\n",
      "2017-08-23T03:07:15.875871: step 387, loss 0.435192, acc 0.86\n",
      "current_step:  387\n",
      "2017-08-23T03:07:16.047957: step 388, loss 0.582668, acc 0.74\n",
      "current_step:  388\n",
      "2017-08-23T03:07:16.219893: step 389, loss 0.457138, acc 0.9\n",
      "current_step:  389\n",
      "2017-08-23T03:07:16.391751: step 390, loss 0.371939, acc 0.92\n",
      "current_step:  390\n",
      "2017-08-23T03:07:16.563185: step 391, loss 0.381567, acc 0.88\n",
      "current_step:  391\n",
      "2017-08-23T03:07:16.735025: step 392, loss 0.400786, acc 0.88\n",
      "current_step:  392\n",
      "2017-08-23T03:07:16.905860: step 393, loss 0.498353, acc 0.82\n",
      "current_step:  393\n",
      "2017-08-23T03:07:17.077573: step 394, loss 0.648675, acc 0.8\n",
      "current_step:  394\n",
      "2017-08-23T03:07:17.248361: step 395, loss 0.60715, acc 0.84\n",
      "current_step:  395\n",
      "2017-08-23T03:07:17.419926: step 396, loss 0.357865, acc 0.86\n",
      "current_step:  396\n",
      "2017-08-23T03:07:17.590058: step 397, loss 0.622135, acc 0.82\n",
      "current_step:  397\n",
      "2017-08-23T03:07:17.761279: step 398, loss 0.575401, acc 0.8\n",
      "current_step:  398\n",
      "2017-08-23T03:07:17.931307: step 399, loss 0.560879, acc 0.82\n",
      "current_step:  399\n",
      "2017-08-23T03:07:18.103493: step 400, loss 0.36054, acc 0.96\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:07:18.744623: step 400, loss 0.911435, acc 0.690391\n",
      "\n",
      "2017-08-23T03:07:18.915816: step 401, loss 0.457554, acc 0.86\n",
      "current_step:  401\n",
      "2017-08-23T03:07:19.086938: step 402, loss 0.58381, acc 0.82\n",
      "current_step:  402\n",
      "2017-08-23T03:07:19.257582: step 403, loss 0.579551, acc 0.78\n",
      "current_step:  403\n",
      "2017-08-23T03:07:19.428866: step 404, loss 0.481708, acc 0.9\n",
      "current_step:  404\n",
      "2017-08-23T03:07:19.602286: step 405, loss 0.590883, acc 0.82\n",
      "current_step:  405\n",
      "2017-08-23T03:07:19.774861: step 406, loss 0.462137, acc 0.84\n",
      "current_step:  406\n",
      "2017-08-23T03:07:19.947458: step 407, loss 0.619007, acc 0.78\n",
      "current_step:  407\n",
      "2017-08-23T03:07:19.982137: step 408, loss 0.617633, acc 0.777778\n",
      "current_step:  408\n",
      "2017-08-23T03:07:20.153573: step 409, loss 0.370643, acc 0.9\n",
      "current_step:  409\n",
      "2017-08-23T03:07:20.324092: step 410, loss 0.359482, acc 0.94\n",
      "current_step:  410\n",
      "2017-08-23T03:07:20.494056: step 411, loss 0.305339, acc 0.98\n",
      "current_step:  411\n",
      "2017-08-23T03:07:20.665916: step 412, loss 0.265929, acc 0.96\n",
      "current_step:  412\n",
      "2017-08-23T03:07:20.838436: step 413, loss 0.412418, acc 0.92\n",
      "current_step:  413\n",
      "2017-08-23T03:07:21.010315: step 414, loss 0.414538, acc 0.86\n",
      "current_step:  414\n",
      "2017-08-23T03:07:21.181270: step 415, loss 0.425677, acc 0.86\n",
      "current_step:  415\n",
      "2017-08-23T03:07:21.353667: step 416, loss 0.344465, acc 0.94\n",
      "current_step:  416\n",
      "2017-08-23T03:07:21.526695: step 417, loss 0.413294, acc 0.84\n",
      "current_step:  417\n",
      "2017-08-23T03:07:21.698081: step 418, loss 0.266401, acc 0.96\n",
      "current_step:  418\n",
      "2017-08-23T03:07:21.870046: step 419, loss 0.541546, acc 0.88\n",
      "current_step:  419\n",
      "2017-08-23T03:07:22.041478: step 420, loss 0.463014, acc 0.84\n",
      "current_step:  420\n",
      "2017-08-23T03:07:22.212780: step 421, loss 0.490252, acc 0.9\n",
      "current_step:  421\n",
      "2017-08-23T03:07:22.384540: step 422, loss 0.338871, acc 0.94\n",
      "current_step:  422\n",
      "2017-08-23T03:07:22.557219: step 423, loss 0.492544, acc 0.84\n",
      "current_step:  423\n",
      "2017-08-23T03:07:22.729046: step 424, loss 0.407311, acc 0.92\n",
      "current_step:  424\n",
      "2017-08-23T03:07:22.901043: step 425, loss 0.321164, acc 0.94\n",
      "current_step:  425\n",
      "2017-08-23T03:07:23.073281: step 426, loss 0.311284, acc 0.96\n",
      "current_step:  426\n",
      "2017-08-23T03:07:23.245328: step 427, loss 0.222597, acc 0.98\n",
      "current_step:  427\n",
      "2017-08-23T03:07:23.417950: step 428, loss 0.506789, acc 0.84\n",
      "current_step:  428\n",
      "2017-08-23T03:07:23.590162: step 429, loss 0.37087, acc 0.88\n",
      "current_step:  429\n",
      "2017-08-23T03:07:23.761535: step 430, loss 0.41347, acc 0.92\n",
      "current_step:  430\n",
      "2017-08-23T03:07:23.934362: step 431, loss 0.309501, acc 0.92\n",
      "current_step:  431\n",
      "2017-08-23T03:07:24.105848: step 432, loss 0.327134, acc 0.92\n",
      "current_step:  432\n",
      "2017-08-23T03:07:24.277032: step 433, loss 0.314688, acc 0.94\n",
      "current_step:  433\n",
      "2017-08-23T03:07:24.449377: step 434, loss 0.54138, acc 0.8\n",
      "current_step:  434\n",
      "2017-08-23T03:07:24.623945: step 435, loss 0.470851, acc 0.88\n",
      "current_step:  435\n",
      "2017-08-23T03:07:24.796505: step 436, loss 0.38125, acc 0.88\n",
      "current_step:  436\n",
      "2017-08-23T03:07:24.967801: step 437, loss 0.33503, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-23T03:07:25.140117: step 438, loss 0.336895, acc 0.94\n",
      "current_step:  438\n",
      "2017-08-23T03:07:25.311326: step 439, loss 0.490062, acc 0.86\n",
      "current_step:  439\n",
      "2017-08-23T03:07:25.485129: step 440, loss 0.438948, acc 0.88\n",
      "current_step:  440\n",
      "2017-08-23T03:07:25.657249: step 441, loss 0.318851, acc 0.92\n",
      "current_step:  441\n",
      "2017-08-23T03:07:25.829660: step 442, loss 0.343867, acc 0.92\n",
      "current_step:  442\n",
      "2017-08-23T03:07:26.001361: step 443, loss 0.362085, acc 0.94\n",
      "current_step:  443\n",
      "2017-08-23T03:07:26.173140: step 444, loss 0.458337, acc 0.88\n",
      "current_step:  444\n",
      "2017-08-23T03:07:26.344685: step 445, loss 0.379951, acc 0.9\n",
      "current_step:  445\n",
      "2017-08-23T03:07:26.519233: step 446, loss 0.263161, acc 0.94\n",
      "current_step:  446\n",
      "2017-08-23T03:07:26.692768: step 447, loss 0.264282, acc 0.92\n",
      "current_step:  447\n",
      "2017-08-23T03:07:26.866376: step 448, loss 0.310251, acc 0.92\n",
      "current_step:  448\n",
      "2017-08-23T03:07:27.039095: step 449, loss 0.371873, acc 0.9\n",
      "current_step:  449\n",
      "2017-08-23T03:07:27.213335: step 450, loss 0.389133, acc 0.84\n",
      "current_step:  450\n",
      "2017-08-23T03:07:27.384756: step 451, loss 0.363087, acc 0.9\n",
      "current_step:  451\n",
      "2017-08-23T03:07:27.556049: step 452, loss 0.27379, acc 0.96\n",
      "current_step:  452\n",
      "2017-08-23T03:07:27.729260: step 453, loss 0.388904, acc 0.9\n",
      "current_step:  453\n",
      "2017-08-23T03:07:27.901174: step 454, loss 0.44664, acc 0.88\n",
      "current_step:  454\n",
      "2017-08-23T03:07:28.072828: step 455, loss 0.483374, acc 0.84\n",
      "current_step:  455\n",
      "2017-08-23T03:07:28.245023: step 456, loss 0.40207, acc 0.88\n",
      "current_step:  456\n",
      "2017-08-23T03:07:28.416728: step 457, loss 0.464176, acc 0.88\n",
      "current_step:  457\n",
      "2017-08-23T03:07:28.588186: step 458, loss 0.27591, acc 0.94\n",
      "current_step:  458\n",
      "2017-08-23T03:07:28.758848: step 459, loss 0.332044, acc 0.98\n",
      "current_step:  459\n",
      "2017-08-23T03:07:28.930133: step 460, loss 0.240953, acc 0.96\n",
      "current_step:  460\n",
      "2017-08-23T03:07:29.102400: step 461, loss 0.459746, acc 0.88\n",
      "current_step:  461\n",
      "2017-08-23T03:07:29.274608: step 462, loss 0.323043, acc 0.96\n",
      "current_step:  462\n",
      "2017-08-23T03:07:29.446804: step 463, loss 0.390127, acc 0.84\n",
      "current_step:  463\n",
      "2017-08-23T03:07:29.618898: step 464, loss 0.336379, acc 0.94\n",
      "current_step:  464\n",
      "2017-08-23T03:07:29.790099: step 465, loss 0.295166, acc 0.94\n",
      "current_step:  465\n",
      "2017-08-23T03:07:29.960820: step 466, loss 0.364435, acc 0.9\n",
      "current_step:  466\n",
      "2017-08-23T03:07:30.131928: step 467, loss 0.463174, acc 0.84\n",
      "current_step:  467\n",
      "2017-08-23T03:07:30.302955: step 468, loss 0.314524, acc 0.92\n",
      "current_step:  468\n",
      "2017-08-23T03:07:30.474296: step 469, loss 0.469497, acc 0.82\n",
      "current_step:  469\n",
      "2017-08-23T03:07:30.646141: step 470, loss 0.347352, acc 0.92\n",
      "current_step:  470\n",
      "2017-08-23T03:07:30.816914: step 471, loss 0.427264, acc 0.92\n",
      "current_step:  471\n",
      "2017-08-23T03:07:30.990463: step 472, loss 0.418868, acc 0.86\n",
      "current_step:  472\n",
      "2017-08-23T03:07:31.162296: step 473, loss 0.213953, acc 0.96\n",
      "current_step:  473\n",
      "2017-08-23T03:07:31.334188: step 474, loss 0.326462, acc 0.94\n",
      "current_step:  474\n",
      "2017-08-23T03:07:31.506216: step 475, loss 0.378632, acc 0.9\n",
      "current_step:  475\n",
      "2017-08-23T03:07:31.678546: step 476, loss 0.481909, acc 0.86\n",
      "current_step:  476\n",
      "2017-08-23T03:07:31.849864: step 477, loss 0.328706, acc 0.94\n",
      "current_step:  477\n",
      "2017-08-23T03:07:32.023758: step 478, loss 0.270511, acc 0.98\n",
      "current_step:  478\n",
      "2017-08-23T03:07:32.194430: step 479, loss 0.35688, acc 0.94\n",
      "current_step:  479\n",
      "2017-08-23T03:07:32.365287: step 480, loss 0.376304, acc 0.94\n",
      "current_step:  480\n",
      "2017-08-23T03:07:32.537149: step 481, loss 0.260822, acc 0.96\n",
      "current_step:  481\n",
      "2017-08-23T03:07:32.708524: step 482, loss 0.434328, acc 0.86\n",
      "current_step:  482\n",
      "2017-08-23T03:07:32.878623: step 483, loss 0.311524, acc 0.96\n",
      "current_step:  483\n",
      "2017-08-23T03:07:33.050919: step 484, loss 0.235544, acc 0.96\n",
      "current_step:  484\n",
      "2017-08-23T03:07:33.222653: step 485, loss 0.46046, acc 0.84\n",
      "current_step:  485\n",
      "2017-08-23T03:07:33.392760: step 486, loss 0.366376, acc 0.94\n",
      "current_step:  486\n",
      "2017-08-23T03:07:33.564283: step 487, loss 0.451379, acc 0.86\n",
      "current_step:  487\n",
      "2017-08-23T03:07:33.734947: step 488, loss 0.372816, acc 0.88\n",
      "current_step:  488\n",
      "2017-08-23T03:07:33.906253: step 489, loss 0.333275, acc 0.94\n",
      "current_step:  489\n",
      "2017-08-23T03:07:34.077314: step 490, loss 0.451956, acc 0.84\n",
      "current_step:  490\n",
      "2017-08-23T03:07:34.248063: step 491, loss 0.402893, acc 0.92\n",
      "current_step:  491\n",
      "2017-08-23T03:07:34.422695: step 492, loss 0.467017, acc 0.88\n",
      "current_step:  492\n",
      "2017-08-23T03:07:34.594267: step 493, loss 0.403275, acc 0.88\n",
      "current_step:  493\n",
      "2017-08-23T03:07:34.767394: step 494, loss 0.46316, acc 0.86\n",
      "current_step:  494\n",
      "2017-08-23T03:07:34.940370: step 495, loss 0.275068, acc 0.92\n",
      "current_step:  495\n",
      "2017-08-23T03:07:35.110946: step 496, loss 0.340779, acc 0.96\n",
      "current_step:  496\n",
      "2017-08-23T03:07:35.281753: step 497, loss 0.356707, acc 0.88\n",
      "current_step:  497\n",
      "2017-08-23T03:07:35.454705: step 498, loss 0.223364, acc 0.96\n",
      "current_step:  498\n",
      "2017-08-23T03:07:35.624390: step 499, loss 0.59592, acc 0.8\n",
      "current_step:  499\n",
      "2017-08-23T03:07:35.795758: step 500, loss 0.430083, acc 0.82\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:07:36.425417: step 500, loss 0.830132, acc 0.718861\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0306_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:07:36.668050: step 501, loss 0.372428, acc 0.9\n",
      "current_step:  501\n",
      "2017-08-23T03:07:36.839686: step 502, loss 0.333922, acc 0.96\n",
      "current_step:  502\n",
      "2017-08-23T03:07:37.011207: step 503, loss 0.467755, acc 0.88\n",
      "current_step:  503\n",
      "2017-08-23T03:07:37.185227: step 504, loss 0.34761, acc 0.92\n",
      "current_step:  504\n",
      "2017-08-23T03:07:37.356525: step 505, loss 0.415542, acc 0.9\n",
      "current_step:  505\n",
      "2017-08-23T03:07:37.528588: step 506, loss 0.488066, acc 0.86\n",
      "current_step:  506\n",
      "2017-08-23T03:07:37.700461: step 507, loss 0.396881, acc 0.94\n",
      "current_step:  507\n",
      "2017-08-23T03:07:37.871866: step 508, loss 0.334971, acc 0.94\n",
      "current_step:  508\n",
      "2017-08-23T03:07:38.044229: step 509, loss 0.575592, acc 0.84\n",
      "current_step:  509\n",
      "2017-08-23T03:07:38.079938: step 510, loss 0.191714, acc 1\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.625772\n",
      "Precision Score = 0.608762\n",
      "Recall Score = 0.654804\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.650158\n",
      "Precision Score = 0.636404\n",
      "Recall Score = 0.676157\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.663837\n",
      "Precision Score = 0.647427\n",
      "Recall Score = 0.701068\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.653732\n",
      "Precision Score = 0.659073\n",
      "Recall Score = 0.681495\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.676723\n",
      "Precision Score = 0.668022\n",
      "Recall Score = 0.711744\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v2 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 2, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  264\n",
      "Vocabulary Size: 6361\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0308_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0308_UTC\n",
      "2017-08-23T03:08:00.826807: step 1, loss 3.4203, acc 0.08\n",
      "current_step:  1\n",
      "2017-08-23T03:08:00.914325: step 2, loss 2.46566, acc 0.1\n",
      "current_step:  2\n",
      "2017-08-23T03:08:00.989778: step 3, loss 1.66181, acc 0.28\n",
      "current_step:  3\n",
      "2017-08-23T03:08:01.061591: step 4, loss 1.3752, acc 0.38\n",
      "current_step:  4\n",
      "2017-08-23T03:08:01.134203: step 5, loss 1.45375, acc 0.52\n",
      "current_step:  5\n",
      "2017-08-23T03:08:01.212795: step 6, loss 1.27894, acc 0.56\n",
      "current_step:  6\n",
      "2017-08-23T03:08:01.291391: step 7, loss 1.71936, acc 0.5\n",
      "current_step:  7\n",
      "2017-08-23T03:08:01.367442: step 8, loss 2.21025, acc 0.42\n",
      "current_step:  8\n",
      "2017-08-23T03:08:01.442047: step 9, loss 1.63909, acc 0.44\n",
      "current_step:  9\n",
      "2017-08-23T03:08:01.521980: step 10, loss 1.5879, acc 0.36\n",
      "current_step:  10\n",
      "2017-08-23T03:08:01.605576: step 11, loss 1.54405, acc 0.52\n",
      "current_step:  11\n",
      "2017-08-23T03:08:01.678700: step 12, loss 1.80197, acc 0.4\n",
      "current_step:  12\n",
      "2017-08-23T03:08:01.757727: step 13, loss 1.86247, acc 0.44\n",
      "current_step:  13\n",
      "2017-08-23T03:08:01.836501: step 14, loss 1.49235, acc 0.52\n",
      "current_step:  14\n",
      "2017-08-23T03:08:01.917745: step 15, loss 1.51704, acc 0.48\n",
      "current_step:  15\n",
      "2017-08-23T03:08:01.999504: step 16, loss 2.03113, acc 0.32\n",
      "current_step:  16\n",
      "2017-08-23T03:08:02.077596: step 17, loss 1.80795, acc 0.38\n",
      "current_step:  17\n",
      "2017-08-23T03:08:02.151370: step 18, loss 1.29683, acc 0.62\n",
      "current_step:  18\n",
      "2017-08-23T03:08:02.229289: step 19, loss 1.58484, acc 0.36\n",
      "current_step:  19\n",
      "2017-08-23T03:08:02.299866: step 20, loss 1.01731, acc 0.6\n",
      "current_step:  20\n",
      "2017-08-23T03:08:02.380225: step 21, loss 1.70344, acc 0.48\n",
      "current_step:  21\n",
      "2017-08-23T03:08:02.456626: step 22, loss 1.97304, acc 0.44\n",
      "current_step:  22\n",
      "2017-08-23T03:08:02.531888: step 23, loss 1.18035, acc 0.56\n",
      "current_step:  23\n",
      "2017-08-23T03:08:02.603244: step 24, loss 1.23724, acc 0.64\n",
      "current_step:  24\n",
      "2017-08-23T03:08:02.677232: step 25, loss 1.04417, acc 0.58\n",
      "current_step:  25\n",
      "2017-08-23T03:08:02.774365: step 26, loss 0.939522, acc 0.66\n",
      "current_step:  26\n",
      "2017-08-23T03:08:02.886641: step 27, loss 1.14679, acc 0.62\n",
      "current_step:  27\n",
      "2017-08-23T03:08:02.999253: step 28, loss 1.19419, acc 0.6\n",
      "current_step:  28\n",
      "2017-08-23T03:08:03.112101: step 29, loss 1.16799, acc 0.64\n",
      "current_step:  29\n",
      "2017-08-23T03:08:03.224898: step 30, loss 1.1058, acc 0.56\n",
      "current_step:  30\n",
      "2017-08-23T03:08:03.337118: step 31, loss 1.34397, acc 0.48\n",
      "current_step:  31\n",
      "2017-08-23T03:08:03.449955: step 32, loss 1.39198, acc 0.44\n",
      "current_step:  32\n",
      "2017-08-23T03:08:03.561981: step 33, loss 1.06132, acc 0.54\n",
      "current_step:  33\n",
      "2017-08-23T03:08:03.674686: step 34, loss 1.12035, acc 0.64\n",
      "current_step:  34\n",
      "2017-08-23T03:08:03.787154: step 35, loss 1.17389, acc 0.54\n",
      "current_step:  35\n",
      "2017-08-23T03:08:03.900067: step 36, loss 1.18489, acc 0.66\n",
      "current_step:  36\n",
      "2017-08-23T03:08:04.011860: step 37, loss 1.09975, acc 0.5\n",
      "current_step:  37\n",
      "2017-08-23T03:08:04.124224: step 38, loss 1.3998, acc 0.52\n",
      "current_step:  38\n",
      "2017-08-23T03:08:04.236336: step 39, loss 1.14819, acc 0.6\n",
      "current_step:  39\n",
      "2017-08-23T03:08:04.348177: step 40, loss 1.28126, acc 0.56\n",
      "current_step:  40\n",
      "2017-08-23T03:08:04.460465: step 41, loss 1.18476, acc 0.54\n",
      "current_step:  41\n",
      "2017-08-23T03:08:04.572489: step 42, loss 1.39246, acc 0.5\n",
      "current_step:  42\n",
      "2017-08-23T03:08:04.685727: step 43, loss 1.35165, acc 0.54\n",
      "current_step:  43\n",
      "2017-08-23T03:08:04.797492: step 44, loss 0.985458, acc 0.66\n",
      "current_step:  44\n",
      "2017-08-23T03:08:04.910780: step 45, loss 1.02638, acc 0.58\n",
      "current_step:  45\n",
      "2017-08-23T03:08:05.023128: step 46, loss 1.00089, acc 0.56\n",
      "current_step:  46\n",
      "2017-08-23T03:08:05.134802: step 47, loss 1.14772, acc 0.72\n",
      "current_step:  47\n",
      "2017-08-23T03:08:05.248290: step 48, loss 1.36166, acc 0.52\n",
      "current_step:  48\n",
      "2017-08-23T03:08:05.362656: step 49, loss 1.03314, acc 0.62\n",
      "current_step:  49\n",
      "2017-08-23T03:08:05.474711: step 50, loss 1.02991, acc 0.6\n",
      "current_step:  50\n",
      "2017-08-23T03:08:05.587291: step 51, loss 1.28641, acc 0.56\n",
      "current_step:  51\n",
      "2017-08-23T03:08:05.699673: step 52, loss 1.25134, acc 0.54\n",
      "current_step:  52\n",
      "2017-08-23T03:08:05.811800: step 53, loss 1.08678, acc 0.58\n",
      "current_step:  53\n",
      "2017-08-23T03:08:05.924341: step 54, loss 1.45414, acc 0.52\n",
      "current_step:  54\n",
      "2017-08-23T03:08:06.037265: step 55, loss 1.32933, acc 0.56\n",
      "current_step:  55\n",
      "2017-08-23T03:08:06.150163: step 56, loss 0.924353, acc 0.72\n",
      "current_step:  56\n",
      "2017-08-23T03:08:06.262780: step 57, loss 1.16533, acc 0.64\n",
      "current_step:  57\n",
      "2017-08-23T03:08:06.376180: step 58, loss 1.1133, acc 0.58\n",
      "current_step:  58\n",
      "2017-08-23T03:08:06.488074: step 59, loss 0.954047, acc 0.56\n",
      "current_step:  59\n",
      "2017-08-23T03:08:06.600081: step 60, loss 1.07028, acc 0.58\n",
      "current_step:  60\n",
      "2017-08-23T03:08:06.712784: step 61, loss 1.24359, acc 0.46\n",
      "current_step:  61\n",
      "2017-08-23T03:08:06.825585: step 62, loss 1.34939, acc 0.56\n",
      "current_step:  62\n",
      "2017-08-23T03:08:06.938166: step 63, loss 1.22122, acc 0.5\n",
      "current_step:  63\n",
      "2017-08-23T03:08:07.050603: step 64, loss 1.29565, acc 0.58\n",
      "current_step:  64\n",
      "2017-08-23T03:08:07.163033: step 65, loss 1.1296, acc 0.62\n",
      "current_step:  65\n",
      "2017-08-23T03:08:07.274662: step 66, loss 1.16808, acc 0.56\n",
      "current_step:  66\n",
      "2017-08-23T03:08:07.387239: step 67, loss 1.23005, acc 0.62\n",
      "current_step:  67\n",
      "2017-08-23T03:08:07.499947: step 68, loss 1.11122, acc 0.56\n",
      "current_step:  68\n",
      "2017-08-23T03:08:07.612486: step 69, loss 1.00903, acc 0.54\n",
      "current_step:  69\n",
      "2017-08-23T03:08:07.724924: step 70, loss 1.02304, acc 0.58\n",
      "current_step:  70\n",
      "2017-08-23T03:08:07.838724: step 71, loss 1.00793, acc 0.62\n",
      "current_step:  71\n",
      "2017-08-23T03:08:07.952426: step 72, loss 1.13707, acc 0.64\n",
      "current_step:  72\n",
      "2017-08-23T03:08:08.066271: step 73, loss 1.16081, acc 0.58\n",
      "current_step:  73\n",
      "2017-08-23T03:08:08.178558: step 74, loss 0.924576, acc 0.7\n",
      "current_step:  74\n",
      "2017-08-23T03:08:08.290960: step 75, loss 0.838, acc 0.78\n",
      "current_step:  75\n",
      "2017-08-23T03:08:08.403292: step 76, loss 1.13946, acc 0.64\n",
      "current_step:  76\n",
      "2017-08-23T03:08:08.519916: step 77, loss 1.01333, acc 0.66\n",
      "current_step:  77\n",
      "2017-08-23T03:08:08.632966: step 78, loss 1.07075, acc 0.52\n",
      "current_step:  78\n",
      "2017-08-23T03:08:08.746298: step 79, loss 1.2887, acc 0.52\n",
      "current_step:  79\n",
      "2017-08-23T03:08:08.858620: step 80, loss 0.982619, acc 0.64\n",
      "current_step:  80\n",
      "2017-08-23T03:08:08.972919: step 81, loss 1.01213, acc 0.64\n",
      "current_step:  81\n",
      "2017-08-23T03:08:09.085970: step 82, loss 0.992526, acc 0.68\n",
      "current_step:  82\n",
      "2017-08-23T03:08:09.198892: step 83, loss 1.04739, acc 0.6\n",
      "current_step:  83\n",
      "2017-08-23T03:08:09.311759: step 84, loss 1.48413, acc 0.46\n",
      "current_step:  84\n",
      "2017-08-23T03:08:09.425088: step 85, loss 1.14608, acc 0.54\n",
      "current_step:  85\n",
      "2017-08-23T03:08:09.538586: step 86, loss 0.932379, acc 0.66\n",
      "current_step:  86\n",
      "2017-08-23T03:08:09.650558: step 87, loss 0.917558, acc 0.66\n",
      "current_step:  87\n",
      "2017-08-23T03:08:09.762457: step 88, loss 1.08638, acc 0.6\n",
      "current_step:  88\n",
      "2017-08-23T03:08:09.875011: step 89, loss 1.03485, acc 0.7\n",
      "current_step:  89\n",
      "2017-08-23T03:08:09.987012: step 90, loss 1.29122, acc 0.52\n",
      "current_step:  90\n",
      "2017-08-23T03:08:10.099557: step 91, loss 1.22005, acc 0.62\n",
      "current_step:  91\n",
      "2017-08-23T03:08:10.212197: step 92, loss 0.883047, acc 0.7\n",
      "current_step:  92\n",
      "2017-08-23T03:08:10.324647: step 93, loss 1.16081, acc 0.7\n",
      "current_step:  93\n",
      "2017-08-23T03:08:10.437859: step 94, loss 0.923368, acc 0.66\n",
      "current_step:  94\n",
      "2017-08-23T03:08:10.549028: step 95, loss 1.07524, acc 0.58\n",
      "current_step:  95\n",
      "2017-08-23T03:08:10.661654: step 96, loss 0.946714, acc 0.66\n",
      "current_step:  96\n",
      "2017-08-23T03:08:10.773916: step 97, loss 1.04147, acc 0.64\n",
      "current_step:  97\n",
      "2017-08-23T03:08:10.887430: step 98, loss 1.06577, acc 0.66\n",
      "current_step:  98\n",
      "2017-08-23T03:08:10.999225: step 99, loss 1.32389, acc 0.52\n",
      "current_step:  99\n",
      "2017-08-23T03:08:11.111581: step 100, loss 1.03745, acc 0.62\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:08:11.525892: step 100, loss 0.991329, acc 0.642349\n",
      "\n",
      "2017-08-23T03:08:11.638995: step 101, loss 1.05194, acc 0.58\n",
      "current_step:  101\n",
      "2017-08-23T03:08:11.662659: step 102, loss 0.807084, acc 0.777778\n",
      "current_step:  102\n",
      "2017-08-23T03:08:11.777348: step 103, loss 0.710358, acc 0.72\n",
      "current_step:  103\n",
      "2017-08-23T03:08:11.888666: step 104, loss 0.764761, acc 0.72\n",
      "current_step:  104\n",
      "2017-08-23T03:08:12.001724: step 105, loss 0.905856, acc 0.66\n",
      "current_step:  105\n",
      "2017-08-23T03:08:12.113974: step 106, loss 1.09219, acc 0.64\n",
      "current_step:  106\n",
      "2017-08-23T03:08:12.226024: step 107, loss 0.925859, acc 0.66\n",
      "current_step:  107\n",
      "2017-08-23T03:08:12.338168: step 108, loss 0.700101, acc 0.76\n",
      "current_step:  108\n",
      "2017-08-23T03:08:12.450175: step 109, loss 1.03284, acc 0.6\n",
      "current_step:  109\n",
      "2017-08-23T03:08:12.562264: step 110, loss 0.781063, acc 0.74\n",
      "current_step:  110\n",
      "2017-08-23T03:08:12.675012: step 111, loss 0.8167, acc 0.7\n",
      "current_step:  111\n",
      "2017-08-23T03:08:12.786923: step 112, loss 0.725273, acc 0.68\n",
      "current_step:  112\n",
      "2017-08-23T03:08:12.899922: step 113, loss 0.807226, acc 0.7\n",
      "current_step:  113\n",
      "2017-08-23T03:08:13.013041: step 114, loss 0.761079, acc 0.78\n",
      "current_step:  114\n",
      "2017-08-23T03:08:13.125414: step 115, loss 0.82709, acc 0.7\n",
      "current_step:  115\n",
      "2017-08-23T03:08:13.237353: step 116, loss 1.06114, acc 0.64\n",
      "current_step:  116\n",
      "2017-08-23T03:08:13.349871: step 117, loss 0.607352, acc 0.82\n",
      "current_step:  117\n",
      "2017-08-23T03:08:13.463118: step 118, loss 0.848212, acc 0.7\n",
      "current_step:  118\n",
      "2017-08-23T03:08:13.574880: step 119, loss 0.824494, acc 0.7\n",
      "current_step:  119\n",
      "2017-08-23T03:08:13.686871: step 120, loss 0.940128, acc 0.68\n",
      "current_step:  120\n",
      "2017-08-23T03:08:13.798694: step 121, loss 0.682349, acc 0.76\n",
      "current_step:  121\n",
      "2017-08-23T03:08:13.912056: step 122, loss 0.929169, acc 0.64\n",
      "current_step:  122\n",
      "2017-08-23T03:08:14.024689: step 123, loss 0.761231, acc 0.7\n",
      "current_step:  123\n",
      "2017-08-23T03:08:14.136813: step 124, loss 0.715376, acc 0.68\n",
      "current_step:  124\n",
      "2017-08-23T03:08:14.248942: step 125, loss 0.915002, acc 0.68\n",
      "current_step:  125\n",
      "2017-08-23T03:08:14.361496: step 126, loss 0.604679, acc 0.76\n",
      "current_step:  126\n",
      "2017-08-23T03:08:14.474266: step 127, loss 1.03555, acc 0.6\n",
      "current_step:  127\n",
      "2017-08-23T03:08:14.587418: step 128, loss 1.04147, acc 0.64\n",
      "current_step:  128\n",
      "2017-08-23T03:08:14.701508: step 129, loss 0.619915, acc 0.8\n",
      "current_step:  129\n",
      "2017-08-23T03:08:14.814236: step 130, loss 0.726624, acc 0.78\n",
      "current_step:  130\n",
      "2017-08-23T03:08:14.927430: step 131, loss 0.867542, acc 0.64\n",
      "current_step:  131\n",
      "2017-08-23T03:08:15.042262: step 132, loss 1.00604, acc 0.56\n",
      "current_step:  132\n",
      "2017-08-23T03:08:15.155417: step 133, loss 0.759599, acc 0.72\n",
      "current_step:  133\n",
      "2017-08-23T03:08:15.270696: step 134, loss 0.657893, acc 0.82\n",
      "current_step:  134\n",
      "2017-08-23T03:08:15.383949: step 135, loss 0.867037, acc 0.72\n",
      "current_step:  135\n",
      "2017-08-23T03:08:15.496646: step 136, loss 0.645429, acc 0.82\n",
      "current_step:  136\n",
      "2017-08-23T03:08:15.609608: step 137, loss 0.901529, acc 0.76\n",
      "current_step:  137\n",
      "2017-08-23T03:08:15.722485: step 138, loss 0.725345, acc 0.74\n",
      "current_step:  138\n",
      "2017-08-23T03:08:15.835266: step 139, loss 0.807721, acc 0.72\n",
      "current_step:  139\n",
      "2017-08-23T03:08:15.950044: step 140, loss 0.723711, acc 0.78\n",
      "current_step:  140\n",
      "2017-08-23T03:08:16.063813: step 141, loss 0.652121, acc 0.76\n",
      "current_step:  141\n",
      "2017-08-23T03:08:16.174960: step 142, loss 0.591868, acc 0.78\n",
      "current_step:  142\n",
      "2017-08-23T03:08:16.287625: step 143, loss 0.639397, acc 0.86\n",
      "current_step:  143\n",
      "2017-08-23T03:08:16.400995: step 144, loss 1.13991, acc 0.6\n",
      "current_step:  144\n",
      "2017-08-23T03:08:16.514408: step 145, loss 0.805091, acc 0.68\n",
      "current_step:  145\n",
      "2017-08-23T03:08:16.627974: step 146, loss 1.1288, acc 0.6\n",
      "current_step:  146\n",
      "2017-08-23T03:08:16.741512: step 147, loss 0.922575, acc 0.66\n",
      "current_step:  147\n",
      "2017-08-23T03:08:16.855409: step 148, loss 0.79187, acc 0.74\n",
      "current_step:  148\n",
      "2017-08-23T03:08:16.970042: step 149, loss 0.737698, acc 0.68\n",
      "current_step:  149\n",
      "2017-08-23T03:08:17.083172: step 150, loss 0.733733, acc 0.78\n",
      "current_step:  150\n",
      "2017-08-23T03:08:17.195894: step 151, loss 0.88501, acc 0.66\n",
      "current_step:  151\n",
      "2017-08-23T03:08:17.307136: step 152, loss 0.709711, acc 0.78\n",
      "current_step:  152\n",
      "2017-08-23T03:08:17.419213: step 153, loss 0.867293, acc 0.6\n",
      "current_step:  153\n",
      "2017-08-23T03:08:17.531520: step 154, loss 0.728641, acc 0.72\n",
      "current_step:  154\n",
      "2017-08-23T03:08:17.644308: step 155, loss 0.875706, acc 0.68\n",
      "current_step:  155\n",
      "2017-08-23T03:08:17.756557: step 156, loss 0.982415, acc 0.58\n",
      "current_step:  156\n",
      "2017-08-23T03:08:17.868667: step 157, loss 0.784274, acc 0.74\n",
      "current_step:  157\n",
      "2017-08-23T03:08:17.981272: step 158, loss 1.07819, acc 0.68\n",
      "current_step:  158\n",
      "2017-08-23T03:08:18.093533: step 159, loss 0.791687, acc 0.72\n",
      "current_step:  159\n",
      "2017-08-23T03:08:18.206700: step 160, loss 0.740589, acc 0.78\n",
      "current_step:  160\n",
      "2017-08-23T03:08:18.319806: step 161, loss 0.925535, acc 0.66\n",
      "current_step:  161\n",
      "2017-08-23T03:08:18.432445: step 162, loss 0.821212, acc 0.72\n",
      "current_step:  162\n",
      "2017-08-23T03:08:18.547279: step 163, loss 0.726429, acc 0.8\n",
      "current_step:  163\n",
      "2017-08-23T03:08:18.660043: step 164, loss 0.756404, acc 0.7\n",
      "current_step:  164\n",
      "2017-08-23T03:08:18.772099: step 165, loss 0.954031, acc 0.66\n",
      "current_step:  165\n",
      "2017-08-23T03:08:18.883756: step 166, loss 0.868165, acc 0.78\n",
      "current_step:  166\n",
      "2017-08-23T03:08:18.995789: step 167, loss 0.891468, acc 0.74\n",
      "current_step:  167\n",
      "2017-08-23T03:08:19.108670: step 168, loss 0.773847, acc 0.82\n",
      "current_step:  168\n",
      "2017-08-23T03:08:19.220727: step 169, loss 0.741678, acc 0.82\n",
      "current_step:  169\n",
      "2017-08-23T03:08:19.334139: step 170, loss 0.880399, acc 0.74\n",
      "current_step:  170\n",
      "2017-08-23T03:08:19.446655: step 171, loss 0.792653, acc 0.68\n",
      "current_step:  171\n",
      "2017-08-23T03:08:19.558670: step 172, loss 0.849863, acc 0.66\n",
      "current_step:  172\n",
      "2017-08-23T03:08:19.670073: step 173, loss 1.03418, acc 0.68\n",
      "current_step:  173\n",
      "2017-08-23T03:08:19.782330: step 174, loss 0.661643, acc 0.78\n",
      "current_step:  174\n",
      "2017-08-23T03:08:19.894007: step 175, loss 0.863994, acc 0.72\n",
      "current_step:  175\n",
      "2017-08-23T03:08:20.006338: step 176, loss 0.886106, acc 0.68\n",
      "current_step:  176\n",
      "2017-08-23T03:08:20.119103: step 177, loss 1.19317, acc 0.56\n",
      "current_step:  177\n",
      "2017-08-23T03:08:20.230697: step 178, loss 1.00987, acc 0.54\n",
      "current_step:  178\n",
      "2017-08-23T03:08:20.342179: step 179, loss 1.05378, acc 0.62\n",
      "current_step:  179\n",
      "2017-08-23T03:08:20.453099: step 180, loss 1.11141, acc 0.6\n",
      "current_step:  180\n",
      "2017-08-23T03:08:20.565759: step 181, loss 0.873791, acc 0.66\n",
      "current_step:  181\n",
      "2017-08-23T03:08:20.677887: step 182, loss 0.97967, acc 0.66\n",
      "current_step:  182\n",
      "2017-08-23T03:08:20.791131: step 183, loss 0.820511, acc 0.76\n",
      "current_step:  183\n",
      "2017-08-23T03:08:20.904550: step 184, loss 0.739626, acc 0.78\n",
      "current_step:  184\n",
      "2017-08-23T03:08:21.017186: step 185, loss 0.924136, acc 0.56\n",
      "current_step:  185\n",
      "2017-08-23T03:08:21.129891: step 186, loss 0.759975, acc 0.72\n",
      "current_step:  186\n",
      "2017-08-23T03:08:21.243507: step 187, loss 0.700754, acc 0.7\n",
      "current_step:  187\n",
      "2017-08-23T03:08:21.355313: step 188, loss 0.875043, acc 0.66\n",
      "current_step:  188\n",
      "2017-08-23T03:08:21.467730: step 189, loss 0.759862, acc 0.64\n",
      "current_step:  189\n",
      "2017-08-23T03:08:21.580612: step 190, loss 0.843317, acc 0.7\n",
      "current_step:  190\n",
      "2017-08-23T03:08:21.692978: step 191, loss 0.744581, acc 0.76\n",
      "current_step:  191\n",
      "2017-08-23T03:08:21.805386: step 192, loss 1.00489, acc 0.64\n",
      "current_step:  192\n",
      "2017-08-23T03:08:21.918700: step 193, loss 0.896617, acc 0.7\n",
      "current_step:  193\n",
      "2017-08-23T03:08:22.031381: step 194, loss 0.820518, acc 0.76\n",
      "current_step:  194\n",
      "2017-08-23T03:08:22.146152: step 195, loss 0.910089, acc 0.68\n",
      "current_step:  195\n",
      "2017-08-23T03:08:22.259028: step 196, loss 0.921487, acc 0.68\n",
      "current_step:  196\n",
      "2017-08-23T03:08:22.370538: step 197, loss 0.906071, acc 0.7\n",
      "current_step:  197\n",
      "2017-08-23T03:08:22.483867: step 198, loss 0.784473, acc 0.66\n",
      "current_step:  198\n",
      "2017-08-23T03:08:22.596317: step 199, loss 1.07099, acc 0.58\n",
      "current_step:  199\n",
      "2017-08-23T03:08:22.709159: step 200, loss 1.01908, acc 0.6\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:08:23.117387: step 200, loss 1.00441, acc 0.626335\n",
      "\n",
      "2017-08-23T03:08:23.229667: step 201, loss 0.907268, acc 0.64\n",
      "current_step:  201\n",
      "2017-08-23T03:08:23.341826: step 202, loss 0.85481, acc 0.74\n",
      "current_step:  202\n",
      "2017-08-23T03:08:23.455276: step 203, loss 0.692738, acc 0.76\n",
      "current_step:  203\n",
      "2017-08-23T03:08:23.479646: step 204, loss 1.36756, acc 0.333333\n",
      "current_step:  204\n",
      "2017-08-23T03:08:23.593401: step 205, loss 0.574016, acc 0.88\n",
      "current_step:  205\n",
      "2017-08-23T03:08:23.706414: step 206, loss 0.791317, acc 0.74\n",
      "current_step:  206\n",
      "2017-08-23T03:08:23.818626: step 207, loss 0.72033, acc 0.8\n",
      "current_step:  207\n",
      "2017-08-23T03:08:23.931649: step 208, loss 0.5732, acc 0.86\n",
      "current_step:  208\n",
      "2017-08-23T03:08:24.044655: step 209, loss 0.572856, acc 0.84\n",
      "current_step:  209\n",
      "2017-08-23T03:08:24.156656: step 210, loss 0.554148, acc 0.8\n",
      "current_step:  210\n",
      "2017-08-23T03:08:24.269407: step 211, loss 0.727946, acc 0.62\n",
      "current_step:  211\n",
      "2017-08-23T03:08:24.382212: step 212, loss 0.526619, acc 0.8\n",
      "current_step:  212\n",
      "2017-08-23T03:08:24.494809: step 213, loss 0.907077, acc 0.56\n",
      "current_step:  213\n",
      "2017-08-23T03:08:24.607502: step 214, loss 0.569097, acc 0.84\n",
      "current_step:  214\n",
      "2017-08-23T03:08:24.720274: step 215, loss 0.806991, acc 0.72\n",
      "current_step:  215\n",
      "2017-08-23T03:08:24.833834: step 216, loss 0.591861, acc 0.78\n",
      "current_step:  216\n",
      "2017-08-23T03:08:24.946825: step 217, loss 0.450307, acc 0.9\n",
      "current_step:  217\n",
      "2017-08-23T03:08:25.060465: step 218, loss 0.661232, acc 0.84\n",
      "current_step:  218\n",
      "2017-08-23T03:08:25.173350: step 219, loss 0.646756, acc 0.82\n",
      "current_step:  219\n",
      "2017-08-23T03:08:25.286594: step 220, loss 0.631127, acc 0.82\n",
      "current_step:  220\n",
      "2017-08-23T03:08:25.398900: step 221, loss 0.669035, acc 0.8\n",
      "current_step:  221\n",
      "2017-08-23T03:08:25.511377: step 222, loss 0.542383, acc 0.9\n",
      "current_step:  222\n",
      "2017-08-23T03:08:25.623283: step 223, loss 0.771146, acc 0.62\n",
      "current_step:  223\n",
      "2017-08-23T03:08:25.736814: step 224, loss 0.542495, acc 0.82\n",
      "current_step:  224\n",
      "2017-08-23T03:08:25.849176: step 225, loss 0.87011, acc 0.72\n",
      "current_step:  225\n",
      "2017-08-23T03:08:25.962842: step 226, loss 0.6672, acc 0.76\n",
      "current_step:  226\n",
      "2017-08-23T03:08:26.074710: step 227, loss 0.649582, acc 0.82\n",
      "current_step:  227\n",
      "2017-08-23T03:08:26.186455: step 228, loss 0.805041, acc 0.78\n",
      "current_step:  228\n",
      "2017-08-23T03:08:26.298748: step 229, loss 0.737391, acc 0.72\n",
      "current_step:  229\n",
      "2017-08-23T03:08:26.411155: step 230, loss 0.716244, acc 0.78\n",
      "current_step:  230\n",
      "2017-08-23T03:08:26.522746: step 231, loss 0.469471, acc 0.86\n",
      "current_step:  231\n",
      "2017-08-23T03:08:26.635027: step 232, loss 0.702455, acc 0.76\n",
      "current_step:  232\n",
      "2017-08-23T03:08:26.747277: step 233, loss 0.882574, acc 0.7\n",
      "current_step:  233\n",
      "2017-08-23T03:08:26.859323: step 234, loss 0.611275, acc 0.8\n",
      "current_step:  234\n",
      "2017-08-23T03:08:26.972493: step 235, loss 0.567383, acc 0.84\n",
      "current_step:  235\n",
      "2017-08-23T03:08:27.084893: step 236, loss 0.856229, acc 0.68\n",
      "current_step:  236\n",
      "2017-08-23T03:08:27.196730: step 237, loss 0.799149, acc 0.64\n",
      "current_step:  237\n",
      "2017-08-23T03:08:27.308526: step 238, loss 0.63951, acc 0.82\n",
      "current_step:  238\n",
      "2017-08-23T03:08:27.421380: step 239, loss 0.636808, acc 0.86\n",
      "current_step:  239\n",
      "2017-08-23T03:08:27.534576: step 240, loss 0.80757, acc 0.66\n",
      "current_step:  240\n",
      "2017-08-23T03:08:27.646715: step 241, loss 0.761856, acc 0.78\n",
      "current_step:  241\n",
      "2017-08-23T03:08:27.759168: step 242, loss 0.628422, acc 0.86\n",
      "current_step:  242\n",
      "2017-08-23T03:08:27.871538: step 243, loss 0.679613, acc 0.8\n",
      "current_step:  243\n",
      "2017-08-23T03:08:27.985479: step 244, loss 0.603517, acc 0.84\n",
      "current_step:  244\n",
      "2017-08-23T03:08:28.098270: step 245, loss 0.677403, acc 0.76\n",
      "current_step:  245\n",
      "2017-08-23T03:08:28.210235: step 246, loss 0.714206, acc 0.68\n",
      "current_step:  246\n",
      "2017-08-23T03:08:28.321786: step 247, loss 0.567136, acc 0.88\n",
      "current_step:  247\n",
      "2017-08-23T03:08:28.433570: step 248, loss 0.511204, acc 0.78\n",
      "current_step:  248\n",
      "2017-08-23T03:08:28.546030: step 249, loss 0.693167, acc 0.74\n",
      "current_step:  249\n",
      "2017-08-23T03:08:28.658543: step 250, loss 0.667801, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-23T03:08:28.771303: step 251, loss 0.544867, acc 0.8\n",
      "current_step:  251\n",
      "2017-08-23T03:08:28.883364: step 252, loss 0.877961, acc 0.7\n",
      "current_step:  252\n",
      "2017-08-23T03:08:28.995331: step 253, loss 0.580617, acc 0.82\n",
      "current_step:  253\n",
      "2017-08-23T03:08:29.108059: step 254, loss 0.701065, acc 0.8\n",
      "current_step:  254\n",
      "2017-08-23T03:08:29.220517: step 255, loss 0.722522, acc 0.68\n",
      "current_step:  255\n",
      "2017-08-23T03:08:29.332163: step 256, loss 0.772232, acc 0.74\n",
      "current_step:  256\n",
      "2017-08-23T03:08:29.444530: step 257, loss 0.756348, acc 0.78\n",
      "current_step:  257\n",
      "2017-08-23T03:08:29.559918: step 258, loss 0.838681, acc 0.74\n",
      "current_step:  258\n",
      "2017-08-23T03:08:29.676225: step 259, loss 0.57734, acc 0.84\n",
      "current_step:  259\n",
      "2017-08-23T03:08:29.791666: step 260, loss 0.817019, acc 0.72\n",
      "current_step:  260\n",
      "2017-08-23T03:08:29.908155: step 261, loss 0.755742, acc 0.74\n",
      "current_step:  261\n",
      "2017-08-23T03:08:30.023228: step 262, loss 0.600299, acc 0.82\n",
      "current_step:  262\n",
      "2017-08-23T03:08:30.140379: step 263, loss 0.620795, acc 0.84\n",
      "current_step:  263\n",
      "2017-08-23T03:08:30.256650: step 264, loss 0.492252, acc 0.84\n",
      "current_step:  264\n",
      "2017-08-23T03:08:30.372525: step 265, loss 0.806194, acc 0.7\n",
      "current_step:  265\n",
      "2017-08-23T03:08:30.487631: step 266, loss 0.581895, acc 0.84\n",
      "current_step:  266\n",
      "2017-08-23T03:08:30.603360: step 267, loss 0.833577, acc 0.72\n",
      "current_step:  267\n",
      "2017-08-23T03:08:30.718448: step 268, loss 0.7339, acc 0.78\n",
      "current_step:  268\n",
      "2017-08-23T03:08:30.835099: step 269, loss 0.491625, acc 0.78\n",
      "current_step:  269\n",
      "2017-08-23T03:08:30.952688: step 270, loss 0.91725, acc 0.66\n",
      "current_step:  270\n",
      "2017-08-23T03:08:31.068747: step 271, loss 0.660159, acc 0.78\n",
      "current_step:  271\n",
      "2017-08-23T03:08:31.184595: step 272, loss 0.740511, acc 0.72\n",
      "current_step:  272\n",
      "2017-08-23T03:08:31.302597: step 273, loss 0.76615, acc 0.7\n",
      "current_step:  273\n",
      "2017-08-23T03:08:31.421064: step 274, loss 0.434089, acc 0.94\n",
      "current_step:  274\n",
      "2017-08-23T03:08:31.537319: step 275, loss 0.742567, acc 0.8\n",
      "current_step:  275\n",
      "2017-08-23T03:08:31.652680: step 276, loss 0.643579, acc 0.74\n",
      "current_step:  276\n",
      "2017-08-23T03:08:31.769168: step 277, loss 0.697547, acc 0.8\n",
      "current_step:  277\n",
      "2017-08-23T03:08:31.886319: step 278, loss 0.681954, acc 0.76\n",
      "current_step:  278\n",
      "2017-08-23T03:08:32.003557: step 279, loss 0.491259, acc 0.82\n",
      "current_step:  279\n",
      "2017-08-23T03:08:32.120728: step 280, loss 0.641252, acc 0.8\n",
      "current_step:  280\n",
      "2017-08-23T03:08:32.237108: step 281, loss 0.386285, acc 0.9\n",
      "current_step:  281\n",
      "2017-08-23T03:08:32.351991: step 282, loss 0.545413, acc 0.74\n",
      "current_step:  282\n",
      "2017-08-23T03:08:32.470263: step 283, loss 0.607628, acc 0.76\n",
      "current_step:  283\n",
      "2017-08-23T03:08:32.586314: step 284, loss 0.547427, acc 0.76\n",
      "current_step:  284\n",
      "2017-08-23T03:08:32.701844: step 285, loss 0.65278, acc 0.8\n",
      "current_step:  285\n",
      "2017-08-23T03:08:32.819091: step 286, loss 0.879559, acc 0.74\n",
      "current_step:  286\n",
      "2017-08-23T03:08:32.936225: step 287, loss 0.943231, acc 0.6\n",
      "current_step:  287\n",
      "2017-08-23T03:08:33.053503: step 288, loss 1.02305, acc 0.6\n",
      "current_step:  288\n",
      "2017-08-23T03:08:33.169955: step 289, loss 0.695278, acc 0.78\n",
      "current_step:  289\n",
      "2017-08-23T03:08:33.287867: step 290, loss 0.686406, acc 0.8\n",
      "current_step:  290\n",
      "2017-08-23T03:08:33.405363: step 291, loss 0.739799, acc 0.78\n",
      "current_step:  291\n",
      "2017-08-23T03:08:33.522304: step 292, loss 0.809974, acc 0.76\n",
      "current_step:  292\n",
      "2017-08-23T03:08:33.637328: step 293, loss 0.633296, acc 0.8\n",
      "current_step:  293\n",
      "2017-08-23T03:08:33.753255: step 294, loss 0.810302, acc 0.68\n",
      "current_step:  294\n",
      "2017-08-23T03:08:33.869746: step 295, loss 0.807899, acc 0.76\n",
      "current_step:  295\n",
      "2017-08-23T03:08:33.988324: step 296, loss 0.720401, acc 0.78\n",
      "current_step:  296\n",
      "2017-08-23T03:08:34.106579: step 297, loss 0.548914, acc 0.86\n",
      "current_step:  297\n",
      "2017-08-23T03:08:34.224466: step 298, loss 0.673855, acc 0.7\n",
      "current_step:  298\n",
      "2017-08-23T03:08:34.341715: step 299, loss 0.599388, acc 0.78\n",
      "current_step:  299\n",
      "2017-08-23T03:08:34.459835: step 300, loss 0.486783, acc 0.84\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:08:34.886256: step 300, loss 0.890156, acc 0.699288\n",
      "\n",
      "2017-08-23T03:08:35.006996: step 301, loss 0.844886, acc 0.68\n",
      "current_step:  301\n",
      "2017-08-23T03:08:35.128709: step 302, loss 0.730224, acc 0.76\n",
      "current_step:  302\n",
      "2017-08-23T03:08:35.245754: step 303, loss 0.483028, acc 0.82\n",
      "current_step:  303\n",
      "2017-08-23T03:08:35.363475: step 304, loss 0.555777, acc 0.82\n",
      "current_step:  304\n",
      "2017-08-23T03:08:35.481293: step 305, loss 0.62708, acc 0.78\n",
      "current_step:  305\n",
      "2017-08-23T03:08:35.506751: step 306, loss 0.582809, acc 0.777778\n",
      "current_step:  306\n",
      "2017-08-23T03:08:35.627075: step 307, loss 0.477687, acc 0.84\n",
      "current_step:  307\n",
      "2017-08-23T03:08:35.743995: step 308, loss 0.619568, acc 0.76\n",
      "current_step:  308\n",
      "2017-08-23T03:08:35.865747: step 309, loss 0.401311, acc 0.94\n",
      "current_step:  309\n",
      "2017-08-23T03:08:35.984177: step 310, loss 0.740595, acc 0.72\n",
      "current_step:  310\n",
      "2017-08-23T03:08:36.104644: step 311, loss 0.458518, acc 0.9\n",
      "current_step:  311\n",
      "2017-08-23T03:08:36.223247: step 312, loss 0.509903, acc 0.82\n",
      "current_step:  312\n",
      "2017-08-23T03:08:36.342726: step 313, loss 0.582134, acc 0.84\n",
      "current_step:  313\n",
      "2017-08-23T03:08:36.460471: step 314, loss 0.519177, acc 0.82\n",
      "current_step:  314\n",
      "2017-08-23T03:08:36.578771: step 315, loss 0.41767, acc 0.9\n",
      "current_step:  315\n",
      "2017-08-23T03:08:36.695126: step 316, loss 0.517502, acc 0.88\n",
      "current_step:  316\n",
      "2017-08-23T03:08:36.812236: step 317, loss 0.46874, acc 0.9\n",
      "current_step:  317\n",
      "2017-08-23T03:08:36.930625: step 318, loss 0.496485, acc 0.86\n",
      "current_step:  318\n",
      "2017-08-23T03:08:37.048228: step 319, loss 0.578617, acc 0.84\n",
      "current_step:  319\n",
      "2017-08-23T03:08:37.166312: step 320, loss 0.620562, acc 0.8\n",
      "current_step:  320\n",
      "2017-08-23T03:08:37.285396: step 321, loss 0.518432, acc 0.82\n",
      "current_step:  321\n",
      "2017-08-23T03:08:37.400840: step 322, loss 0.446958, acc 0.9\n",
      "current_step:  322\n",
      "2017-08-23T03:08:37.519584: step 323, loss 0.526211, acc 0.82\n",
      "current_step:  323\n",
      "2017-08-23T03:08:37.638018: step 324, loss 0.457786, acc 0.88\n",
      "current_step:  324\n",
      "2017-08-23T03:08:37.753732: step 325, loss 0.71623, acc 0.74\n",
      "current_step:  325\n",
      "2017-08-23T03:08:37.868995: step 326, loss 0.655098, acc 0.76\n",
      "current_step:  326\n",
      "2017-08-23T03:08:37.986421: step 327, loss 0.483389, acc 0.88\n",
      "current_step:  327\n",
      "2017-08-23T03:08:38.104544: step 328, loss 0.483273, acc 0.84\n",
      "current_step:  328\n",
      "2017-08-23T03:08:38.222688: step 329, loss 0.612676, acc 0.8\n",
      "current_step:  329\n",
      "2017-08-23T03:08:38.339779: step 330, loss 0.431983, acc 0.9\n",
      "current_step:  330\n",
      "2017-08-23T03:08:38.458900: step 331, loss 0.633573, acc 0.82\n",
      "current_step:  331\n",
      "2017-08-23T03:08:38.577007: step 332, loss 0.638182, acc 0.78\n",
      "current_step:  332\n",
      "2017-08-23T03:08:38.696036: step 333, loss 0.482948, acc 0.88\n",
      "current_step:  333\n",
      "2017-08-23T03:08:38.812831: step 334, loss 0.541862, acc 0.8\n",
      "current_step:  334\n",
      "2017-08-23T03:08:38.928430: step 335, loss 0.459266, acc 0.84\n",
      "current_step:  335\n",
      "2017-08-23T03:08:39.044259: step 336, loss 0.523326, acc 0.84\n",
      "current_step:  336\n",
      "2017-08-23T03:08:39.161009: step 337, loss 0.627472, acc 0.82\n",
      "current_step:  337\n",
      "2017-08-23T03:08:39.275694: step 338, loss 0.325807, acc 0.92\n",
      "current_step:  338\n",
      "2017-08-23T03:08:39.392516: step 339, loss 0.522256, acc 0.86\n",
      "current_step:  339\n",
      "2017-08-23T03:08:39.508554: step 340, loss 0.570395, acc 0.82\n",
      "current_step:  340\n",
      "2017-08-23T03:08:39.622025: step 341, loss 0.568146, acc 0.84\n",
      "current_step:  341\n",
      "2017-08-23T03:08:39.734138: step 342, loss 0.59225, acc 0.78\n",
      "current_step:  342\n",
      "2017-08-23T03:08:39.846067: step 343, loss 0.444139, acc 0.84\n",
      "current_step:  343\n",
      "2017-08-23T03:08:39.958631: step 344, loss 0.515501, acc 0.84\n",
      "current_step:  344\n",
      "2017-08-23T03:08:40.071757: step 345, loss 0.522055, acc 0.82\n",
      "current_step:  345\n",
      "2017-08-23T03:08:40.183849: step 346, loss 0.530239, acc 0.82\n",
      "current_step:  346\n",
      "2017-08-23T03:08:40.296949: step 347, loss 0.434132, acc 0.88\n",
      "current_step:  347\n",
      "2017-08-23T03:08:40.409359: step 348, loss 0.640491, acc 0.82\n",
      "current_step:  348\n",
      "2017-08-23T03:08:40.522713: step 349, loss 0.535365, acc 0.84\n",
      "current_step:  349\n",
      "2017-08-23T03:08:40.634585: step 350, loss 0.577016, acc 0.78\n",
      "current_step:  350\n",
      "2017-08-23T03:08:40.746856: step 351, loss 0.602458, acc 0.78\n",
      "current_step:  351\n",
      "2017-08-23T03:08:40.859908: step 352, loss 0.655374, acc 0.84\n",
      "current_step:  352\n",
      "2017-08-23T03:08:40.972584: step 353, loss 0.442946, acc 0.92\n",
      "current_step:  353\n",
      "2017-08-23T03:08:41.085260: step 354, loss 0.606432, acc 0.86\n",
      "current_step:  354\n",
      "2017-08-23T03:08:41.198221: step 355, loss 0.48053, acc 0.88\n",
      "current_step:  355\n",
      "2017-08-23T03:08:41.310746: step 356, loss 0.460419, acc 0.86\n",
      "current_step:  356\n",
      "2017-08-23T03:08:41.423294: step 357, loss 0.510992, acc 0.86\n",
      "current_step:  357\n",
      "2017-08-23T03:08:41.536841: step 358, loss 0.613898, acc 0.78\n",
      "current_step:  358\n",
      "2017-08-23T03:08:41.650386: step 359, loss 0.492587, acc 0.86\n",
      "current_step:  359\n",
      "2017-08-23T03:08:41.766790: step 360, loss 0.579789, acc 0.76\n",
      "current_step:  360\n",
      "2017-08-23T03:08:41.883085: step 361, loss 0.542114, acc 0.82\n",
      "current_step:  361\n",
      "2017-08-23T03:08:41.999911: step 362, loss 0.43975, acc 0.9\n",
      "current_step:  362\n",
      "2017-08-23T03:08:42.116014: step 363, loss 0.570274, acc 0.74\n",
      "current_step:  363\n",
      "2017-08-23T03:08:42.231076: step 364, loss 0.440718, acc 0.86\n",
      "current_step:  364\n",
      "2017-08-23T03:08:42.346580: step 365, loss 0.47386, acc 0.92\n",
      "current_step:  365\n",
      "2017-08-23T03:08:42.461067: step 366, loss 0.385026, acc 0.9\n",
      "current_step:  366\n",
      "2017-08-23T03:08:42.574319: step 367, loss 0.683665, acc 0.86\n",
      "current_step:  367\n",
      "2017-08-23T03:08:42.687561: step 368, loss 0.48336, acc 0.9\n",
      "current_step:  368\n",
      "2017-08-23T03:08:42.801896: step 369, loss 0.485913, acc 0.84\n",
      "current_step:  369\n",
      "2017-08-23T03:08:42.914920: step 370, loss 0.408038, acc 0.92\n",
      "current_step:  370\n",
      "2017-08-23T03:08:43.027963: step 371, loss 0.607486, acc 0.82\n",
      "current_step:  371\n",
      "2017-08-23T03:08:43.141428: step 372, loss 0.444406, acc 0.88\n",
      "current_step:  372\n",
      "2017-08-23T03:08:43.253780: step 373, loss 0.621222, acc 0.76\n",
      "current_step:  373\n",
      "2017-08-23T03:08:43.366707: step 374, loss 0.49708, acc 0.86\n",
      "current_step:  374\n",
      "2017-08-23T03:08:43.479826: step 375, loss 0.645427, acc 0.78\n",
      "current_step:  375\n",
      "2017-08-23T03:08:43.592719: step 376, loss 0.568285, acc 0.86\n",
      "current_step:  376\n",
      "2017-08-23T03:08:43.705869: step 377, loss 0.617524, acc 0.84\n",
      "current_step:  377\n",
      "2017-08-23T03:08:43.818475: step 378, loss 0.41696, acc 0.84\n",
      "current_step:  378\n",
      "2017-08-23T03:08:43.932613: step 379, loss 0.688611, acc 0.8\n",
      "current_step:  379\n",
      "2017-08-23T03:08:44.046138: step 380, loss 0.50786, acc 0.86\n",
      "current_step:  380\n",
      "2017-08-23T03:08:44.159232: step 381, loss 0.572687, acc 0.82\n",
      "current_step:  381\n",
      "2017-08-23T03:08:44.271281: step 382, loss 0.491001, acc 0.86\n",
      "current_step:  382\n",
      "2017-08-23T03:08:44.383392: step 383, loss 0.498657, acc 0.86\n",
      "current_step:  383\n",
      "2017-08-23T03:08:44.496463: step 384, loss 0.523973, acc 0.84\n",
      "current_step:  384\n",
      "2017-08-23T03:08:44.609317: step 385, loss 0.485291, acc 0.82\n",
      "current_step:  385\n",
      "2017-08-23T03:08:44.721131: step 386, loss 0.4567, acc 0.86\n",
      "current_step:  386\n",
      "2017-08-23T03:08:44.833913: step 387, loss 0.592689, acc 0.78\n",
      "current_step:  387\n",
      "2017-08-23T03:08:44.947209: step 388, loss 0.498414, acc 0.82\n",
      "current_step:  388\n",
      "2017-08-23T03:08:45.060055: step 389, loss 0.512724, acc 0.86\n",
      "current_step:  389\n",
      "2017-08-23T03:08:45.173283: step 390, loss 0.598591, acc 0.82\n",
      "current_step:  390\n",
      "2017-08-23T03:08:45.286460: step 391, loss 0.564669, acc 0.8\n",
      "current_step:  391\n",
      "2017-08-23T03:08:45.398169: step 392, loss 0.304486, acc 0.92\n",
      "current_step:  392\n",
      "2017-08-23T03:08:45.511686: step 393, loss 0.44394, acc 0.9\n",
      "current_step:  393\n",
      "2017-08-23T03:08:45.624486: step 394, loss 0.369059, acc 0.9\n",
      "current_step:  394\n",
      "2017-08-23T03:08:45.737381: step 395, loss 0.663899, acc 0.76\n",
      "current_step:  395\n",
      "2017-08-23T03:08:45.851592: step 396, loss 0.574732, acc 0.84\n",
      "current_step:  396\n",
      "2017-08-23T03:08:45.969736: step 397, loss 0.597374, acc 0.8\n",
      "current_step:  397\n",
      "2017-08-23T03:08:46.083061: step 398, loss 0.605571, acc 0.8\n",
      "current_step:  398\n",
      "2017-08-23T03:08:46.195177: step 399, loss 0.496842, acc 0.88\n",
      "current_step:  399\n",
      "2017-08-23T03:08:46.307502: step 400, loss 0.455076, acc 0.88\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:08:46.713749: step 400, loss 0.887501, acc 0.681495\n",
      "\n",
      "2017-08-23T03:08:46.826342: step 401, loss 0.588988, acc 0.76\n",
      "current_step:  401\n",
      "2017-08-23T03:08:46.938223: step 402, loss 0.609773, acc 0.8\n",
      "current_step:  402\n",
      "2017-08-23T03:08:47.050456: step 403, loss 0.653187, acc 0.76\n",
      "current_step:  403\n",
      "2017-08-23T03:08:47.162420: step 404, loss 0.636538, acc 0.76\n",
      "current_step:  404\n",
      "2017-08-23T03:08:47.275607: step 405, loss 0.562486, acc 0.86\n",
      "current_step:  405\n",
      "2017-08-23T03:08:47.387245: step 406, loss 0.59481, acc 0.82\n",
      "current_step:  406\n",
      "2017-08-23T03:08:47.499465: step 407, loss 0.585336, acc 0.88\n",
      "current_step:  407\n",
      "2017-08-23T03:08:47.523144: step 408, loss 0.620076, acc 0.777778\n",
      "current_step:  408\n",
      "2017-08-23T03:08:47.635665: step 409, loss 0.450495, acc 0.96\n",
      "current_step:  409\n",
      "2017-08-23T03:08:47.747592: step 410, loss 0.410035, acc 0.9\n",
      "current_step:  410\n",
      "2017-08-23T03:08:47.862077: step 411, loss 0.366287, acc 0.94\n",
      "current_step:  411\n",
      "2017-08-23T03:08:47.976050: step 412, loss 0.466636, acc 0.92\n",
      "current_step:  412\n",
      "2017-08-23T03:08:48.088864: step 413, loss 0.39238, acc 0.9\n",
      "current_step:  413\n",
      "2017-08-23T03:08:48.201582: step 414, loss 0.454518, acc 0.84\n",
      "current_step:  414\n",
      "2017-08-23T03:08:48.316362: step 415, loss 0.464232, acc 0.9\n",
      "current_step:  415\n",
      "2017-08-23T03:08:48.432693: step 416, loss 0.468527, acc 0.88\n",
      "current_step:  416\n",
      "2017-08-23T03:08:48.546153: step 417, loss 0.384717, acc 0.94\n",
      "current_step:  417\n",
      "2017-08-23T03:08:48.659984: step 418, loss 0.423336, acc 0.9\n",
      "current_step:  418\n",
      "2017-08-23T03:08:48.773833: step 419, loss 0.338211, acc 0.9\n",
      "current_step:  419\n",
      "2017-08-23T03:08:48.885906: step 420, loss 0.441923, acc 0.84\n",
      "current_step:  420\n",
      "2017-08-23T03:08:48.999001: step 421, loss 0.314366, acc 0.92\n",
      "current_step:  421\n",
      "2017-08-23T03:08:49.111500: step 422, loss 0.566009, acc 0.76\n",
      "current_step:  422\n",
      "2017-08-23T03:08:49.224303: step 423, loss 0.484864, acc 0.88\n",
      "current_step:  423\n",
      "2017-08-23T03:08:49.335593: step 424, loss 0.384403, acc 0.9\n",
      "current_step:  424\n",
      "2017-08-23T03:08:49.448098: step 425, loss 0.399018, acc 0.9\n",
      "current_step:  425\n",
      "2017-08-23T03:08:49.559970: step 426, loss 0.439615, acc 0.86\n",
      "current_step:  426\n",
      "2017-08-23T03:08:49.672042: step 427, loss 0.329963, acc 0.94\n",
      "current_step:  427\n",
      "2017-08-23T03:08:49.783354: step 428, loss 0.53889, acc 0.82\n",
      "current_step:  428\n",
      "2017-08-23T03:08:49.896004: step 429, loss 0.315915, acc 0.94\n",
      "current_step:  429\n",
      "2017-08-23T03:08:50.009210: step 430, loss 0.513557, acc 0.88\n",
      "current_step:  430\n",
      "2017-08-23T03:08:50.121734: step 431, loss 0.461211, acc 0.88\n",
      "current_step:  431\n",
      "2017-08-23T03:08:50.233733: step 432, loss 0.52883, acc 0.86\n",
      "current_step:  432\n",
      "2017-08-23T03:08:50.345447: step 433, loss 0.534731, acc 0.78\n",
      "current_step:  433\n",
      "2017-08-23T03:08:50.457261: step 434, loss 0.366984, acc 0.92\n",
      "current_step:  434\n",
      "2017-08-23T03:08:50.568668: step 435, loss 0.412901, acc 0.92\n",
      "current_step:  435\n",
      "2017-08-23T03:08:50.681120: step 436, loss 0.353006, acc 0.96\n",
      "current_step:  436\n",
      "2017-08-23T03:08:50.794475: step 437, loss 0.350842, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-23T03:08:50.906427: step 438, loss 0.357363, acc 0.92\n",
      "current_step:  438\n",
      "2017-08-23T03:08:51.018837: step 439, loss 0.371308, acc 0.94\n",
      "current_step:  439\n",
      "2017-08-23T03:08:51.130988: step 440, loss 0.450502, acc 0.92\n",
      "current_step:  440\n",
      "2017-08-23T03:08:51.243627: step 441, loss 0.391401, acc 0.9\n",
      "current_step:  441\n",
      "2017-08-23T03:08:51.356006: step 442, loss 0.496391, acc 0.86\n",
      "current_step:  442\n",
      "2017-08-23T03:08:51.468213: step 443, loss 0.484548, acc 0.86\n",
      "current_step:  443\n",
      "2017-08-23T03:08:51.580163: step 444, loss 0.514256, acc 0.86\n",
      "current_step:  444\n",
      "2017-08-23T03:08:51.692460: step 445, loss 0.549272, acc 0.84\n",
      "current_step:  445\n",
      "2017-08-23T03:08:51.804732: step 446, loss 0.362855, acc 0.92\n",
      "current_step:  446\n",
      "2017-08-23T03:08:51.917558: step 447, loss 0.365369, acc 0.88\n",
      "current_step:  447\n",
      "2017-08-23T03:08:52.029334: step 448, loss 0.428027, acc 0.86\n",
      "current_step:  448\n",
      "2017-08-23T03:08:52.141565: step 449, loss 0.498294, acc 0.84\n",
      "current_step:  449\n",
      "2017-08-23T03:08:52.254082: step 450, loss 0.304822, acc 0.94\n",
      "current_step:  450\n",
      "2017-08-23T03:08:52.367649: step 451, loss 0.437729, acc 0.9\n",
      "current_step:  451\n",
      "2017-08-23T03:08:52.479826: step 452, loss 0.662042, acc 0.82\n",
      "current_step:  452\n",
      "2017-08-23T03:08:52.592158: step 453, loss 0.424909, acc 0.88\n",
      "current_step:  453\n",
      "2017-08-23T03:08:52.704182: step 454, loss 0.41472, acc 0.84\n",
      "current_step:  454\n",
      "2017-08-23T03:08:52.815228: step 455, loss 0.461911, acc 0.84\n",
      "current_step:  455\n",
      "2017-08-23T03:08:52.927377: step 456, loss 0.427335, acc 0.92\n",
      "current_step:  456\n",
      "2017-08-23T03:08:53.040440: step 457, loss 0.439439, acc 0.88\n",
      "current_step:  457\n",
      "2017-08-23T03:08:53.151838: step 458, loss 0.289192, acc 0.96\n",
      "current_step:  458\n",
      "2017-08-23T03:08:53.263494: step 459, loss 0.521326, acc 0.84\n",
      "current_step:  459\n",
      "2017-08-23T03:08:53.377079: step 460, loss 0.426577, acc 0.84\n",
      "current_step:  460\n",
      "2017-08-23T03:08:53.489512: step 461, loss 0.398772, acc 0.92\n",
      "current_step:  461\n",
      "2017-08-23T03:08:53.602632: step 462, loss 0.430222, acc 0.9\n",
      "current_step:  462\n",
      "2017-08-23T03:08:53.714329: step 463, loss 0.386838, acc 0.92\n",
      "current_step:  463\n",
      "2017-08-23T03:08:53.825634: step 464, loss 0.446031, acc 0.92\n",
      "current_step:  464\n",
      "2017-08-23T03:08:53.937679: step 465, loss 0.495295, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-23T03:08:54.048580: step 466, loss 0.528939, acc 0.86\n",
      "current_step:  466\n",
      "2017-08-23T03:08:54.161493: step 467, loss 0.349447, acc 0.92\n",
      "current_step:  467\n",
      "2017-08-23T03:08:54.273069: step 468, loss 0.415074, acc 0.9\n",
      "current_step:  468\n",
      "2017-08-23T03:08:54.386177: step 469, loss 0.506094, acc 0.9\n",
      "current_step:  469\n",
      "2017-08-23T03:08:54.498067: step 470, loss 0.399312, acc 0.86\n",
      "current_step:  470\n",
      "2017-08-23T03:08:54.609544: step 471, loss 0.394512, acc 0.9\n",
      "current_step:  471\n",
      "2017-08-23T03:08:54.720988: step 472, loss 0.401902, acc 0.9\n",
      "current_step:  472\n",
      "2017-08-23T03:08:54.833065: step 473, loss 0.396226, acc 0.84\n",
      "current_step:  473\n",
      "2017-08-23T03:08:54.946114: step 474, loss 0.590588, acc 0.86\n",
      "current_step:  474\n",
      "2017-08-23T03:08:55.059713: step 475, loss 0.430771, acc 0.88\n",
      "current_step:  475\n",
      "2017-08-23T03:08:55.171801: step 476, loss 0.396776, acc 0.9\n",
      "current_step:  476\n",
      "2017-08-23T03:08:55.282655: step 477, loss 0.422316, acc 0.86\n",
      "current_step:  477\n",
      "2017-08-23T03:08:55.394536: step 478, loss 0.336326, acc 0.96\n",
      "current_step:  478\n",
      "2017-08-23T03:08:55.505513: step 479, loss 0.559985, acc 0.86\n",
      "current_step:  479\n",
      "2017-08-23T03:08:55.617897: step 480, loss 0.339156, acc 0.94\n",
      "current_step:  480\n",
      "2017-08-23T03:08:55.729563: step 481, loss 0.27283, acc 0.94\n",
      "current_step:  481\n",
      "2017-08-23T03:08:55.841858: step 482, loss 0.494234, acc 0.88\n",
      "current_step:  482\n",
      "2017-08-23T03:08:55.953563: step 483, loss 0.492406, acc 0.86\n",
      "current_step:  483\n",
      "2017-08-23T03:08:56.065372: step 484, loss 0.430455, acc 0.88\n",
      "current_step:  484\n",
      "2017-08-23T03:08:56.176784: step 485, loss 0.362538, acc 0.9\n",
      "current_step:  485\n",
      "2017-08-23T03:08:56.288416: step 486, loss 0.240586, acc 0.96\n",
      "current_step:  486\n",
      "2017-08-23T03:08:56.400531: step 487, loss 0.437302, acc 0.88\n",
      "current_step:  487\n",
      "2017-08-23T03:08:56.512170: step 488, loss 0.383571, acc 0.86\n",
      "current_step:  488\n",
      "2017-08-23T03:08:56.624074: step 489, loss 0.48023, acc 0.86\n",
      "current_step:  489\n",
      "2017-08-23T03:08:56.736567: step 490, loss 0.277242, acc 0.94\n",
      "current_step:  490\n",
      "2017-08-23T03:08:56.849171: step 491, loss 0.487284, acc 0.8\n",
      "current_step:  491\n",
      "2017-08-23T03:08:56.963345: step 492, loss 0.515905, acc 0.88\n",
      "current_step:  492\n",
      "2017-08-23T03:08:57.075887: step 493, loss 0.363572, acc 0.88\n",
      "current_step:  493\n",
      "2017-08-23T03:08:57.189443: step 494, loss 0.47805, acc 0.86\n",
      "current_step:  494\n",
      "2017-08-23T03:08:57.301351: step 495, loss 0.400767, acc 0.9\n",
      "current_step:  495\n",
      "2017-08-23T03:08:57.412888: step 496, loss 0.495369, acc 0.84\n",
      "current_step:  496\n",
      "2017-08-23T03:08:57.524259: step 497, loss 0.389392, acc 0.88\n",
      "current_step:  497\n",
      "2017-08-23T03:08:57.637365: step 498, loss 0.434873, acc 0.88\n",
      "current_step:  498\n",
      "2017-08-23T03:08:57.749480: step 499, loss 0.49895, acc 0.88\n",
      "current_step:  499\n",
      "2017-08-23T03:08:57.861496: step 500, loss 0.300249, acc 0.94\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:08:58.265212: step 500, loss 0.860473, acc 0.701068\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0308_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:08:58.430929: step 501, loss 0.407796, acc 0.88\n",
      "current_step:  501\n",
      "2017-08-23T03:08:58.543751: step 502, loss 0.382415, acc 0.88\n",
      "current_step:  502\n",
      "2017-08-23T03:08:58.656825: step 503, loss 0.405856, acc 0.86\n",
      "current_step:  503\n",
      "2017-08-23T03:08:58.768799: step 504, loss 0.487502, acc 0.9\n",
      "current_step:  504\n",
      "2017-08-23T03:08:58.882523: step 505, loss 0.394844, acc 0.92\n",
      "current_step:  505\n",
      "2017-08-23T03:08:58.994881: step 506, loss 0.376171, acc 0.92\n",
      "current_step:  506\n",
      "2017-08-23T03:08:59.109514: step 507, loss 0.319444, acc 0.92\n",
      "current_step:  507\n",
      "2017-08-23T03:08:59.221269: step 508, loss 0.386709, acc 0.9\n",
      "current_step:  508\n",
      "2017-08-23T03:08:59.333503: step 509, loss 0.375065, acc 0.88\n",
      "current_step:  509\n",
      "2017-08-23T03:08:59.357610: step 510, loss 0.468791, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.592082\n",
      "Precision Score = 0.570915\n",
      "Recall Score = 0.633452\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.588999\n",
      "Precision Score = 0.599078\n",
      "Recall Score = 0.615658\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.646767\n",
      "Precision Score = 0.639921\n",
      "Recall Score = 0.688612\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.644697\n",
      "Precision Score = 0.649273\n",
      "Recall Score = 0.672598\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.667114\n",
      "Precision Score = 0.655006\n",
      "Recall Score = 0.693950\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v3 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 3, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  320\n",
      "Vocabulary Size: 20222\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0309_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0309_UTC\n",
      "2017-08-23T03:09:18.702140: step 1, loss 3.01912, acc 0.04\n",
      "current_step:  1\n",
      "2017-08-23T03:09:18.794672: step 2, loss 1.93971, acc 0.14\n",
      "current_step:  2\n",
      "2017-08-23T03:09:18.894374: step 3, loss 1.79899, acc 0.48\n",
      "current_step:  3\n",
      "2017-08-23T03:09:18.989998: step 4, loss 1.19963, acc 0.5\n",
      "current_step:  4\n",
      "2017-08-23T03:09:19.085034: step 5, loss 0.890985, acc 0.62\n",
      "current_step:  5\n",
      "2017-08-23T03:09:19.181276: step 6, loss 1.61626, acc 0.42\n",
      "current_step:  6\n",
      "2017-08-23T03:09:19.277671: step 7, loss 1.56966, acc 0.46\n",
      "current_step:  7\n",
      "2017-08-23T03:09:19.375756: step 8, loss 1.57618, acc 0.5\n",
      "current_step:  8\n",
      "2017-08-23T03:09:19.465699: step 9, loss 1.55041, acc 0.42\n",
      "current_step:  9\n",
      "2017-08-23T03:09:19.560353: step 10, loss 1.49754, acc 0.52\n",
      "current_step:  10\n",
      "2017-08-23T03:09:19.653819: step 11, loss 1.31601, acc 0.52\n",
      "current_step:  11\n",
      "2017-08-23T03:09:19.747101: step 12, loss 1.55023, acc 0.48\n",
      "current_step:  12\n",
      "2017-08-23T03:09:19.844215: step 13, loss 1.06905, acc 0.6\n",
      "current_step:  13\n",
      "2017-08-23T03:09:19.933643: step 14, loss 1.52881, acc 0.38\n",
      "current_step:  14\n",
      "2017-08-23T03:09:20.024359: step 15, loss 1.43209, acc 0.44\n",
      "current_step:  15\n",
      "2017-08-23T03:09:20.123003: step 16, loss 1.40523, acc 0.4\n",
      "current_step:  16\n",
      "2017-08-23T03:09:20.211321: step 17, loss 1.53614, acc 0.5\n",
      "current_step:  17\n",
      "2017-08-23T03:09:20.301195: step 18, loss 1.50318, acc 0.46\n",
      "current_step:  18\n",
      "2017-08-23T03:09:20.388395: step 19, loss 1.29527, acc 0.56\n",
      "current_step:  19\n",
      "2017-08-23T03:09:20.477904: step 20, loss 1.61728, acc 0.48\n",
      "current_step:  20\n",
      "2017-08-23T03:09:20.569326: step 21, loss 1.27009, acc 0.56\n",
      "current_step:  21\n",
      "2017-08-23T03:09:20.659376: step 22, loss 2.02236, acc 0.36\n",
      "current_step:  22\n",
      "2017-08-23T03:09:20.775525: step 23, loss 1.60964, acc 0.54\n",
      "current_step:  23\n",
      "2017-08-23T03:09:20.912640: step 24, loss 1.22275, acc 0.5\n",
      "current_step:  24\n",
      "2017-08-23T03:09:21.049621: step 25, loss 1.94007, acc 0.42\n",
      "current_step:  25\n",
      "2017-08-23T03:09:21.186649: step 26, loss 1.34916, acc 0.46\n",
      "current_step:  26\n",
      "2017-08-23T03:09:21.322914: step 27, loss 1.34618, acc 0.44\n",
      "current_step:  27\n",
      "2017-08-23T03:09:21.459375: step 28, loss 1.57475, acc 0.42\n",
      "current_step:  28\n",
      "2017-08-23T03:09:21.594698: step 29, loss 1.52109, acc 0.44\n",
      "current_step:  29\n",
      "2017-08-23T03:09:21.731421: step 30, loss 1.28771, acc 0.52\n",
      "current_step:  30\n",
      "2017-08-23T03:09:21.869214: step 31, loss 1.3704, acc 0.58\n",
      "current_step:  31\n",
      "2017-08-23T03:09:22.005840: step 32, loss 1.63202, acc 0.4\n",
      "current_step:  32\n",
      "2017-08-23T03:09:22.141501: step 33, loss 1.50544, acc 0.5\n",
      "current_step:  33\n",
      "2017-08-23T03:09:22.278838: step 34, loss 1.45341, acc 0.44\n",
      "current_step:  34\n",
      "2017-08-23T03:09:22.415649: step 35, loss 1.34063, acc 0.46\n",
      "current_step:  35\n",
      "2017-08-23T03:09:22.552012: step 36, loss 1.38373, acc 0.5\n",
      "current_step:  36\n",
      "2017-08-23T03:09:22.687146: step 37, loss 1.17045, acc 0.56\n",
      "current_step:  37\n",
      "2017-08-23T03:09:22.822816: step 38, loss 1.15911, acc 0.56\n",
      "current_step:  38\n",
      "2017-08-23T03:09:22.958712: step 39, loss 1.35638, acc 0.38\n",
      "current_step:  39\n",
      "2017-08-23T03:09:23.095645: step 40, loss 1.003, acc 0.58\n",
      "current_step:  40\n",
      "2017-08-23T03:09:23.230712: step 41, loss 1.40755, acc 0.54\n",
      "current_step:  41\n",
      "2017-08-23T03:09:23.367469: step 42, loss 1.70569, acc 0.38\n",
      "current_step:  42\n",
      "2017-08-23T03:09:23.504012: step 43, loss 1.12665, acc 0.62\n",
      "current_step:  43\n",
      "2017-08-23T03:09:23.640814: step 44, loss 1.17466, acc 0.52\n",
      "current_step:  44\n",
      "2017-08-23T03:09:23.777465: step 45, loss 1.17449, acc 0.56\n",
      "current_step:  45\n",
      "2017-08-23T03:09:23.913470: step 46, loss 1.14439, acc 0.54\n",
      "current_step:  46\n",
      "2017-08-23T03:09:24.050941: step 47, loss 1.16485, acc 0.62\n",
      "current_step:  47\n",
      "2017-08-23T03:09:24.187116: step 48, loss 1.34337, acc 0.46\n",
      "current_step:  48\n",
      "2017-08-23T03:09:24.322771: step 49, loss 1.14504, acc 0.54\n",
      "current_step:  49\n",
      "2017-08-23T03:09:24.458419: step 50, loss 1.29352, acc 0.4\n",
      "current_step:  50\n",
      "2017-08-23T03:09:24.595014: step 51, loss 1.02214, acc 0.58\n",
      "current_step:  51\n",
      "2017-08-23T03:09:24.732191: step 52, loss 1.07161, acc 0.62\n",
      "current_step:  52\n",
      "2017-08-23T03:09:24.868617: step 53, loss 1.14307, acc 0.54\n",
      "current_step:  53\n",
      "2017-08-23T03:09:25.005902: step 54, loss 1.25678, acc 0.48\n",
      "current_step:  54\n",
      "2017-08-23T03:09:25.142148: step 55, loss 1.15437, acc 0.6\n",
      "current_step:  55\n",
      "2017-08-23T03:09:25.277918: step 56, loss 1.00765, acc 0.7\n",
      "current_step:  56\n",
      "2017-08-23T03:09:25.413692: step 57, loss 1.65012, acc 0.48\n",
      "current_step:  57\n",
      "2017-08-23T03:09:25.549558: step 58, loss 1.19124, acc 0.56\n",
      "current_step:  58\n",
      "2017-08-23T03:09:25.684519: step 59, loss 1.27101, acc 0.48\n",
      "current_step:  59\n",
      "2017-08-23T03:09:25.820646: step 60, loss 1.32744, acc 0.58\n",
      "current_step:  60\n",
      "2017-08-23T03:09:25.957742: step 61, loss 1.12531, acc 0.64\n",
      "current_step:  61\n",
      "2017-08-23T03:09:26.094480: step 62, loss 1.25874, acc 0.54\n",
      "current_step:  62\n",
      "2017-08-23T03:09:26.231352: step 63, loss 1.15408, acc 0.54\n",
      "current_step:  63\n",
      "2017-08-23T03:09:26.368513: step 64, loss 1.25422, acc 0.56\n",
      "current_step:  64\n",
      "2017-08-23T03:09:26.504992: step 65, loss 1.32604, acc 0.56\n",
      "current_step:  65\n",
      "2017-08-23T03:09:26.641977: step 66, loss 1.15539, acc 0.62\n",
      "current_step:  66\n",
      "2017-08-23T03:09:26.778307: step 67, loss 1.0379, acc 0.7\n",
      "current_step:  67\n",
      "2017-08-23T03:09:26.915117: step 68, loss 1.19417, acc 0.54\n",
      "current_step:  68\n",
      "2017-08-23T03:09:27.052131: step 69, loss 1.19561, acc 0.62\n",
      "current_step:  69\n",
      "2017-08-23T03:09:27.188683: step 70, loss 1.06857, acc 0.66\n",
      "current_step:  70\n",
      "2017-08-23T03:09:27.324492: step 71, loss 1.47519, acc 0.5\n",
      "current_step:  71\n",
      "2017-08-23T03:09:27.460090: step 72, loss 1.16816, acc 0.58\n",
      "current_step:  72\n",
      "2017-08-23T03:09:27.596274: step 73, loss 1.44346, acc 0.44\n",
      "current_step:  73\n",
      "2017-08-23T03:09:27.732135: step 74, loss 1.22491, acc 0.58\n",
      "current_step:  74\n",
      "2017-08-23T03:09:27.869137: step 75, loss 0.980484, acc 0.62\n",
      "current_step:  75\n",
      "2017-08-23T03:09:28.005734: step 76, loss 1.18776, acc 0.58\n",
      "current_step:  76\n",
      "2017-08-23T03:09:28.141401: step 77, loss 1.06548, acc 0.64\n",
      "current_step:  77\n",
      "2017-08-23T03:09:28.277488: step 78, loss 1.13447, acc 0.54\n",
      "current_step:  78\n",
      "2017-08-23T03:09:28.414532: step 79, loss 1.26976, acc 0.48\n",
      "current_step:  79\n",
      "2017-08-23T03:09:28.551890: step 80, loss 1.32194, acc 0.52\n",
      "current_step:  80\n",
      "2017-08-23T03:09:28.688417: step 81, loss 1.18286, acc 0.6\n",
      "current_step:  81\n",
      "2017-08-23T03:09:28.825627: step 82, loss 1.06226, acc 0.58\n",
      "current_step:  82\n",
      "2017-08-23T03:09:28.961478: step 83, loss 1.07057, acc 0.64\n",
      "current_step:  83\n",
      "2017-08-23T03:09:29.098759: step 84, loss 1.34018, acc 0.56\n",
      "current_step:  84\n",
      "2017-08-23T03:09:29.234667: step 85, loss 1.15779, acc 0.6\n",
      "current_step:  85\n",
      "2017-08-23T03:09:29.371923: step 86, loss 1.08001, acc 0.58\n",
      "current_step:  86\n",
      "2017-08-23T03:09:29.507660: step 87, loss 1.25781, acc 0.52\n",
      "current_step:  87\n",
      "2017-08-23T03:09:29.644754: step 88, loss 1.10217, acc 0.6\n",
      "current_step:  88\n",
      "2017-08-23T03:09:29.780037: step 89, loss 1.00772, acc 0.62\n",
      "current_step:  89\n",
      "2017-08-23T03:09:29.915620: step 90, loss 1.17632, acc 0.58\n",
      "current_step:  90\n",
      "2017-08-23T03:09:30.051764: step 91, loss 1.22738, acc 0.5\n",
      "current_step:  91\n",
      "2017-08-23T03:09:30.188200: step 92, loss 1.36482, acc 0.48\n",
      "current_step:  92\n",
      "2017-08-23T03:09:30.323142: step 93, loss 1.06675, acc 0.64\n",
      "current_step:  93\n",
      "2017-08-23T03:09:30.458982: step 94, loss 1.3244, acc 0.5\n",
      "current_step:  94\n",
      "2017-08-23T03:09:30.596228: step 95, loss 1.19087, acc 0.54\n",
      "current_step:  95\n",
      "2017-08-23T03:09:30.732093: step 96, loss 1.17268, acc 0.58\n",
      "current_step:  96\n",
      "2017-08-23T03:09:30.868362: step 97, loss 1.24777, acc 0.5\n",
      "current_step:  97\n",
      "2017-08-23T03:09:31.005332: step 98, loss 1.16293, acc 0.6\n",
      "current_step:  98\n",
      "2017-08-23T03:09:31.141613: step 99, loss 1.19548, acc 0.56\n",
      "current_step:  99\n",
      "2017-08-23T03:09:31.278147: step 100, loss 1.08007, acc 0.6\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:09:31.776505: step 100, loss 1.06746, acc 0.587189\n",
      "\n",
      "2017-08-23T03:09:31.913729: step 101, loss 1.10788, acc 0.62\n",
      "current_step:  101\n",
      "2017-08-23T03:09:31.943690: step 102, loss 1.21835, acc 0.555556\n",
      "current_step:  102\n",
      "2017-08-23T03:09:32.081843: step 103, loss 0.928346, acc 0.62\n",
      "current_step:  103\n",
      "2017-08-23T03:09:32.218201: step 104, loss 0.969357, acc 0.56\n",
      "current_step:  104\n",
      "2017-08-23T03:09:32.355384: step 105, loss 1.2093, acc 0.66\n",
      "current_step:  105\n",
      "2017-08-23T03:09:32.493561: step 106, loss 0.66164, acc 0.82\n",
      "current_step:  106\n",
      "2017-08-23T03:09:32.633071: step 107, loss 0.926222, acc 0.7\n",
      "current_step:  107\n",
      "2017-08-23T03:09:32.770656: step 108, loss 0.685934, acc 0.74\n",
      "current_step:  108\n",
      "2017-08-23T03:09:32.908225: step 109, loss 1.02446, acc 0.58\n",
      "current_step:  109\n",
      "2017-08-23T03:09:33.046359: step 110, loss 0.932029, acc 0.5\n",
      "current_step:  110\n",
      "2017-08-23T03:09:33.182664: step 111, loss 1.09563, acc 0.5\n",
      "current_step:  111\n",
      "2017-08-23T03:09:33.316922: step 112, loss 0.921454, acc 0.68\n",
      "current_step:  112\n",
      "2017-08-23T03:09:33.454049: step 113, loss 0.953112, acc 0.66\n",
      "current_step:  113\n",
      "2017-08-23T03:09:33.588575: step 114, loss 0.955036, acc 0.82\n",
      "current_step:  114\n",
      "2017-08-23T03:09:33.724695: step 115, loss 0.681363, acc 0.82\n",
      "current_step:  115\n",
      "2017-08-23T03:09:33.860496: step 116, loss 0.98819, acc 0.64\n",
      "current_step:  116\n",
      "2017-08-23T03:09:33.996585: step 117, loss 0.848816, acc 0.76\n",
      "current_step:  117\n",
      "2017-08-23T03:09:34.132684: step 118, loss 0.961812, acc 0.76\n",
      "current_step:  118\n",
      "2017-08-23T03:09:34.268856: step 119, loss 0.904114, acc 0.68\n",
      "current_step:  119\n",
      "2017-08-23T03:09:34.404854: step 120, loss 0.903796, acc 0.68\n",
      "current_step:  120\n",
      "2017-08-23T03:09:34.540341: step 121, loss 0.732113, acc 0.76\n",
      "current_step:  121\n",
      "2017-08-23T03:09:34.675165: step 122, loss 0.780704, acc 0.68\n",
      "current_step:  122\n",
      "2017-08-23T03:09:34.811366: step 123, loss 0.929384, acc 0.7\n",
      "current_step:  123\n",
      "2017-08-23T03:09:34.947343: step 124, loss 0.944478, acc 0.58\n",
      "current_step:  124\n",
      "2017-08-23T03:09:35.083563: step 125, loss 0.794644, acc 0.74\n",
      "current_step:  125\n",
      "2017-08-23T03:09:35.219616: step 126, loss 0.807614, acc 0.7\n",
      "current_step:  126\n",
      "2017-08-23T03:09:35.354909: step 127, loss 0.616084, acc 0.86\n",
      "current_step:  127\n",
      "2017-08-23T03:09:35.489875: step 128, loss 1.2657, acc 0.54\n",
      "current_step:  128\n",
      "2017-08-23T03:09:35.626708: step 129, loss 0.904931, acc 0.76\n",
      "current_step:  129\n",
      "2017-08-23T03:09:35.762544: step 130, loss 0.771142, acc 0.7\n",
      "current_step:  130\n",
      "2017-08-23T03:09:35.897820: step 131, loss 1.06071, acc 0.56\n",
      "current_step:  131\n",
      "2017-08-23T03:09:36.034176: step 132, loss 0.838855, acc 0.74\n",
      "current_step:  132\n",
      "2017-08-23T03:09:36.170281: step 133, loss 0.843992, acc 0.76\n",
      "current_step:  133\n",
      "2017-08-23T03:09:36.306161: step 134, loss 1.01749, acc 0.74\n",
      "current_step:  134\n",
      "2017-08-23T03:09:36.441623: step 135, loss 1.04746, acc 0.7\n",
      "current_step:  135\n",
      "2017-08-23T03:09:36.577255: step 136, loss 0.810298, acc 0.7\n",
      "current_step:  136\n",
      "2017-08-23T03:09:36.712865: step 137, loss 0.984232, acc 0.64\n",
      "current_step:  137\n",
      "2017-08-23T03:09:36.847755: step 138, loss 0.862795, acc 0.66\n",
      "current_step:  138\n",
      "2017-08-23T03:09:36.983682: step 139, loss 0.961736, acc 0.64\n",
      "current_step:  139\n",
      "2017-08-23T03:09:37.120998: step 140, loss 0.77374, acc 0.76\n",
      "current_step:  140\n",
      "2017-08-23T03:09:37.256304: step 141, loss 0.986241, acc 0.64\n",
      "current_step:  141\n",
      "2017-08-23T03:09:37.392389: step 142, loss 0.924439, acc 0.76\n",
      "current_step:  142\n",
      "2017-08-23T03:09:37.527624: step 143, loss 0.754686, acc 0.68\n",
      "current_step:  143\n",
      "2017-08-23T03:09:37.662700: step 144, loss 0.927613, acc 0.66\n",
      "current_step:  144\n",
      "2017-08-23T03:09:37.799065: step 145, loss 0.857655, acc 0.7\n",
      "current_step:  145\n",
      "2017-08-23T03:09:37.933815: step 146, loss 0.758706, acc 0.76\n",
      "current_step:  146\n",
      "2017-08-23T03:09:38.070235: step 147, loss 0.968367, acc 0.74\n",
      "current_step:  147\n",
      "2017-08-23T03:09:38.206664: step 148, loss 0.845459, acc 0.7\n",
      "current_step:  148\n",
      "2017-08-23T03:09:38.341785: step 149, loss 0.774039, acc 0.76\n",
      "current_step:  149\n",
      "2017-08-23T03:09:38.478425: step 150, loss 1.05564, acc 0.56\n",
      "current_step:  150\n",
      "2017-08-23T03:09:38.612561: step 151, loss 0.743894, acc 0.78\n",
      "current_step:  151\n",
      "2017-08-23T03:09:38.747111: step 152, loss 0.748959, acc 0.7\n",
      "current_step:  152\n",
      "2017-08-23T03:09:38.883370: step 153, loss 0.996974, acc 0.68\n",
      "current_step:  153\n",
      "2017-08-23T03:09:39.021219: step 154, loss 0.909582, acc 0.7\n",
      "current_step:  154\n",
      "2017-08-23T03:09:39.156477: step 155, loss 0.766161, acc 0.78\n",
      "current_step:  155\n",
      "2017-08-23T03:09:39.291623: step 156, loss 1.06203, acc 0.64\n",
      "current_step:  156\n",
      "2017-08-23T03:09:39.427691: step 157, loss 0.807008, acc 0.8\n",
      "current_step:  157\n",
      "2017-08-23T03:09:39.563432: step 158, loss 0.832414, acc 0.8\n",
      "current_step:  158\n",
      "2017-08-23T03:09:39.699502: step 159, loss 0.747855, acc 0.84\n",
      "current_step:  159\n",
      "2017-08-23T03:09:39.835846: step 160, loss 0.722183, acc 0.78\n",
      "current_step:  160\n",
      "2017-08-23T03:09:39.973215: step 161, loss 0.791024, acc 0.74\n",
      "current_step:  161\n",
      "2017-08-23T03:09:40.110903: step 162, loss 0.784391, acc 0.68\n",
      "current_step:  162\n",
      "2017-08-23T03:09:40.246074: step 163, loss 0.988574, acc 0.6\n",
      "current_step:  163\n",
      "2017-08-23T03:09:40.382563: step 164, loss 0.633918, acc 0.86\n",
      "current_step:  164\n",
      "2017-08-23T03:09:40.517493: step 165, loss 0.989163, acc 0.64\n",
      "current_step:  165\n",
      "2017-08-23T03:09:40.653821: step 166, loss 0.834425, acc 0.8\n",
      "current_step:  166\n",
      "2017-08-23T03:09:40.789435: step 167, loss 0.868725, acc 0.68\n",
      "current_step:  167\n",
      "2017-08-23T03:09:40.924919: step 168, loss 0.885242, acc 0.72\n",
      "current_step:  168\n",
      "2017-08-23T03:09:41.061034: step 169, loss 0.923417, acc 0.76\n",
      "current_step:  169\n",
      "2017-08-23T03:09:41.198017: step 170, loss 0.829475, acc 0.76\n",
      "current_step:  170\n",
      "2017-08-23T03:09:41.332953: step 171, loss 0.821335, acc 0.68\n",
      "current_step:  171\n",
      "2017-08-23T03:09:41.468040: step 172, loss 0.879373, acc 0.7\n",
      "current_step:  172\n",
      "2017-08-23T03:09:41.604724: step 173, loss 0.874685, acc 0.74\n",
      "current_step:  173\n",
      "2017-08-23T03:09:41.740146: step 174, loss 0.884035, acc 0.7\n",
      "current_step:  174\n",
      "2017-08-23T03:09:41.875586: step 175, loss 0.810413, acc 0.72\n",
      "current_step:  175\n",
      "2017-08-23T03:09:42.011745: step 176, loss 0.682078, acc 0.82\n",
      "current_step:  176\n",
      "2017-08-23T03:09:42.147964: step 177, loss 0.85019, acc 0.76\n",
      "current_step:  177\n",
      "2017-08-23T03:09:42.283380: step 178, loss 1.07653, acc 0.66\n",
      "current_step:  178\n",
      "2017-08-23T03:09:42.418393: step 179, loss 0.657543, acc 0.86\n",
      "current_step:  179\n",
      "2017-08-23T03:09:42.554020: step 180, loss 0.787484, acc 0.84\n",
      "current_step:  180\n",
      "2017-08-23T03:09:42.690676: step 181, loss 0.801128, acc 0.74\n",
      "current_step:  181\n",
      "2017-08-23T03:09:42.826628: step 182, loss 0.72001, acc 0.8\n",
      "current_step:  182\n",
      "2017-08-23T03:09:42.962763: step 183, loss 0.658709, acc 0.8\n",
      "current_step:  183\n",
      "2017-08-23T03:09:43.099508: step 184, loss 0.741918, acc 0.84\n",
      "current_step:  184\n",
      "2017-08-23T03:09:43.235845: step 185, loss 0.82177, acc 0.68\n",
      "current_step:  185\n",
      "2017-08-23T03:09:43.370536: step 186, loss 0.857562, acc 0.66\n",
      "current_step:  186\n",
      "2017-08-23T03:09:43.505995: step 187, loss 1.0178, acc 0.6\n",
      "current_step:  187\n",
      "2017-08-23T03:09:43.642784: step 188, loss 0.611864, acc 0.82\n",
      "current_step:  188\n",
      "2017-08-23T03:09:43.778288: step 189, loss 0.822819, acc 0.76\n",
      "current_step:  189\n",
      "2017-08-23T03:09:43.914684: step 190, loss 0.58141, acc 0.92\n",
      "current_step:  190\n",
      "2017-08-23T03:09:44.051453: step 191, loss 0.754704, acc 0.76\n",
      "current_step:  191\n",
      "2017-08-23T03:09:44.189749: step 192, loss 0.732568, acc 0.76\n",
      "current_step:  192\n",
      "2017-08-23T03:09:44.325872: step 193, loss 0.818863, acc 0.76\n",
      "current_step:  193\n",
      "2017-08-23T03:09:44.462087: step 194, loss 0.910595, acc 0.76\n",
      "current_step:  194\n",
      "2017-08-23T03:09:44.599581: step 195, loss 0.868726, acc 0.8\n",
      "current_step:  195\n",
      "2017-08-23T03:09:44.735352: step 196, loss 0.816711, acc 0.66\n",
      "current_step:  196\n",
      "2017-08-23T03:09:44.872012: step 197, loss 0.720532, acc 0.76\n",
      "current_step:  197\n",
      "2017-08-23T03:09:45.008275: step 198, loss 0.711522, acc 0.7\n",
      "current_step:  198\n",
      "2017-08-23T03:09:45.144188: step 199, loss 0.913061, acc 0.66\n",
      "current_step:  199\n",
      "2017-08-23T03:09:45.280023: step 200, loss 0.786707, acc 0.68\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:09:45.780549: step 200, loss 0.985754, acc 0.631673\n",
      "\n",
      "2017-08-23T03:09:45.917001: step 201, loss 0.846506, acc 0.8\n",
      "current_step:  201\n",
      "2017-08-23T03:09:46.053553: step 202, loss 0.777993, acc 0.74\n",
      "current_step:  202\n",
      "2017-08-23T03:09:46.189377: step 203, loss 0.840025, acc 0.72\n",
      "current_step:  203\n",
      "2017-08-23T03:09:46.219604: step 204, loss 0.815117, acc 0.666667\n",
      "current_step:  204\n",
      "2017-08-23T03:09:46.355816: step 205, loss 0.563408, acc 0.88\n",
      "current_step:  205\n",
      "2017-08-23T03:09:46.492060: step 206, loss 0.765929, acc 0.72\n",
      "current_step:  206\n",
      "2017-08-23T03:09:46.630022: step 207, loss 0.599446, acc 0.88\n",
      "current_step:  207\n",
      "2017-08-23T03:09:46.767145: step 208, loss 0.529165, acc 0.92\n",
      "current_step:  208\n",
      "2017-08-23T03:09:46.905200: step 209, loss 0.531213, acc 0.9\n",
      "current_step:  209\n",
      "2017-08-23T03:09:47.042662: step 210, loss 0.620682, acc 0.84\n",
      "current_step:  210\n",
      "2017-08-23T03:09:47.180936: step 211, loss 0.690225, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-23T03:09:47.317017: step 212, loss 0.518304, acc 0.88\n",
      "current_step:  212\n",
      "2017-08-23T03:09:47.454141: step 213, loss 0.643112, acc 0.84\n",
      "current_step:  213\n",
      "2017-08-23T03:09:47.591012: step 214, loss 0.686913, acc 0.78\n",
      "current_step:  214\n",
      "2017-08-23T03:09:47.727901: step 215, loss 0.874836, acc 0.68\n",
      "current_step:  215\n",
      "2017-08-23T03:09:47.865511: step 216, loss 0.542055, acc 0.88\n",
      "current_step:  216\n",
      "2017-08-23T03:09:48.004656: step 217, loss 0.518536, acc 0.86\n",
      "current_step:  217\n",
      "2017-08-23T03:09:48.142298: step 218, loss 0.495551, acc 0.86\n",
      "current_step:  218\n",
      "2017-08-23T03:09:48.279417: step 219, loss 0.726183, acc 0.74\n",
      "current_step:  219\n",
      "2017-08-23T03:09:48.415715: step 220, loss 0.482544, acc 0.92\n",
      "current_step:  220\n",
      "2017-08-23T03:09:48.553475: step 221, loss 0.508166, acc 0.84\n",
      "current_step:  221\n",
      "2017-08-23T03:09:48.688841: step 222, loss 0.590005, acc 0.78\n",
      "current_step:  222\n",
      "2017-08-23T03:09:48.825748: step 223, loss 0.584301, acc 0.9\n",
      "current_step:  223\n",
      "2017-08-23T03:09:48.961999: step 224, loss 0.830807, acc 0.8\n",
      "current_step:  224\n",
      "2017-08-23T03:09:49.098107: step 225, loss 0.469313, acc 0.92\n",
      "current_step:  225\n",
      "2017-08-23T03:09:49.235975: step 226, loss 0.651192, acc 0.84\n",
      "current_step:  226\n",
      "2017-08-23T03:09:49.371961: step 227, loss 0.871533, acc 0.68\n",
      "current_step:  227\n",
      "2017-08-23T03:09:49.508736: step 228, loss 0.60434, acc 0.84\n",
      "current_step:  228\n",
      "2017-08-23T03:09:49.644471: step 229, loss 0.543185, acc 0.86\n",
      "current_step:  229\n",
      "2017-08-23T03:09:49.780396: step 230, loss 0.616611, acc 0.76\n",
      "current_step:  230\n",
      "2017-08-23T03:09:49.917849: step 231, loss 0.57437, acc 0.82\n",
      "current_step:  231\n",
      "2017-08-23T03:09:50.055765: step 232, loss 0.589801, acc 0.8\n",
      "current_step:  232\n",
      "2017-08-23T03:09:50.192380: step 233, loss 0.659576, acc 0.82\n",
      "current_step:  233\n",
      "2017-08-23T03:09:50.328316: step 234, loss 0.608787, acc 0.84\n",
      "current_step:  234\n",
      "2017-08-23T03:09:50.464259: step 235, loss 0.627663, acc 0.78\n",
      "current_step:  235\n",
      "2017-08-23T03:09:50.600392: step 236, loss 0.616405, acc 0.8\n",
      "current_step:  236\n",
      "2017-08-23T03:09:50.736044: step 237, loss 0.473314, acc 0.92\n",
      "current_step:  237\n",
      "2017-08-23T03:09:50.872528: step 238, loss 0.751297, acc 0.78\n",
      "current_step:  238\n",
      "2017-08-23T03:09:51.008502: step 239, loss 0.734096, acc 0.8\n",
      "current_step:  239\n",
      "2017-08-23T03:09:51.145188: step 240, loss 0.461381, acc 0.94\n",
      "current_step:  240\n",
      "2017-08-23T03:09:51.281280: step 241, loss 0.636337, acc 0.8\n",
      "current_step:  241\n",
      "2017-08-23T03:09:51.416314: step 242, loss 0.663329, acc 0.82\n",
      "current_step:  242\n",
      "2017-08-23T03:09:51.551848: step 243, loss 0.558386, acc 0.86\n",
      "current_step:  243\n",
      "2017-08-23T03:09:51.687331: step 244, loss 0.459452, acc 0.88\n",
      "current_step:  244\n",
      "2017-08-23T03:09:51.823043: step 245, loss 0.567015, acc 0.88\n",
      "current_step:  245\n",
      "2017-08-23T03:09:51.958621: step 246, loss 0.564111, acc 0.82\n",
      "current_step:  246\n",
      "2017-08-23T03:09:52.095434: step 247, loss 0.551626, acc 0.86\n",
      "current_step:  247\n",
      "2017-08-23T03:09:52.232307: step 248, loss 0.723965, acc 0.74\n",
      "current_step:  248\n",
      "2017-08-23T03:09:52.367764: step 249, loss 0.539393, acc 0.84\n",
      "current_step:  249\n",
      "2017-08-23T03:09:52.502298: step 250, loss 0.72601, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-23T03:09:52.639557: step 251, loss 0.593306, acc 0.86\n",
      "current_step:  251\n",
      "2017-08-23T03:09:52.774654: step 252, loss 0.50575, acc 0.9\n",
      "current_step:  252\n",
      "2017-08-23T03:09:52.912099: step 253, loss 0.808314, acc 0.74\n",
      "current_step:  253\n",
      "2017-08-23T03:09:53.048815: step 254, loss 0.503919, acc 0.86\n",
      "current_step:  254\n",
      "2017-08-23T03:09:53.186497: step 255, loss 0.654264, acc 0.84\n",
      "current_step:  255\n",
      "2017-08-23T03:09:53.322988: step 256, loss 0.616673, acc 0.84\n",
      "current_step:  256\n",
      "2017-08-23T03:09:53.459824: step 257, loss 0.627678, acc 0.7\n",
      "current_step:  257\n",
      "2017-08-23T03:09:53.595625: step 258, loss 0.551369, acc 0.82\n",
      "current_step:  258\n",
      "2017-08-23T03:09:53.731736: step 259, loss 0.641821, acc 0.74\n",
      "current_step:  259\n",
      "2017-08-23T03:09:53.867268: step 260, loss 0.535198, acc 0.78\n",
      "current_step:  260\n",
      "2017-08-23T03:09:54.004634: step 261, loss 0.636333, acc 0.78\n",
      "current_step:  261\n",
      "2017-08-23T03:09:54.140709: step 262, loss 0.688051, acc 0.78\n",
      "current_step:  262\n",
      "2017-08-23T03:09:54.277465: step 263, loss 0.560636, acc 0.84\n",
      "current_step:  263\n",
      "2017-08-23T03:09:54.412614: step 264, loss 0.538494, acc 0.86\n",
      "current_step:  264\n",
      "2017-08-23T03:09:54.549786: step 265, loss 0.527281, acc 0.84\n",
      "current_step:  265\n",
      "2017-08-23T03:09:54.685734: step 266, loss 0.561349, acc 0.84\n",
      "current_step:  266\n",
      "2017-08-23T03:09:54.822381: step 267, loss 0.765479, acc 0.82\n",
      "current_step:  267\n",
      "2017-08-23T03:09:54.958832: step 268, loss 0.575493, acc 0.86\n",
      "current_step:  268\n",
      "2017-08-23T03:09:55.095282: step 269, loss 0.607174, acc 0.88\n",
      "current_step:  269\n",
      "2017-08-23T03:09:55.231663: step 270, loss 0.614798, acc 0.84\n",
      "current_step:  270\n",
      "2017-08-23T03:09:55.367978: step 271, loss 0.651473, acc 0.78\n",
      "current_step:  271\n",
      "2017-08-23T03:09:55.504123: step 272, loss 0.65941, acc 0.78\n",
      "current_step:  272\n",
      "2017-08-23T03:09:55.640553: step 273, loss 0.721574, acc 0.7\n",
      "current_step:  273\n",
      "2017-08-23T03:09:55.775162: step 274, loss 0.720302, acc 0.78\n",
      "current_step:  274\n",
      "2017-08-23T03:09:55.912108: step 275, loss 0.44872, acc 0.86\n",
      "current_step:  275\n",
      "2017-08-23T03:09:56.047712: step 276, loss 0.572928, acc 0.78\n",
      "current_step:  276\n",
      "2017-08-23T03:09:56.184345: step 277, loss 0.415752, acc 0.88\n",
      "current_step:  277\n",
      "2017-08-23T03:09:56.320848: step 278, loss 0.772554, acc 0.68\n",
      "current_step:  278\n",
      "2017-08-23T03:09:56.456884: step 279, loss 0.631118, acc 0.78\n",
      "current_step:  279\n",
      "2017-08-23T03:09:56.591874: step 280, loss 0.499418, acc 0.84\n",
      "current_step:  280\n",
      "2017-08-23T03:09:56.727754: step 281, loss 0.520192, acc 0.88\n",
      "current_step:  281\n",
      "2017-08-23T03:09:56.863719: step 282, loss 0.711813, acc 0.8\n",
      "current_step:  282\n",
      "2017-08-23T03:09:57.000331: step 283, loss 0.43897, acc 0.94\n",
      "current_step:  283\n",
      "2017-08-23T03:09:57.137118: step 284, loss 0.609039, acc 0.88\n",
      "current_step:  284\n",
      "2017-08-23T03:09:57.273891: step 285, loss 0.655359, acc 0.8\n",
      "current_step:  285\n",
      "2017-08-23T03:09:57.409656: step 286, loss 0.650771, acc 0.78\n",
      "current_step:  286\n",
      "2017-08-23T03:09:57.546057: step 287, loss 0.604179, acc 0.82\n",
      "current_step:  287\n",
      "2017-08-23T03:09:57.681604: step 288, loss 0.550285, acc 0.78\n",
      "current_step:  288\n",
      "2017-08-23T03:09:57.816176: step 289, loss 0.679179, acc 0.76\n",
      "current_step:  289\n",
      "2017-08-23T03:09:57.953077: step 290, loss 0.684162, acc 0.76\n",
      "current_step:  290\n",
      "2017-08-23T03:09:58.089097: step 291, loss 0.67751, acc 0.76\n",
      "current_step:  291\n",
      "2017-08-23T03:09:58.224770: step 292, loss 0.572425, acc 0.88\n",
      "current_step:  292\n",
      "2017-08-23T03:09:58.360247: step 293, loss 0.541037, acc 0.86\n",
      "current_step:  293\n",
      "2017-08-23T03:09:58.496379: step 294, loss 0.66431, acc 0.8\n",
      "current_step:  294\n",
      "2017-08-23T03:09:58.632291: step 295, loss 0.592857, acc 0.9\n",
      "current_step:  295\n",
      "2017-08-23T03:09:58.767154: step 296, loss 0.557966, acc 0.84\n",
      "current_step:  296\n",
      "2017-08-23T03:09:58.905763: step 297, loss 0.50055, acc 0.82\n",
      "current_step:  297\n",
      "2017-08-23T03:09:59.041223: step 298, loss 0.64488, acc 0.78\n",
      "current_step:  298\n",
      "2017-08-23T03:09:59.176854: step 299, loss 0.369646, acc 0.9\n",
      "current_step:  299\n",
      "2017-08-23T03:09:59.312752: step 300, loss 0.630078, acc 0.84\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:09:59.804045: step 300, loss 0.973103, acc 0.631673\n",
      "\n",
      "2017-08-23T03:09:59.940139: step 301, loss 0.41647, acc 0.86\n",
      "current_step:  301\n",
      "2017-08-23T03:10:00.074709: step 302, loss 0.573529, acc 0.82\n",
      "current_step:  302\n",
      "2017-08-23T03:10:00.211243: step 303, loss 0.740798, acc 0.74\n",
      "current_step:  303\n",
      "2017-08-23T03:10:00.346660: step 304, loss 0.80626, acc 0.68\n",
      "current_step:  304\n",
      "2017-08-23T03:10:00.483526: step 305, loss 0.692775, acc 0.82\n",
      "current_step:  305\n",
      "2017-08-23T03:10:00.513128: step 306, loss 0.800337, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-23T03:10:00.649783: step 307, loss 0.37941, acc 0.96\n",
      "current_step:  307\n",
      "2017-08-23T03:10:00.785557: step 308, loss 0.483949, acc 0.92\n",
      "current_step:  308\n",
      "2017-08-23T03:10:00.921474: step 309, loss 0.36076, acc 0.94\n",
      "current_step:  309\n",
      "2017-08-23T03:10:01.058744: step 310, loss 0.42972, acc 0.86\n",
      "current_step:  310\n",
      "2017-08-23T03:10:01.195091: step 311, loss 0.38476, acc 0.96\n",
      "current_step:  311\n",
      "2017-08-23T03:10:01.331064: step 312, loss 0.436514, acc 0.86\n",
      "current_step:  312\n",
      "2017-08-23T03:10:01.472466: step 313, loss 0.453421, acc 0.92\n",
      "current_step:  313\n",
      "2017-08-23T03:10:01.607183: step 314, loss 0.421738, acc 0.9\n",
      "current_step:  314\n",
      "2017-08-23T03:10:01.743077: step 315, loss 0.472322, acc 0.94\n",
      "current_step:  315\n",
      "2017-08-23T03:10:01.878442: step 316, loss 0.368474, acc 0.92\n",
      "current_step:  316\n",
      "2017-08-23T03:10:02.015128: step 317, loss 0.295876, acc 0.96\n",
      "current_step:  317\n",
      "2017-08-23T03:10:02.151163: step 318, loss 0.583456, acc 0.84\n",
      "current_step:  318\n",
      "2017-08-23T03:10:02.289588: step 319, loss 0.570879, acc 0.82\n",
      "current_step:  319\n",
      "2017-08-23T03:10:02.426338: step 320, loss 0.509617, acc 0.88\n",
      "current_step:  320\n",
      "2017-08-23T03:10:02.563030: step 321, loss 0.450816, acc 0.84\n",
      "current_step:  321\n",
      "2017-08-23T03:10:02.698794: step 322, loss 0.467423, acc 0.86\n",
      "current_step:  322\n",
      "2017-08-23T03:10:02.834645: step 323, loss 0.330879, acc 0.96\n",
      "current_step:  323\n",
      "2017-08-23T03:10:02.969985: step 324, loss 0.529594, acc 0.9\n",
      "current_step:  324\n",
      "2017-08-23T03:10:03.106523: step 325, loss 0.387516, acc 0.9\n",
      "current_step:  325\n",
      "2017-08-23T03:10:03.241844: step 326, loss 0.49069, acc 0.86\n",
      "current_step:  326\n",
      "2017-08-23T03:10:03.378325: step 327, loss 0.433767, acc 0.94\n",
      "current_step:  327\n",
      "2017-08-23T03:10:03.514871: step 328, loss 0.429697, acc 0.88\n",
      "current_step:  328\n",
      "2017-08-23T03:10:03.652292: step 329, loss 0.445803, acc 0.92\n",
      "current_step:  329\n",
      "2017-08-23T03:10:03.790113: step 330, loss 0.408555, acc 0.9\n",
      "current_step:  330\n",
      "2017-08-23T03:10:03.926292: step 331, loss 0.489452, acc 0.88\n",
      "current_step:  331\n",
      "2017-08-23T03:10:04.063306: step 332, loss 0.482203, acc 0.86\n",
      "current_step:  332\n",
      "2017-08-23T03:10:04.198985: step 333, loss 0.401602, acc 0.92\n",
      "current_step:  333\n",
      "2017-08-23T03:10:04.334742: step 334, loss 0.422176, acc 0.92\n",
      "current_step:  334\n",
      "2017-08-23T03:10:04.470904: step 335, loss 0.545511, acc 0.84\n",
      "current_step:  335\n",
      "2017-08-23T03:10:04.606364: step 336, loss 0.420872, acc 0.84\n",
      "current_step:  336\n",
      "2017-08-23T03:10:04.742028: step 337, loss 0.513126, acc 0.84\n",
      "current_step:  337\n",
      "2017-08-23T03:10:04.878430: step 338, loss 0.395218, acc 0.92\n",
      "current_step:  338\n",
      "2017-08-23T03:10:05.013676: step 339, loss 0.411209, acc 0.86\n",
      "current_step:  339\n",
      "2017-08-23T03:10:05.148667: step 340, loss 0.404297, acc 0.88\n",
      "current_step:  340\n",
      "2017-08-23T03:10:05.283891: step 341, loss 0.5297, acc 0.84\n",
      "current_step:  341\n",
      "2017-08-23T03:10:05.418625: step 342, loss 0.40659, acc 0.9\n",
      "current_step:  342\n",
      "2017-08-23T03:10:05.554166: step 343, loss 0.488699, acc 0.9\n",
      "current_step:  343\n",
      "2017-08-23T03:10:05.689730: step 344, loss 0.463626, acc 0.9\n",
      "current_step:  344\n",
      "2017-08-23T03:10:05.827664: step 345, loss 0.440508, acc 0.96\n",
      "current_step:  345\n",
      "2017-08-23T03:10:05.964524: step 346, loss 0.437693, acc 0.92\n",
      "current_step:  346\n",
      "2017-08-23T03:10:06.100294: step 347, loss 0.427408, acc 0.92\n",
      "current_step:  347\n",
      "2017-08-23T03:10:06.235479: step 348, loss 0.379377, acc 0.94\n",
      "current_step:  348\n",
      "2017-08-23T03:10:06.371590: step 349, loss 0.386018, acc 0.92\n",
      "current_step:  349\n",
      "2017-08-23T03:10:06.507668: step 350, loss 0.397474, acc 0.94\n",
      "current_step:  350\n",
      "2017-08-23T03:10:06.642390: step 351, loss 0.453895, acc 0.9\n",
      "current_step:  351\n",
      "2017-08-23T03:10:06.776921: step 352, loss 0.534745, acc 0.84\n",
      "current_step:  352\n",
      "2017-08-23T03:10:06.912428: step 353, loss 0.424072, acc 0.92\n",
      "current_step:  353\n",
      "2017-08-23T03:10:07.047498: step 354, loss 0.446613, acc 0.86\n",
      "current_step:  354\n",
      "2017-08-23T03:10:07.182177: step 355, loss 0.480485, acc 0.88\n",
      "current_step:  355\n",
      "2017-08-23T03:10:07.317150: step 356, loss 0.335619, acc 0.94\n",
      "current_step:  356\n",
      "2017-08-23T03:10:07.452497: step 357, loss 0.352037, acc 0.92\n",
      "current_step:  357\n",
      "2017-08-23T03:10:07.587591: step 358, loss 0.442689, acc 0.84\n",
      "current_step:  358\n",
      "2017-08-23T03:10:07.724027: step 359, loss 0.406775, acc 0.86\n",
      "current_step:  359\n",
      "2017-08-23T03:10:07.860674: step 360, loss 0.441334, acc 0.88\n",
      "current_step:  360\n",
      "2017-08-23T03:10:07.997025: step 361, loss 0.55124, acc 0.82\n",
      "current_step:  361\n",
      "2017-08-23T03:10:08.134138: step 362, loss 0.303432, acc 0.98\n",
      "current_step:  362\n",
      "2017-08-23T03:10:08.269821: step 363, loss 0.463034, acc 0.84\n",
      "current_step:  363\n",
      "2017-08-23T03:10:08.405353: step 364, loss 0.428198, acc 0.92\n",
      "current_step:  364\n",
      "2017-08-23T03:10:08.542618: step 365, loss 0.411403, acc 0.92\n",
      "current_step:  365\n",
      "2017-08-23T03:10:08.678816: step 366, loss 0.471876, acc 0.92\n",
      "current_step:  366\n",
      "2017-08-23T03:10:08.814112: step 367, loss 0.442982, acc 0.9\n",
      "current_step:  367\n",
      "2017-08-23T03:10:08.949874: step 368, loss 0.450566, acc 0.94\n",
      "current_step:  368\n",
      "2017-08-23T03:10:09.085399: step 369, loss 0.412597, acc 0.9\n",
      "current_step:  369\n",
      "2017-08-23T03:10:09.222148: step 370, loss 0.378253, acc 0.9\n",
      "current_step:  370\n",
      "2017-08-23T03:10:09.358509: step 371, loss 0.319993, acc 0.96\n",
      "current_step:  371\n",
      "2017-08-23T03:10:09.493638: step 372, loss 0.381406, acc 0.92\n",
      "current_step:  372\n",
      "2017-08-23T03:10:09.629625: step 373, loss 0.391102, acc 0.88\n",
      "current_step:  373\n",
      "2017-08-23T03:10:09.764878: step 374, loss 0.381589, acc 0.88\n",
      "current_step:  374\n",
      "2017-08-23T03:10:09.900192: step 375, loss 0.50321, acc 0.82\n",
      "current_step:  375\n",
      "2017-08-23T03:10:10.036084: step 376, loss 0.659632, acc 0.8\n",
      "current_step:  376\n",
      "2017-08-23T03:10:10.172630: step 377, loss 0.442714, acc 0.84\n",
      "current_step:  377\n",
      "2017-08-23T03:10:10.306180: step 378, loss 0.491483, acc 0.84\n",
      "current_step:  378\n",
      "2017-08-23T03:10:10.441902: step 379, loss 0.369903, acc 0.92\n",
      "current_step:  379\n",
      "2017-08-23T03:10:10.577162: step 380, loss 0.336382, acc 0.94\n",
      "current_step:  380\n",
      "2017-08-23T03:10:10.715156: step 381, loss 0.354405, acc 0.94\n",
      "current_step:  381\n",
      "2017-08-23T03:10:10.851730: step 382, loss 0.352866, acc 0.94\n",
      "current_step:  382\n",
      "2017-08-23T03:10:10.987993: step 383, loss 0.416653, acc 0.9\n",
      "current_step:  383\n",
      "2017-08-23T03:10:11.124026: step 384, loss 0.303923, acc 0.94\n",
      "current_step:  384\n",
      "2017-08-23T03:10:11.258879: step 385, loss 0.42848, acc 0.9\n",
      "current_step:  385\n",
      "2017-08-23T03:10:11.395699: step 386, loss 0.530106, acc 0.86\n",
      "current_step:  386\n",
      "2017-08-23T03:10:11.532714: step 387, loss 0.351942, acc 0.88\n",
      "current_step:  387\n",
      "2017-08-23T03:10:11.667944: step 388, loss 0.307029, acc 0.98\n",
      "current_step:  388\n",
      "2017-08-23T03:10:11.805007: step 389, loss 0.422623, acc 0.88\n",
      "current_step:  389\n",
      "2017-08-23T03:10:11.941025: step 390, loss 0.376019, acc 0.92\n",
      "current_step:  390\n",
      "2017-08-23T03:10:12.077640: step 391, loss 0.336221, acc 0.96\n",
      "current_step:  391\n",
      "2017-08-23T03:10:12.212546: step 392, loss 0.335686, acc 0.92\n",
      "current_step:  392\n",
      "2017-08-23T03:10:12.348584: step 393, loss 0.37504, acc 0.92\n",
      "current_step:  393\n",
      "2017-08-23T03:10:12.484677: step 394, loss 0.487072, acc 0.92\n",
      "current_step:  394\n",
      "2017-08-23T03:10:12.620365: step 395, loss 0.439226, acc 0.84\n",
      "current_step:  395\n",
      "2017-08-23T03:10:12.756387: step 396, loss 0.26853, acc 0.98\n",
      "current_step:  396\n",
      "2017-08-23T03:10:12.893773: step 397, loss 0.634745, acc 0.8\n",
      "current_step:  397\n",
      "2017-08-23T03:10:13.029847: step 398, loss 0.36121, acc 0.9\n",
      "current_step:  398\n",
      "2017-08-23T03:10:13.165044: step 399, loss 0.402058, acc 0.9\n",
      "current_step:  399\n",
      "2017-08-23T03:10:13.299572: step 400, loss 0.553228, acc 0.8\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:10:13.790705: step 400, loss 0.907392, acc 0.66726\n",
      "\n",
      "2017-08-23T03:10:13.926776: step 401, loss 0.539708, acc 0.82\n",
      "current_step:  401\n",
      "2017-08-23T03:10:14.064195: step 402, loss 0.459851, acc 0.88\n",
      "current_step:  402\n",
      "2017-08-23T03:10:14.200259: step 403, loss 0.55457, acc 0.84\n",
      "current_step:  403\n",
      "2017-08-23T03:10:14.337757: step 404, loss 0.391071, acc 0.88\n",
      "current_step:  404\n",
      "2017-08-23T03:10:14.474826: step 405, loss 0.529107, acc 0.82\n",
      "current_step:  405\n",
      "2017-08-23T03:10:14.610204: step 406, loss 0.422003, acc 0.94\n",
      "current_step:  406\n",
      "2017-08-23T03:10:14.746661: step 407, loss 0.543633, acc 0.9\n",
      "current_step:  407\n",
      "2017-08-23T03:10:14.777265: step 408, loss 0.432201, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-23T03:10:14.914563: step 409, loss 0.317997, acc 0.92\n",
      "current_step:  409\n",
      "2017-08-23T03:10:15.049536: step 410, loss 0.314428, acc 0.98\n",
      "current_step:  410\n",
      "2017-08-23T03:10:15.185059: step 411, loss 0.302439, acc 0.92\n",
      "current_step:  411\n",
      "2017-08-23T03:10:15.320873: step 412, loss 0.390966, acc 0.94\n",
      "current_step:  412\n",
      "2017-08-23T03:10:15.456395: step 413, loss 0.258153, acc 1\n",
      "current_step:  413\n",
      "2017-08-23T03:10:15.591807: step 414, loss 0.206996, acc 1\n",
      "current_step:  414\n",
      "2017-08-23T03:10:15.727641: step 415, loss 0.298543, acc 0.9\n",
      "current_step:  415\n",
      "2017-08-23T03:10:15.862468: step 416, loss 0.32963, acc 0.94\n",
      "current_step:  416\n",
      "2017-08-23T03:10:15.998861: step 417, loss 0.29019, acc 0.94\n",
      "current_step:  417\n",
      "2017-08-23T03:10:16.133844: step 418, loss 0.27442, acc 0.96\n",
      "current_step:  418\n",
      "2017-08-23T03:10:16.270096: step 419, loss 0.346495, acc 0.86\n",
      "current_step:  419\n",
      "2017-08-23T03:10:16.405220: step 420, loss 0.251469, acc 0.94\n",
      "current_step:  420\n",
      "2017-08-23T03:10:16.541246: step 421, loss 0.240434, acc 0.98\n",
      "current_step:  421\n",
      "2017-08-23T03:10:16.676957: step 422, loss 0.337721, acc 0.92\n",
      "current_step:  422\n",
      "2017-08-23T03:10:16.813192: step 423, loss 0.275345, acc 0.98\n",
      "current_step:  423\n",
      "2017-08-23T03:10:16.949809: step 424, loss 0.21099, acc 0.98\n",
      "current_step:  424\n",
      "2017-08-23T03:10:17.088032: step 425, loss 0.345037, acc 0.94\n",
      "current_step:  425\n",
      "2017-08-23T03:10:17.224707: step 426, loss 0.211589, acc 0.98\n",
      "current_step:  426\n",
      "2017-08-23T03:10:17.362073: step 427, loss 0.272461, acc 0.98\n",
      "current_step:  427\n",
      "2017-08-23T03:10:17.498576: step 428, loss 0.30159, acc 0.96\n",
      "current_step:  428\n",
      "2017-08-23T03:10:17.633815: step 429, loss 0.237077, acc 0.98\n",
      "current_step:  429\n",
      "2017-08-23T03:10:17.769952: step 430, loss 0.234699, acc 0.96\n",
      "current_step:  430\n",
      "2017-08-23T03:10:17.908000: step 431, loss 0.306797, acc 0.94\n",
      "current_step:  431\n",
      "2017-08-23T03:10:18.045001: step 432, loss 0.390461, acc 0.88\n",
      "current_step:  432\n",
      "2017-08-23T03:10:18.180602: step 433, loss 0.364001, acc 0.94\n",
      "current_step:  433\n",
      "2017-08-23T03:10:18.315797: step 434, loss 0.279013, acc 0.92\n",
      "current_step:  434\n",
      "2017-08-23T03:10:18.451563: step 435, loss 0.280087, acc 0.94\n",
      "current_step:  435\n",
      "2017-08-23T03:10:18.586541: step 436, loss 0.294478, acc 0.92\n",
      "current_step:  436\n",
      "2017-08-23T03:10:18.720294: step 437, loss 0.35775, acc 0.94\n",
      "current_step:  437\n",
      "2017-08-23T03:10:18.856596: step 438, loss 0.202387, acc 0.98\n",
      "current_step:  438\n",
      "2017-08-23T03:10:18.992044: step 439, loss 0.230941, acc 0.96\n",
      "current_step:  439\n",
      "2017-08-23T03:10:19.126337: step 440, loss 0.290173, acc 0.98\n",
      "current_step:  440\n",
      "2017-08-23T03:10:19.262001: step 441, loss 0.278975, acc 0.98\n",
      "current_step:  441\n",
      "2017-08-23T03:10:19.397515: step 442, loss 0.443128, acc 0.84\n",
      "current_step:  442\n",
      "2017-08-23T03:10:19.533954: step 443, loss 0.353734, acc 0.88\n",
      "current_step:  443\n",
      "2017-08-23T03:10:19.669797: step 444, loss 0.289173, acc 0.96\n",
      "current_step:  444\n",
      "2017-08-23T03:10:19.806308: step 445, loss 0.382557, acc 0.88\n",
      "current_step:  445\n",
      "2017-08-23T03:10:19.942788: step 446, loss 0.265464, acc 0.94\n",
      "current_step:  446\n",
      "2017-08-23T03:10:20.080102: step 447, loss 0.257485, acc 0.96\n",
      "current_step:  447\n",
      "2017-08-23T03:10:20.215183: step 448, loss 0.279573, acc 0.96\n",
      "current_step:  448\n",
      "2017-08-23T03:10:20.350561: step 449, loss 0.379981, acc 0.92\n",
      "current_step:  449\n",
      "2017-08-23T03:10:20.486432: step 450, loss 0.234856, acc 1\n",
      "current_step:  450\n",
      "2017-08-23T03:10:20.622667: step 451, loss 0.294388, acc 0.96\n",
      "current_step:  451\n",
      "2017-08-23T03:10:20.758868: step 452, loss 0.31673, acc 0.96\n",
      "current_step:  452\n",
      "2017-08-23T03:10:20.898618: step 453, loss 0.25536, acc 0.94\n",
      "current_step:  453\n",
      "2017-08-23T03:10:21.038422: step 454, loss 0.353151, acc 0.94\n",
      "current_step:  454\n",
      "2017-08-23T03:10:21.177770: step 455, loss 0.318578, acc 0.94\n",
      "current_step:  455\n",
      "2017-08-23T03:10:21.316338: step 456, loss 0.329874, acc 0.96\n",
      "current_step:  456\n",
      "2017-08-23T03:10:21.454355: step 457, loss 0.38556, acc 0.9\n",
      "current_step:  457\n",
      "2017-08-23T03:10:21.592276: step 458, loss 0.190954, acc 1\n",
      "current_step:  458\n",
      "2017-08-23T03:10:21.729143: step 459, loss 0.376812, acc 0.92\n",
      "current_step:  459\n",
      "2017-08-23T03:10:21.866394: step 460, loss 0.315988, acc 0.94\n",
      "current_step:  460\n",
      "2017-08-23T03:10:22.003124: step 461, loss 0.522095, acc 0.86\n",
      "current_step:  461\n",
      "2017-08-23T03:10:22.141381: step 462, loss 0.245564, acc 0.98\n",
      "current_step:  462\n",
      "2017-08-23T03:10:22.279030: step 463, loss 0.315264, acc 0.9\n",
      "current_step:  463\n",
      "2017-08-23T03:10:22.416946: step 464, loss 0.266838, acc 0.98\n",
      "current_step:  464\n",
      "2017-08-23T03:10:22.556711: step 465, loss 0.355452, acc 0.88\n",
      "current_step:  465\n",
      "2017-08-23T03:10:22.692216: step 466, loss 0.306141, acc 0.92\n",
      "current_step:  466\n",
      "2017-08-23T03:10:22.830659: step 467, loss 0.282516, acc 0.94\n",
      "current_step:  467\n",
      "2017-08-23T03:10:22.971563: step 468, loss 0.210942, acc 0.98\n",
      "current_step:  468\n",
      "2017-08-23T03:10:23.111037: step 469, loss 0.291889, acc 0.94\n",
      "current_step:  469\n",
      "2017-08-23T03:10:23.249905: step 470, loss 0.319406, acc 0.92\n",
      "current_step:  470\n",
      "2017-08-23T03:10:23.391106: step 471, loss 0.360414, acc 0.92\n",
      "current_step:  471\n",
      "2017-08-23T03:10:23.534448: step 472, loss 0.321358, acc 0.9\n",
      "current_step:  472\n",
      "2017-08-23T03:10:23.675630: step 473, loss 0.251849, acc 1\n",
      "current_step:  473\n",
      "2017-08-23T03:10:23.814793: step 474, loss 0.282688, acc 0.94\n",
      "current_step:  474\n",
      "2017-08-23T03:10:23.954670: step 475, loss 0.387189, acc 0.94\n",
      "current_step:  475\n",
      "2017-08-23T03:10:24.093406: step 476, loss 0.34286, acc 0.88\n",
      "current_step:  476\n",
      "2017-08-23T03:10:24.232638: step 477, loss 0.349226, acc 0.92\n",
      "current_step:  477\n",
      "2017-08-23T03:10:24.370645: step 478, loss 0.263765, acc 0.94\n",
      "current_step:  478\n",
      "2017-08-23T03:10:24.509585: step 479, loss 0.312703, acc 0.96\n",
      "current_step:  479\n",
      "2017-08-23T03:10:24.647739: step 480, loss 0.303569, acc 0.92\n",
      "current_step:  480\n",
      "2017-08-23T03:10:24.786445: step 481, loss 0.264674, acc 0.94\n",
      "current_step:  481\n",
      "2017-08-23T03:10:24.925525: step 482, loss 0.228743, acc 0.96\n",
      "current_step:  482\n",
      "2017-08-23T03:10:25.065095: step 483, loss 0.297328, acc 0.94\n",
      "current_step:  483\n",
      "2017-08-23T03:10:25.204230: step 484, loss 0.262701, acc 0.96\n",
      "current_step:  484\n",
      "2017-08-23T03:10:25.343018: step 485, loss 0.324907, acc 0.98\n",
      "current_step:  485\n",
      "2017-08-23T03:10:25.484187: step 486, loss 0.313325, acc 0.9\n",
      "current_step:  486\n",
      "2017-08-23T03:10:25.620440: step 487, loss 0.275019, acc 0.96\n",
      "current_step:  487\n",
      "2017-08-23T03:10:25.756996: step 488, loss 0.40541, acc 0.88\n",
      "current_step:  488\n",
      "2017-08-23T03:10:25.895409: step 489, loss 0.372794, acc 0.94\n",
      "current_step:  489\n",
      "2017-08-23T03:10:26.033733: step 490, loss 0.297725, acc 0.92\n",
      "current_step:  490\n",
      "2017-08-23T03:10:26.173134: step 491, loss 0.391625, acc 0.88\n",
      "current_step:  491\n",
      "2017-08-23T03:10:26.311768: step 492, loss 0.245592, acc 0.94\n",
      "current_step:  492\n",
      "2017-08-23T03:10:26.450503: step 493, loss 0.320544, acc 0.94\n",
      "current_step:  493\n",
      "2017-08-23T03:10:26.591473: step 494, loss 0.41512, acc 0.88\n",
      "current_step:  494\n",
      "2017-08-23T03:10:26.728903: step 495, loss 0.315272, acc 0.94\n",
      "current_step:  495\n",
      "2017-08-23T03:10:26.865769: step 496, loss 0.370729, acc 0.96\n",
      "current_step:  496\n",
      "2017-08-23T03:10:27.003296: step 497, loss 0.261135, acc 0.94\n",
      "current_step:  497\n",
      "2017-08-23T03:10:27.139899: step 498, loss 0.348908, acc 0.9\n",
      "current_step:  498\n",
      "2017-08-23T03:10:27.276129: step 499, loss 0.394627, acc 0.88\n",
      "current_step:  499\n",
      "2017-08-23T03:10:27.412261: step 500, loss 0.304724, acc 0.96\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:10:27.908894: step 500, loss 0.909815, acc 0.679715\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0309_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:10:28.108106: step 501, loss 0.338094, acc 0.94\n",
      "current_step:  501\n",
      "2017-08-23T03:10:28.244875: step 502, loss 0.359795, acc 0.9\n",
      "current_step:  502\n",
      "2017-08-23T03:10:28.381829: step 503, loss 0.418845, acc 0.86\n",
      "current_step:  503\n",
      "2017-08-23T03:10:28.520814: step 504, loss 0.311751, acc 0.92\n",
      "current_step:  504\n",
      "2017-08-23T03:10:28.658497: step 505, loss 0.43558, acc 0.9\n",
      "current_step:  505\n",
      "2017-08-23T03:10:28.796692: step 506, loss 0.289836, acc 0.9\n",
      "current_step:  506\n",
      "2017-08-23T03:10:28.935597: step 507, loss 0.328039, acc 0.9\n",
      "current_step:  507\n",
      "2017-08-23T03:10:29.075838: step 508, loss 0.539369, acc 0.76\n",
      "current_step:  508\n",
      "2017-08-23T03:10:29.213674: step 509, loss 0.361354, acc 0.88\n",
      "current_step:  509\n",
      "2017-08-23T03:10:29.244675: step 510, loss 0.520856, acc 0.777778\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.531195\n",
      "Precision Score = 0.506136\n",
      "Recall Score = 0.583630\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.573966\n",
      "Precision Score = 0.559518\n",
      "Recall Score = 0.626335\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.567882\n",
      "Precision Score = 0.577396\n",
      "Recall Score = 0.624555\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.628864\n",
      "Precision Score = 0.631523\n",
      "Recall Score = 0.661922\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.650949\n",
      "Precision Score = 0.647040\n",
      "Recall Score = 0.676157\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v1 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 1, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  205\n",
      "Vocabulary Size: 13764\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0310_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0310_UTC\n",
      "2017-08-23T03:10:47.272967: step 1, loss 7.2555, acc 0.04\n",
      "current_step:  1\n",
      "2017-08-23T03:10:47.340437: step 2, loss 4.41457, acc 0.24\n",
      "current_step:  2\n",
      "2017-08-23T03:10:47.405412: step 3, loss 4.04104, acc 0.26\n",
      "current_step:  3\n",
      "2017-08-23T03:10:47.479342: step 4, loss 3.18448, acc 0.26\n",
      "current_step:  4\n",
      "2017-08-23T03:10:47.548936: step 5, loss 3.05283, acc 0.32\n",
      "current_step:  5\n",
      "2017-08-23T03:10:47.616171: step 6, loss 2.64419, acc 0.24\n",
      "current_step:  6\n",
      "2017-08-23T03:10:47.683953: step 7, loss 1.98713, acc 0.24\n",
      "current_step:  7\n",
      "2017-08-23T03:10:47.753197: step 8, loss 1.6498, acc 0.38\n",
      "current_step:  8\n",
      "2017-08-23T03:10:47.818565: step 9, loss 1.66788, acc 0.5\n",
      "current_step:  9\n",
      "2017-08-23T03:10:47.886322: step 10, loss 2.06616, acc 0.34\n",
      "current_step:  10\n",
      "2017-08-23T03:10:47.956106: step 11, loss 2.10565, acc 0.5\n",
      "current_step:  11\n",
      "2017-08-23T03:10:48.032545: step 12, loss 1.8243, acc 0.5\n",
      "current_step:  12\n",
      "2017-08-23T03:10:48.104505: step 13, loss 2.75524, acc 0.32\n",
      "current_step:  13\n",
      "2017-08-23T03:10:48.174078: step 14, loss 1.54577, acc 0.62\n",
      "current_step:  14\n",
      "2017-08-23T03:10:48.238664: step 15, loss 1.37998, acc 0.64\n",
      "current_step:  15\n",
      "2017-08-23T03:10:48.306420: step 16, loss 1.7883, acc 0.46\n",
      "current_step:  16\n",
      "2017-08-23T03:10:48.376272: step 17, loss 1.5244, acc 0.5\n",
      "current_step:  17\n",
      "2017-08-23T03:10:48.442152: step 18, loss 1.45259, acc 0.56\n",
      "current_step:  18\n",
      "2017-08-23T03:10:48.515631: step 19, loss 1.62665, acc 0.54\n",
      "current_step:  19\n",
      "2017-08-23T03:10:48.577464: step 20, loss 1.39626, acc 0.46\n",
      "current_step:  20\n",
      "2017-08-23T03:10:48.647364: step 21, loss 1.54606, acc 0.4\n",
      "current_step:  21\n",
      "2017-08-23T03:10:48.710852: step 22, loss 1.18502, acc 0.56\n",
      "current_step:  22\n",
      "2017-08-23T03:10:48.801983: step 23, loss 1.62187, acc 0.36\n",
      "current_step:  23\n",
      "2017-08-23T03:10:48.892519: step 24, loss 1.3569, acc 0.48\n",
      "current_step:  24\n",
      "2017-08-23T03:10:48.981847: step 25, loss 1.57235, acc 0.42\n",
      "current_step:  25\n",
      "2017-08-23T03:10:49.070617: step 26, loss 1.42362, acc 0.48\n",
      "current_step:  26\n",
      "2017-08-23T03:10:49.159050: step 27, loss 1.18323, acc 0.54\n",
      "current_step:  27\n",
      "2017-08-23T03:10:49.248873: step 28, loss 1.18931, acc 0.46\n",
      "current_step:  28\n",
      "2017-08-23T03:10:49.338190: step 29, loss 1.13866, acc 0.56\n",
      "current_step:  29\n",
      "2017-08-23T03:10:49.428105: step 30, loss 1.20641, acc 0.58\n",
      "current_step:  30\n",
      "2017-08-23T03:10:49.518454: step 31, loss 1.34626, acc 0.42\n",
      "current_step:  31\n",
      "2017-08-23T03:10:49.607848: step 32, loss 1.2473, acc 0.6\n",
      "current_step:  32\n",
      "2017-08-23T03:10:49.696926: step 33, loss 1.36063, acc 0.46\n",
      "current_step:  33\n",
      "2017-08-23T03:10:49.785652: step 34, loss 1.44485, acc 0.44\n",
      "current_step:  34\n",
      "2017-08-23T03:10:49.875289: step 35, loss 1.34944, acc 0.52\n",
      "current_step:  35\n",
      "2017-08-23T03:10:49.964729: step 36, loss 1.36978, acc 0.44\n",
      "current_step:  36\n",
      "2017-08-23T03:10:50.053437: step 37, loss 1.40454, acc 0.54\n",
      "current_step:  37\n",
      "2017-08-23T03:10:50.142940: step 38, loss 1.33941, acc 0.48\n",
      "current_step:  38\n",
      "2017-08-23T03:10:50.231617: step 39, loss 1.49336, acc 0.46\n",
      "current_step:  39\n",
      "2017-08-23T03:10:50.320465: step 40, loss 1.42893, acc 0.52\n",
      "current_step:  40\n",
      "2017-08-23T03:10:50.409524: step 41, loss 1.2988, acc 0.46\n",
      "current_step:  41\n",
      "2017-08-23T03:10:50.498558: step 42, loss 1.08251, acc 0.58\n",
      "current_step:  42\n",
      "2017-08-23T03:10:50.587632: step 43, loss 1.17891, acc 0.54\n",
      "current_step:  43\n",
      "2017-08-23T03:10:50.676371: step 44, loss 1.12424, acc 0.54\n",
      "current_step:  44\n",
      "2017-08-23T03:10:50.765527: step 45, loss 1.2222, acc 0.62\n",
      "current_step:  45\n",
      "2017-08-23T03:10:50.853915: step 46, loss 1.33488, acc 0.6\n",
      "current_step:  46\n",
      "2017-08-23T03:10:50.942683: step 47, loss 1.23691, acc 0.52\n",
      "current_step:  47\n",
      "2017-08-23T03:10:51.032306: step 48, loss 1.09634, acc 0.62\n",
      "current_step:  48\n",
      "2017-08-23T03:10:51.121723: step 49, loss 1.12313, acc 0.6\n",
      "current_step:  49\n",
      "2017-08-23T03:10:51.211460: step 50, loss 1.13392, acc 0.58\n",
      "current_step:  50\n",
      "2017-08-23T03:10:51.300292: step 51, loss 1.34844, acc 0.52\n",
      "current_step:  51\n",
      "2017-08-23T03:10:51.388740: step 52, loss 1.20297, acc 0.58\n",
      "current_step:  52\n",
      "2017-08-23T03:10:51.478083: step 53, loss 1.17603, acc 0.54\n",
      "current_step:  53\n",
      "2017-08-23T03:10:51.567277: step 54, loss 1.26559, acc 0.56\n",
      "current_step:  54\n",
      "2017-08-23T03:10:51.656195: step 55, loss 1.49801, acc 0.48\n",
      "current_step:  55\n",
      "2017-08-23T03:10:51.745982: step 56, loss 1.02831, acc 0.64\n",
      "current_step:  56\n",
      "2017-08-23T03:10:51.834679: step 57, loss 1.40128, acc 0.52\n",
      "current_step:  57\n",
      "2017-08-23T03:10:51.922927: step 58, loss 1.1577, acc 0.56\n",
      "current_step:  58\n",
      "2017-08-23T03:10:52.012307: step 59, loss 1.15995, acc 0.56\n",
      "current_step:  59\n",
      "2017-08-23T03:10:52.101678: step 60, loss 1.32413, acc 0.52\n",
      "current_step:  60\n",
      "2017-08-23T03:10:52.191061: step 61, loss 1.30988, acc 0.58\n",
      "current_step:  61\n",
      "2017-08-23T03:10:52.281627: step 62, loss 1.08995, acc 0.6\n",
      "current_step:  62\n",
      "2017-08-23T03:10:52.371144: step 63, loss 1.1766, acc 0.52\n",
      "current_step:  63\n",
      "2017-08-23T03:10:52.460366: step 64, loss 1.31016, acc 0.56\n",
      "current_step:  64\n",
      "2017-08-23T03:10:52.549330: step 65, loss 1.01064, acc 0.62\n",
      "current_step:  65\n",
      "2017-08-23T03:10:52.637808: step 66, loss 1.18205, acc 0.6\n",
      "current_step:  66\n",
      "2017-08-23T03:10:52.726862: step 67, loss 1.09658, acc 0.6\n",
      "current_step:  67\n",
      "2017-08-23T03:10:52.816232: step 68, loss 1.02026, acc 0.56\n",
      "current_step:  68\n",
      "2017-08-23T03:10:52.905750: step 69, loss 1.16327, acc 0.54\n",
      "current_step:  69\n",
      "2017-08-23T03:10:52.995191: step 70, loss 1.06827, acc 0.6\n",
      "current_step:  70\n",
      "2017-08-23T03:10:53.084999: step 71, loss 1.06735, acc 0.68\n",
      "current_step:  71\n",
      "2017-08-23T03:10:53.175049: step 72, loss 1.34696, acc 0.58\n",
      "current_step:  72\n",
      "2017-08-23T03:10:53.264044: step 73, loss 1.05087, acc 0.68\n",
      "current_step:  73\n",
      "2017-08-23T03:10:53.352477: step 74, loss 1.20364, acc 0.58\n",
      "current_step:  74\n",
      "2017-08-23T03:10:53.441119: step 75, loss 1.27491, acc 0.58\n",
      "current_step:  75\n",
      "2017-08-23T03:10:53.531642: step 76, loss 1.37658, acc 0.48\n",
      "current_step:  76\n",
      "2017-08-23T03:10:53.622548: step 77, loss 1.02732, acc 0.62\n",
      "current_step:  77\n",
      "2017-08-23T03:10:53.712910: step 78, loss 1.04405, acc 0.64\n",
      "current_step:  78\n",
      "2017-08-23T03:10:53.804323: step 79, loss 1.20773, acc 0.54\n",
      "current_step:  79\n",
      "2017-08-23T03:10:53.895363: step 80, loss 1.17504, acc 0.56\n",
      "current_step:  80\n",
      "2017-08-23T03:10:53.986231: step 81, loss 1.20615, acc 0.5\n",
      "current_step:  81\n",
      "2017-08-23T03:10:54.075954: step 82, loss 0.975444, acc 0.68\n",
      "current_step:  82\n",
      "2017-08-23T03:10:54.165449: step 83, loss 1.02434, acc 0.66\n",
      "current_step:  83\n",
      "2017-08-23T03:10:54.254764: step 84, loss 1.11183, acc 0.7\n",
      "current_step:  84\n",
      "2017-08-23T03:10:54.343423: step 85, loss 1.03046, acc 0.54\n",
      "current_step:  85\n",
      "2017-08-23T03:10:54.433530: step 86, loss 1.39452, acc 0.56\n",
      "current_step:  86\n",
      "2017-08-23T03:10:54.523268: step 87, loss 0.970541, acc 0.64\n",
      "current_step:  87\n",
      "2017-08-23T03:10:54.613111: step 88, loss 1.03315, acc 0.66\n",
      "current_step:  88\n",
      "2017-08-23T03:10:54.702509: step 89, loss 1.1354, acc 0.56\n",
      "current_step:  89\n",
      "2017-08-23T03:10:54.791872: step 90, loss 1.28541, acc 0.52\n",
      "current_step:  90\n",
      "2017-08-23T03:10:54.880789: step 91, loss 1.04824, acc 0.68\n",
      "current_step:  91\n",
      "2017-08-23T03:10:54.971386: step 92, loss 1.26985, acc 0.48\n",
      "current_step:  92\n",
      "2017-08-23T03:10:55.061364: step 93, loss 1.13379, acc 0.6\n",
      "current_step:  93\n",
      "2017-08-23T03:10:55.150495: step 94, loss 1.06431, acc 0.6\n",
      "current_step:  94\n",
      "2017-08-23T03:10:55.239558: step 95, loss 0.953453, acc 0.66\n",
      "current_step:  95\n",
      "2017-08-23T03:10:55.327878: step 96, loss 1.07941, acc 0.62\n",
      "current_step:  96\n",
      "2017-08-23T03:10:55.416024: step 97, loss 1.00643, acc 0.64\n",
      "current_step:  97\n",
      "2017-08-23T03:10:55.504685: step 98, loss 1.09396, acc 0.58\n",
      "current_step:  98\n",
      "2017-08-23T03:10:55.592903: step 99, loss 1.16308, acc 0.54\n",
      "current_step:  99\n",
      "2017-08-23T03:10:55.681019: step 100, loss 1.43293, acc 0.56\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:10:56.000701: step 100, loss 1.06199, acc 0.599644\n",
      "\n",
      "2017-08-23T03:10:56.090106: step 101, loss 1.26627, acc 0.48\n",
      "current_step:  101\n",
      "2017-08-23T03:10:56.111634: step 102, loss 1.00319, acc 0.777778\n",
      "current_step:  102\n",
      "2017-08-23T03:10:56.203383: step 103, loss 0.911145, acc 0.64\n",
      "current_step:  103\n",
      "2017-08-23T03:10:56.292528: step 104, loss 0.84945, acc 0.68\n",
      "current_step:  104\n",
      "2017-08-23T03:10:56.382060: step 105, loss 0.690242, acc 0.76\n",
      "current_step:  105\n",
      "2017-08-23T03:10:56.471645: step 106, loss 0.792581, acc 0.66\n",
      "current_step:  106\n",
      "2017-08-23T03:10:56.562252: step 107, loss 0.868281, acc 0.74\n",
      "current_step:  107\n",
      "2017-08-23T03:10:56.651121: step 108, loss 0.899153, acc 0.62\n",
      "current_step:  108\n",
      "2017-08-23T03:10:56.740186: step 109, loss 0.935443, acc 0.72\n",
      "current_step:  109\n",
      "2017-08-23T03:10:56.829740: step 110, loss 0.857933, acc 0.7\n",
      "current_step:  110\n",
      "2017-08-23T03:10:56.919496: step 111, loss 1.0274, acc 0.72\n",
      "current_step:  111\n",
      "2017-08-23T03:10:57.008815: step 112, loss 1.23846, acc 0.58\n",
      "current_step:  112\n",
      "2017-08-23T03:10:57.098490: step 113, loss 0.812455, acc 0.76\n",
      "current_step:  113\n",
      "2017-08-23T03:10:57.187654: step 114, loss 1.01866, acc 0.62\n",
      "current_step:  114\n",
      "2017-08-23T03:10:57.277883: step 115, loss 0.891443, acc 0.7\n",
      "current_step:  115\n",
      "2017-08-23T03:10:57.368184: step 116, loss 0.647793, acc 0.82\n",
      "current_step:  116\n",
      "2017-08-23T03:10:57.458206: step 117, loss 0.744028, acc 0.72\n",
      "current_step:  117\n",
      "2017-08-23T03:10:57.547634: step 118, loss 0.799993, acc 0.68\n",
      "current_step:  118\n",
      "2017-08-23T03:10:57.637620: step 119, loss 0.696436, acc 0.74\n",
      "current_step:  119\n",
      "2017-08-23T03:10:57.726079: step 120, loss 1.14383, acc 0.54\n",
      "current_step:  120\n",
      "2017-08-23T03:10:57.816133: step 121, loss 0.813562, acc 0.7\n",
      "current_step:  121\n",
      "2017-08-23T03:10:57.907887: step 122, loss 0.908748, acc 0.68\n",
      "current_step:  122\n",
      "2017-08-23T03:10:57.997598: step 123, loss 0.910847, acc 0.64\n",
      "current_step:  123\n",
      "2017-08-23T03:10:58.088277: step 124, loss 0.851092, acc 0.7\n",
      "current_step:  124\n",
      "2017-08-23T03:10:58.178404: step 125, loss 0.964182, acc 0.7\n",
      "current_step:  125\n",
      "2017-08-23T03:10:58.267931: step 126, loss 0.693571, acc 0.8\n",
      "current_step:  126\n",
      "2017-08-23T03:10:58.358313: step 127, loss 0.748195, acc 0.8\n",
      "current_step:  127\n",
      "2017-08-23T03:10:58.447402: step 128, loss 0.895833, acc 0.7\n",
      "current_step:  128\n",
      "2017-08-23T03:10:58.538059: step 129, loss 1.01032, acc 0.56\n",
      "current_step:  129\n",
      "2017-08-23T03:10:58.628100: step 130, loss 0.825714, acc 0.76\n",
      "current_step:  130\n",
      "2017-08-23T03:10:58.717133: step 131, loss 0.765961, acc 0.7\n",
      "current_step:  131\n",
      "2017-08-23T03:10:58.807370: step 132, loss 0.795054, acc 0.74\n",
      "current_step:  132\n",
      "2017-08-23T03:10:58.896899: step 133, loss 0.97433, acc 0.64\n",
      "current_step:  133\n",
      "2017-08-23T03:10:58.987014: step 134, loss 0.959641, acc 0.58\n",
      "current_step:  134\n",
      "2017-08-23T03:10:59.075598: step 135, loss 1.06068, acc 0.6\n",
      "current_step:  135\n",
      "2017-08-23T03:10:59.166280: step 136, loss 0.75858, acc 0.72\n",
      "current_step:  136\n",
      "2017-08-23T03:10:59.256107: step 137, loss 0.810836, acc 0.68\n",
      "current_step:  137\n",
      "2017-08-23T03:10:59.345234: step 138, loss 0.971508, acc 0.6\n",
      "current_step:  138\n",
      "2017-08-23T03:10:59.435330: step 139, loss 0.897219, acc 0.64\n",
      "current_step:  139\n",
      "2017-08-23T03:10:59.523975: step 140, loss 0.714474, acc 0.78\n",
      "current_step:  140\n",
      "2017-08-23T03:10:59.612709: step 141, loss 0.88902, acc 0.66\n",
      "current_step:  141\n",
      "2017-08-23T03:10:59.701864: step 142, loss 0.889564, acc 0.74\n",
      "current_step:  142\n",
      "2017-08-23T03:10:59.790808: step 143, loss 1.1555, acc 0.54\n",
      "current_step:  143\n",
      "2017-08-23T03:10:59.880383: step 144, loss 0.812459, acc 0.74\n",
      "current_step:  144\n",
      "2017-08-23T03:10:59.969424: step 145, loss 0.82973, acc 0.74\n",
      "current_step:  145\n",
      "2017-08-23T03:11:00.059978: step 146, loss 1.10132, acc 0.62\n",
      "current_step:  146\n",
      "2017-08-23T03:11:00.148681: step 147, loss 0.865683, acc 0.64\n",
      "current_step:  147\n",
      "2017-08-23T03:11:00.238392: step 148, loss 0.922016, acc 0.62\n",
      "current_step:  148\n",
      "2017-08-23T03:11:00.327687: step 149, loss 0.799462, acc 0.68\n",
      "current_step:  149\n",
      "2017-08-23T03:11:00.417173: step 150, loss 0.873481, acc 0.66\n",
      "current_step:  150\n",
      "2017-08-23T03:11:00.507479: step 151, loss 1.10175, acc 0.62\n",
      "current_step:  151\n",
      "2017-08-23T03:11:00.597456: step 152, loss 0.90537, acc 0.68\n",
      "current_step:  152\n",
      "2017-08-23T03:11:00.686585: step 153, loss 0.96823, acc 0.66\n",
      "current_step:  153\n",
      "2017-08-23T03:11:00.775810: step 154, loss 0.913507, acc 0.74\n",
      "current_step:  154\n",
      "2017-08-23T03:11:00.864739: step 155, loss 0.83228, acc 0.72\n",
      "current_step:  155\n",
      "2017-08-23T03:11:00.954516: step 156, loss 0.696231, acc 0.82\n",
      "current_step:  156\n",
      "2017-08-23T03:11:01.044103: step 157, loss 0.95016, acc 0.66\n",
      "current_step:  157\n",
      "2017-08-23T03:11:01.134457: step 158, loss 0.873979, acc 0.68\n",
      "current_step:  158\n",
      "2017-08-23T03:11:01.222974: step 159, loss 1.27645, acc 0.54\n",
      "current_step:  159\n",
      "2017-08-23T03:11:01.312011: step 160, loss 0.779984, acc 0.72\n",
      "current_step:  160\n",
      "2017-08-23T03:11:01.401281: step 161, loss 0.848305, acc 0.66\n",
      "current_step:  161\n",
      "2017-08-23T03:11:01.491749: step 162, loss 0.995134, acc 0.62\n",
      "current_step:  162\n",
      "2017-08-23T03:11:01.581154: step 163, loss 0.76633, acc 0.8\n",
      "current_step:  163\n",
      "2017-08-23T03:11:01.671299: step 164, loss 0.778941, acc 0.68\n",
      "current_step:  164\n",
      "2017-08-23T03:11:01.761030: step 165, loss 0.794941, acc 0.72\n",
      "current_step:  165\n",
      "2017-08-23T03:11:01.851910: step 166, loss 0.839525, acc 0.64\n",
      "current_step:  166\n",
      "2017-08-23T03:11:01.941170: step 167, loss 1.08511, acc 0.6\n",
      "current_step:  167\n",
      "2017-08-23T03:11:02.029449: step 168, loss 0.958397, acc 0.68\n",
      "current_step:  168\n",
      "2017-08-23T03:11:02.118659: step 169, loss 0.885269, acc 0.74\n",
      "current_step:  169\n",
      "2017-08-23T03:11:02.208324: step 170, loss 0.726722, acc 0.72\n",
      "current_step:  170\n",
      "2017-08-23T03:11:02.297156: step 171, loss 0.893443, acc 0.72\n",
      "current_step:  171\n",
      "2017-08-23T03:11:02.386091: step 172, loss 0.839903, acc 0.68\n",
      "current_step:  172\n",
      "2017-08-23T03:11:02.474945: step 173, loss 1.05211, acc 0.58\n",
      "current_step:  173\n",
      "2017-08-23T03:11:02.563227: step 174, loss 0.791242, acc 0.76\n",
      "current_step:  174\n",
      "2017-08-23T03:11:02.651818: step 175, loss 0.724071, acc 0.7\n",
      "current_step:  175\n",
      "2017-08-23T03:11:02.740438: step 176, loss 0.846798, acc 0.66\n",
      "current_step:  176\n",
      "2017-08-23T03:11:02.829204: step 177, loss 0.844245, acc 0.74\n",
      "current_step:  177\n",
      "2017-08-23T03:11:02.918065: step 178, loss 0.968076, acc 0.72\n",
      "current_step:  178\n",
      "2017-08-23T03:11:03.006816: step 179, loss 0.856981, acc 0.72\n",
      "current_step:  179\n",
      "2017-08-23T03:11:03.095446: step 180, loss 0.800713, acc 0.72\n",
      "current_step:  180\n",
      "2017-08-23T03:11:03.184518: step 181, loss 0.962593, acc 0.7\n",
      "current_step:  181\n",
      "2017-08-23T03:11:03.273223: step 182, loss 0.902228, acc 0.72\n",
      "current_step:  182\n",
      "2017-08-23T03:11:03.361652: step 183, loss 0.822944, acc 0.7\n",
      "current_step:  183\n",
      "2017-08-23T03:11:03.450289: step 184, loss 0.775796, acc 0.74\n",
      "current_step:  184\n",
      "2017-08-23T03:11:03.539005: step 185, loss 0.777991, acc 0.72\n",
      "current_step:  185\n",
      "2017-08-23T03:11:03.627464: step 186, loss 0.955544, acc 0.66\n",
      "current_step:  186\n",
      "2017-08-23T03:11:03.716103: step 187, loss 0.895505, acc 0.6\n",
      "current_step:  187\n",
      "2017-08-23T03:11:03.805027: step 188, loss 0.848664, acc 0.74\n",
      "current_step:  188\n",
      "2017-08-23T03:11:03.894038: step 189, loss 0.689059, acc 0.8\n",
      "current_step:  189\n",
      "2017-08-23T03:11:03.982044: step 190, loss 0.650577, acc 0.82\n",
      "current_step:  190\n",
      "2017-08-23T03:11:04.071086: step 191, loss 0.860448, acc 0.64\n",
      "current_step:  191\n",
      "2017-08-23T03:11:04.159736: step 192, loss 0.864479, acc 0.7\n",
      "current_step:  192\n",
      "2017-08-23T03:11:04.248899: step 193, loss 0.739369, acc 0.7\n",
      "current_step:  193\n",
      "2017-08-23T03:11:04.338416: step 194, loss 0.642946, acc 0.78\n",
      "current_step:  194\n",
      "2017-08-23T03:11:04.427615: step 195, loss 0.701373, acc 0.68\n",
      "current_step:  195\n",
      "2017-08-23T03:11:04.517086: step 196, loss 0.743145, acc 0.7\n",
      "current_step:  196\n",
      "2017-08-23T03:11:04.605863: step 197, loss 0.974412, acc 0.6\n",
      "current_step:  197\n",
      "2017-08-23T03:11:04.695126: step 198, loss 0.847813, acc 0.66\n",
      "current_step:  198\n",
      "2017-08-23T03:11:04.785548: step 199, loss 0.967758, acc 0.72\n",
      "current_step:  199\n",
      "2017-08-23T03:11:04.874187: step 200, loss 0.85459, acc 0.66\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:11:05.189016: step 200, loss 0.983599, acc 0.629893\n",
      "\n",
      "2017-08-23T03:11:05.278076: step 201, loss 0.780301, acc 0.7\n",
      "current_step:  201\n",
      "2017-08-23T03:11:05.366457: step 202, loss 0.717711, acc 0.76\n",
      "current_step:  202\n",
      "2017-08-23T03:11:05.454523: step 203, loss 0.832135, acc 0.68\n",
      "current_step:  203\n",
      "2017-08-23T03:11:05.477604: step 204, loss 0.551298, acc 0.777778\n",
      "current_step:  204\n",
      "2017-08-23T03:11:05.567872: step 205, loss 0.803975, acc 0.74\n",
      "current_step:  205\n",
      "2017-08-23T03:11:05.657691: step 206, loss 0.592709, acc 0.86\n",
      "current_step:  206\n",
      "2017-08-23T03:11:05.747531: step 207, loss 0.513376, acc 0.86\n",
      "current_step:  207\n",
      "2017-08-23T03:11:05.836911: step 208, loss 0.63801, acc 0.82\n",
      "current_step:  208\n",
      "2017-08-23T03:11:05.926343: step 209, loss 0.596035, acc 0.8\n",
      "current_step:  209\n",
      "2017-08-23T03:11:06.015212: step 210, loss 0.633552, acc 0.84\n",
      "current_step:  210\n",
      "2017-08-23T03:11:06.103595: step 211, loss 0.575798, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-23T03:11:06.193513: step 212, loss 0.485991, acc 0.88\n",
      "current_step:  212\n",
      "2017-08-23T03:11:06.283153: step 213, loss 0.673389, acc 0.74\n",
      "current_step:  213\n",
      "2017-08-23T03:11:06.370970: step 214, loss 0.864207, acc 0.64\n",
      "current_step:  214\n",
      "2017-08-23T03:11:06.459902: step 215, loss 0.684046, acc 0.78\n",
      "current_step:  215\n",
      "2017-08-23T03:11:06.548117: step 216, loss 0.685926, acc 0.72\n",
      "current_step:  216\n",
      "2017-08-23T03:11:06.636417: step 217, loss 0.651004, acc 0.8\n",
      "current_step:  217\n",
      "2017-08-23T03:11:06.726279: step 218, loss 0.648945, acc 0.84\n",
      "current_step:  218\n",
      "2017-08-23T03:11:06.815804: step 219, loss 0.661183, acc 0.78\n",
      "current_step:  219\n",
      "2017-08-23T03:11:06.906263: step 220, loss 0.582089, acc 0.76\n",
      "current_step:  220\n",
      "2017-08-23T03:11:06.995597: step 221, loss 0.508451, acc 0.84\n",
      "current_step:  221\n",
      "2017-08-23T03:11:07.084212: step 222, loss 0.885996, acc 0.66\n",
      "current_step:  222\n",
      "2017-08-23T03:11:07.172723: step 223, loss 0.843006, acc 0.7\n",
      "current_step:  223\n",
      "2017-08-23T03:11:07.261304: step 224, loss 0.624452, acc 0.84\n",
      "current_step:  224\n",
      "2017-08-23T03:11:07.350240: step 225, loss 0.61757, acc 0.78\n",
      "current_step:  225\n",
      "2017-08-23T03:11:07.439127: step 226, loss 0.457682, acc 0.88\n",
      "current_step:  226\n",
      "2017-08-23T03:11:07.528959: step 227, loss 0.734317, acc 0.7\n",
      "current_step:  227\n",
      "2017-08-23T03:11:07.617862: step 228, loss 0.834639, acc 0.6\n",
      "current_step:  228\n",
      "2017-08-23T03:11:07.706684: step 229, loss 0.84719, acc 0.7\n",
      "current_step:  229\n",
      "2017-08-23T03:11:07.795707: step 230, loss 0.659519, acc 0.76\n",
      "current_step:  230\n",
      "2017-08-23T03:11:07.884193: step 231, loss 0.548598, acc 0.84\n",
      "current_step:  231\n",
      "2017-08-23T03:11:07.972979: step 232, loss 0.640109, acc 0.78\n",
      "current_step:  232\n",
      "2017-08-23T03:11:08.062363: step 233, loss 0.616368, acc 0.8\n",
      "current_step:  233\n",
      "2017-08-23T03:11:08.151397: step 234, loss 0.693497, acc 0.74\n",
      "current_step:  234\n",
      "2017-08-23T03:11:08.240895: step 235, loss 0.668017, acc 0.8\n",
      "current_step:  235\n",
      "2017-08-23T03:11:08.329525: step 236, loss 0.655114, acc 0.74\n",
      "current_step:  236\n",
      "2017-08-23T03:11:08.418529: step 237, loss 0.685597, acc 0.74\n",
      "current_step:  237\n",
      "2017-08-23T03:11:08.507438: step 238, loss 0.610008, acc 0.8\n",
      "current_step:  238\n",
      "2017-08-23T03:11:08.597008: step 239, loss 0.96733, acc 0.72\n",
      "current_step:  239\n",
      "2017-08-23T03:11:08.685817: step 240, loss 0.668256, acc 0.78\n",
      "current_step:  240\n",
      "2017-08-23T03:11:08.775509: step 241, loss 0.585873, acc 0.8\n",
      "current_step:  241\n",
      "2017-08-23T03:11:08.864911: step 242, loss 0.689045, acc 0.76\n",
      "current_step:  242\n",
      "2017-08-23T03:11:08.954833: step 243, loss 0.830271, acc 0.72\n",
      "current_step:  243\n",
      "2017-08-23T03:11:09.043738: step 244, loss 0.449748, acc 0.84\n",
      "current_step:  244\n",
      "2017-08-23T03:11:09.132419: step 245, loss 0.570245, acc 0.74\n",
      "current_step:  245\n",
      "2017-08-23T03:11:09.222374: step 246, loss 0.634144, acc 0.82\n",
      "current_step:  246\n",
      "2017-08-23T03:11:09.311203: step 247, loss 0.781951, acc 0.72\n",
      "current_step:  247\n",
      "2017-08-23T03:11:09.400359: step 248, loss 0.495035, acc 0.86\n",
      "current_step:  248\n",
      "2017-08-23T03:11:09.490094: step 249, loss 0.811323, acc 0.76\n",
      "current_step:  249\n",
      "2017-08-23T03:11:09.578891: step 250, loss 0.672087, acc 0.76\n",
      "current_step:  250\n",
      "2017-08-23T03:11:09.668371: step 251, loss 0.810303, acc 0.72\n",
      "current_step:  251\n",
      "2017-08-23T03:11:09.757362: step 252, loss 0.623168, acc 0.82\n",
      "current_step:  252\n",
      "2017-08-23T03:11:09.845629: step 253, loss 0.750151, acc 0.7\n",
      "current_step:  253\n",
      "2017-08-23T03:11:09.935931: step 254, loss 0.625392, acc 0.78\n",
      "current_step:  254\n",
      "2017-08-23T03:11:10.026442: step 255, loss 0.633116, acc 0.8\n",
      "current_step:  255\n",
      "2017-08-23T03:11:10.115899: step 256, loss 0.537428, acc 0.86\n",
      "current_step:  256\n",
      "2017-08-23T03:11:10.206075: step 257, loss 0.840323, acc 0.68\n",
      "current_step:  257\n",
      "2017-08-23T03:11:10.295493: step 258, loss 0.566811, acc 0.78\n",
      "current_step:  258\n",
      "2017-08-23T03:11:10.384677: step 259, loss 0.717534, acc 0.7\n",
      "current_step:  259\n",
      "2017-08-23T03:11:10.474261: step 260, loss 0.707997, acc 0.74\n",
      "current_step:  260\n",
      "2017-08-23T03:11:10.564082: step 261, loss 0.613769, acc 0.84\n",
      "current_step:  261\n",
      "2017-08-23T03:11:10.654865: step 262, loss 0.546074, acc 0.86\n",
      "current_step:  262\n",
      "2017-08-23T03:11:10.744458: step 263, loss 0.648235, acc 0.74\n",
      "current_step:  263\n",
      "2017-08-23T03:11:10.834269: step 264, loss 0.719383, acc 0.76\n",
      "current_step:  264\n",
      "2017-08-23T03:11:10.923636: step 265, loss 0.628694, acc 0.8\n",
      "current_step:  265\n",
      "2017-08-23T03:11:11.012336: step 266, loss 0.63942, acc 0.82\n",
      "current_step:  266\n",
      "2017-08-23T03:11:11.100739: step 267, loss 0.73386, acc 0.76\n",
      "current_step:  267\n",
      "2017-08-23T03:11:11.189621: step 268, loss 0.689778, acc 0.84\n",
      "current_step:  268\n",
      "2017-08-23T03:11:11.279857: step 269, loss 0.800391, acc 0.68\n",
      "current_step:  269\n",
      "2017-08-23T03:11:11.369704: step 270, loss 0.689311, acc 0.8\n",
      "current_step:  270\n",
      "2017-08-23T03:11:11.459244: step 271, loss 0.416577, acc 0.94\n",
      "current_step:  271\n",
      "2017-08-23T03:11:11.548757: step 272, loss 0.776101, acc 0.72\n",
      "current_step:  272\n",
      "2017-08-23T03:11:11.639868: step 273, loss 0.684797, acc 0.82\n",
      "current_step:  273\n",
      "2017-08-23T03:11:11.730049: step 274, loss 0.657961, acc 0.8\n",
      "current_step:  274\n",
      "2017-08-23T03:11:11.818991: step 275, loss 0.702045, acc 0.74\n",
      "current_step:  275\n",
      "2017-08-23T03:11:11.910032: step 276, loss 0.728692, acc 0.72\n",
      "current_step:  276\n",
      "2017-08-23T03:11:11.998864: step 277, loss 0.695644, acc 0.72\n",
      "current_step:  277\n",
      "2017-08-23T03:11:12.088111: step 278, loss 0.687167, acc 0.78\n",
      "current_step:  278\n",
      "2017-08-23T03:11:12.176458: step 279, loss 0.503052, acc 0.9\n",
      "current_step:  279\n",
      "2017-08-23T03:11:12.265497: step 280, loss 0.488559, acc 0.88\n",
      "current_step:  280\n",
      "2017-08-23T03:11:12.354771: step 281, loss 0.628308, acc 0.84\n",
      "current_step:  281\n",
      "2017-08-23T03:11:12.443519: step 282, loss 0.585619, acc 0.84\n",
      "current_step:  282\n",
      "2017-08-23T03:11:12.533065: step 283, loss 0.751988, acc 0.8\n",
      "current_step:  283\n",
      "2017-08-23T03:11:12.622737: step 284, loss 0.715428, acc 0.74\n",
      "current_step:  284\n",
      "2017-08-23T03:11:12.712350: step 285, loss 0.615583, acc 0.82\n",
      "current_step:  285\n",
      "2017-08-23T03:11:12.802267: step 286, loss 0.6905, acc 0.8\n",
      "current_step:  286\n",
      "2017-08-23T03:11:12.892041: step 287, loss 0.659166, acc 0.86\n",
      "current_step:  287\n",
      "2017-08-23T03:11:12.981783: step 288, loss 0.701898, acc 0.72\n",
      "current_step:  288\n",
      "2017-08-23T03:11:13.070491: step 289, loss 0.617979, acc 0.82\n",
      "current_step:  289\n",
      "2017-08-23T03:11:13.159657: step 290, loss 0.610026, acc 0.82\n",
      "current_step:  290\n",
      "2017-08-23T03:11:13.248965: step 291, loss 0.572975, acc 0.82\n",
      "current_step:  291\n",
      "2017-08-23T03:11:13.337973: step 292, loss 0.846238, acc 0.66\n",
      "current_step:  292\n",
      "2017-08-23T03:11:13.428339: step 293, loss 0.441217, acc 0.88\n",
      "current_step:  293\n",
      "2017-08-23T03:11:13.518455: step 294, loss 0.638955, acc 0.84\n",
      "current_step:  294\n",
      "2017-08-23T03:11:13.607608: step 295, loss 0.529422, acc 0.82\n",
      "current_step:  295\n",
      "2017-08-23T03:11:13.697160: step 296, loss 0.703986, acc 0.76\n",
      "current_step:  296\n",
      "2017-08-23T03:11:13.786015: step 297, loss 0.535709, acc 0.8\n",
      "current_step:  297\n",
      "2017-08-23T03:11:13.875900: step 298, loss 0.480869, acc 0.82\n",
      "current_step:  298\n",
      "2017-08-23T03:11:13.966863: step 299, loss 0.697355, acc 0.72\n",
      "current_step:  299\n",
      "2017-08-23T03:11:14.055904: step 300, loss 0.656424, acc 0.76\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:11:14.368877: step 300, loss 0.9378, acc 0.642349\n",
      "\n",
      "2017-08-23T03:11:14.458878: step 301, loss 0.749665, acc 0.74\n",
      "current_step:  301\n",
      "2017-08-23T03:11:14.549093: step 302, loss 0.799637, acc 0.76\n",
      "current_step:  302\n",
      "2017-08-23T03:11:14.639247: step 303, loss 0.58959, acc 0.82\n",
      "current_step:  303\n",
      "2017-08-23T03:11:14.729521: step 304, loss 0.622971, acc 0.78\n",
      "current_step:  304\n",
      "2017-08-23T03:11:14.819392: step 305, loss 0.622718, acc 0.82\n",
      "current_step:  305\n",
      "2017-08-23T03:11:14.840732: step 306, loss 0.522318, acc 0.888889\n",
      "current_step:  306\n",
      "2017-08-23T03:11:14.930893: step 307, loss 0.488556, acc 0.92\n",
      "current_step:  307\n",
      "2017-08-23T03:11:15.019569: step 308, loss 0.402026, acc 0.86\n",
      "current_step:  308\n",
      "2017-08-23T03:11:15.110072: step 309, loss 0.497744, acc 0.86\n",
      "current_step:  309\n",
      "2017-08-23T03:11:15.199888: step 310, loss 0.436498, acc 0.88\n",
      "current_step:  310\n",
      "2017-08-23T03:11:15.289747: step 311, loss 0.583795, acc 0.78\n",
      "current_step:  311\n",
      "2017-08-23T03:11:15.379327: step 312, loss 0.413408, acc 0.88\n",
      "current_step:  312\n",
      "2017-08-23T03:11:15.468752: step 313, loss 0.527976, acc 0.78\n",
      "current_step:  313\n",
      "2017-08-23T03:11:15.557727: step 314, loss 0.467118, acc 0.86\n",
      "current_step:  314\n",
      "2017-08-23T03:11:15.647512: step 315, loss 0.499552, acc 0.86\n",
      "current_step:  315\n",
      "2017-08-23T03:11:15.736719: step 316, loss 0.517622, acc 0.82\n",
      "current_step:  316\n",
      "2017-08-23T03:11:15.825522: step 317, loss 0.396106, acc 0.92\n",
      "current_step:  317\n",
      "2017-08-23T03:11:15.915233: step 318, loss 0.626812, acc 0.74\n",
      "current_step:  318\n",
      "2017-08-23T03:11:16.003981: step 319, loss 0.561757, acc 0.86\n",
      "current_step:  319\n",
      "2017-08-23T03:11:16.094026: step 320, loss 0.553822, acc 0.82\n",
      "current_step:  320\n",
      "2017-08-23T03:11:16.183847: step 321, loss 0.496736, acc 0.82\n",
      "current_step:  321\n",
      "2017-08-23T03:11:16.273346: step 322, loss 0.46382, acc 0.9\n",
      "current_step:  322\n",
      "2017-08-23T03:11:16.363607: step 323, loss 0.507902, acc 0.88\n",
      "current_step:  323\n",
      "2017-08-23T03:11:16.452607: step 324, loss 0.418234, acc 0.9\n",
      "current_step:  324\n",
      "2017-08-23T03:11:16.541869: step 325, loss 0.429694, acc 0.92\n",
      "current_step:  325\n",
      "2017-08-23T03:11:16.630521: step 326, loss 0.575407, acc 0.86\n",
      "current_step:  326\n",
      "2017-08-23T03:11:16.718915: step 327, loss 0.417422, acc 0.92\n",
      "current_step:  327\n",
      "2017-08-23T03:11:16.807304: step 328, loss 0.412846, acc 0.9\n",
      "current_step:  328\n",
      "2017-08-23T03:11:16.896404: step 329, loss 0.445, acc 0.92\n",
      "current_step:  329\n",
      "2017-08-23T03:11:16.986432: step 330, loss 0.658194, acc 0.74\n",
      "current_step:  330\n",
      "2017-08-23T03:11:17.076225: step 331, loss 0.421308, acc 0.86\n",
      "current_step:  331\n",
      "2017-08-23T03:11:17.164452: step 332, loss 0.420457, acc 0.9\n",
      "current_step:  332\n",
      "2017-08-23T03:11:17.254185: step 333, loss 0.536609, acc 0.84\n",
      "current_step:  333\n",
      "2017-08-23T03:11:17.343450: step 334, loss 0.550312, acc 0.86\n",
      "current_step:  334\n",
      "2017-08-23T03:11:17.433408: step 335, loss 0.461447, acc 0.86\n",
      "current_step:  335\n",
      "2017-08-23T03:11:17.522687: step 336, loss 0.453046, acc 0.88\n",
      "current_step:  336\n",
      "2017-08-23T03:11:17.611660: step 337, loss 0.530506, acc 0.86\n",
      "current_step:  337\n",
      "2017-08-23T03:11:17.700611: step 338, loss 0.478463, acc 0.9\n",
      "current_step:  338\n",
      "2017-08-23T03:11:17.790472: step 339, loss 0.325872, acc 0.94\n",
      "current_step:  339\n",
      "2017-08-23T03:11:17.880093: step 340, loss 0.649642, acc 0.8\n",
      "current_step:  340\n",
      "2017-08-23T03:11:17.969905: step 341, loss 0.576839, acc 0.82\n",
      "current_step:  341\n",
      "2017-08-23T03:11:18.058921: step 342, loss 0.43091, acc 0.92\n",
      "current_step:  342\n",
      "2017-08-23T03:11:18.149180: step 343, loss 0.70192, acc 0.82\n",
      "current_step:  343\n",
      "2017-08-23T03:11:18.238233: step 344, loss 0.510457, acc 0.88\n",
      "current_step:  344\n",
      "2017-08-23T03:11:18.327528: step 345, loss 0.358549, acc 0.92\n",
      "current_step:  345\n",
      "2017-08-23T03:11:18.415691: step 346, loss 0.487515, acc 0.92\n",
      "current_step:  346\n",
      "2017-08-23T03:11:18.505281: step 347, loss 0.540799, acc 0.82\n",
      "current_step:  347\n",
      "2017-08-23T03:11:18.594132: step 348, loss 0.626399, acc 0.82\n",
      "current_step:  348\n",
      "2017-08-23T03:11:18.683810: step 349, loss 0.398074, acc 0.94\n",
      "current_step:  349\n",
      "2017-08-23T03:11:18.772948: step 350, loss 0.543051, acc 0.82\n",
      "current_step:  350\n",
      "2017-08-23T03:11:18.863580: step 351, loss 0.375224, acc 0.92\n",
      "current_step:  351\n",
      "2017-08-23T03:11:18.952260: step 352, loss 0.533797, acc 0.78\n",
      "current_step:  352\n",
      "2017-08-23T03:11:19.041630: step 353, loss 0.429388, acc 0.86\n",
      "current_step:  353\n",
      "2017-08-23T03:11:19.130105: step 354, loss 0.526411, acc 0.86\n",
      "current_step:  354\n",
      "2017-08-23T03:11:19.219119: step 355, loss 0.610025, acc 0.82\n",
      "current_step:  355\n",
      "2017-08-23T03:11:19.307282: step 356, loss 0.52807, acc 0.84\n",
      "current_step:  356\n",
      "2017-08-23T03:11:19.396140: step 357, loss 0.752979, acc 0.72\n",
      "current_step:  357\n",
      "2017-08-23T03:11:19.485876: step 358, loss 0.403135, acc 0.94\n",
      "current_step:  358\n",
      "2017-08-23T03:11:19.574364: step 359, loss 0.444143, acc 0.92\n",
      "current_step:  359\n",
      "2017-08-23T03:11:19.663134: step 360, loss 0.407442, acc 0.94\n",
      "current_step:  360\n",
      "2017-08-23T03:11:19.752048: step 361, loss 0.452695, acc 0.86\n",
      "current_step:  361\n",
      "2017-08-23T03:11:19.840703: step 362, loss 0.514683, acc 0.86\n",
      "current_step:  362\n",
      "2017-08-23T03:11:19.929227: step 363, loss 0.304893, acc 0.96\n",
      "current_step:  363\n",
      "2017-08-23T03:11:20.018175: step 364, loss 0.497393, acc 0.84\n",
      "current_step:  364\n",
      "2017-08-23T03:11:20.107324: step 365, loss 0.659549, acc 0.84\n",
      "current_step:  365\n",
      "2017-08-23T03:11:20.195686: step 366, loss 0.456406, acc 0.88\n",
      "current_step:  366\n",
      "2017-08-23T03:11:20.284784: step 367, loss 0.459276, acc 0.9\n",
      "current_step:  367\n",
      "2017-08-23T03:11:20.372968: step 368, loss 0.491293, acc 0.84\n",
      "current_step:  368\n",
      "2017-08-23T03:11:20.462461: step 369, loss 0.507976, acc 0.84\n",
      "current_step:  369\n",
      "2017-08-23T03:11:20.552309: step 370, loss 0.6196, acc 0.76\n",
      "current_step:  370\n",
      "2017-08-23T03:11:20.641335: step 371, loss 0.673571, acc 0.78\n",
      "current_step:  371\n",
      "2017-08-23T03:11:20.731546: step 372, loss 0.539991, acc 0.78\n",
      "current_step:  372\n",
      "2017-08-23T03:11:20.821770: step 373, loss 0.612078, acc 0.8\n",
      "current_step:  373\n",
      "2017-08-23T03:11:20.911317: step 374, loss 0.374544, acc 0.86\n",
      "current_step:  374\n",
      "2017-08-23T03:11:21.000021: step 375, loss 0.557988, acc 0.8\n",
      "current_step:  375\n",
      "2017-08-23T03:11:21.089164: step 376, loss 0.481828, acc 0.84\n",
      "current_step:  376\n",
      "2017-08-23T03:11:21.179081: step 377, loss 0.557377, acc 0.8\n",
      "current_step:  377\n",
      "2017-08-23T03:11:21.268197: step 378, loss 0.403907, acc 0.9\n",
      "current_step:  378\n",
      "2017-08-23T03:11:21.357294: step 379, loss 0.384283, acc 0.88\n",
      "current_step:  379\n",
      "2017-08-23T03:11:21.447756: step 380, loss 0.445905, acc 0.84\n",
      "current_step:  380\n",
      "2017-08-23T03:11:21.537999: step 381, loss 0.506078, acc 0.92\n",
      "current_step:  381\n",
      "2017-08-23T03:11:21.627630: step 382, loss 0.562219, acc 0.84\n",
      "current_step:  382\n",
      "2017-08-23T03:11:21.717664: step 383, loss 0.5775, acc 0.86\n",
      "current_step:  383\n",
      "2017-08-23T03:11:21.807043: step 384, loss 0.487636, acc 0.82\n",
      "current_step:  384\n",
      "2017-08-23T03:11:21.896729: step 385, loss 0.622387, acc 0.8\n",
      "current_step:  385\n",
      "2017-08-23T03:11:21.986112: step 386, loss 0.690883, acc 0.74\n",
      "current_step:  386\n",
      "2017-08-23T03:11:22.077665: step 387, loss 0.461588, acc 0.9\n",
      "current_step:  387\n",
      "2017-08-23T03:11:22.167206: step 388, loss 0.544864, acc 0.82\n",
      "current_step:  388\n",
      "2017-08-23T03:11:22.256461: step 389, loss 0.458769, acc 0.84\n",
      "current_step:  389\n",
      "2017-08-23T03:11:22.347584: step 390, loss 0.517439, acc 0.8\n",
      "current_step:  390\n",
      "2017-08-23T03:11:22.437948: step 391, loss 0.398626, acc 0.86\n",
      "current_step:  391\n",
      "2017-08-23T03:11:22.526980: step 392, loss 0.394491, acc 0.92\n",
      "current_step:  392\n",
      "2017-08-23T03:11:22.618966: step 393, loss 0.437124, acc 0.82\n",
      "current_step:  393\n",
      "2017-08-23T03:11:22.708173: step 394, loss 0.446936, acc 0.88\n",
      "current_step:  394\n",
      "2017-08-23T03:11:22.797989: step 395, loss 0.536511, acc 0.84\n",
      "current_step:  395\n",
      "2017-08-23T03:11:22.887281: step 396, loss 0.345732, acc 0.96\n",
      "current_step:  396\n",
      "2017-08-23T03:11:22.976507: step 397, loss 0.538613, acc 0.86\n",
      "current_step:  397\n",
      "2017-08-23T03:11:23.066538: step 398, loss 0.420433, acc 0.86\n",
      "current_step:  398\n",
      "2017-08-23T03:11:23.156279: step 399, loss 0.528765, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-23T03:11:23.245978: step 400, loss 0.571856, acc 0.8\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:11:23.559233: step 400, loss 0.923094, acc 0.658363\n",
      "\n",
      "2017-08-23T03:11:23.649342: step 401, loss 0.487448, acc 0.78\n",
      "current_step:  401\n",
      "2017-08-23T03:11:23.739210: step 402, loss 0.485534, acc 0.8\n",
      "current_step:  402\n",
      "2017-08-23T03:11:23.827813: step 403, loss 0.55318, acc 0.78\n",
      "current_step:  403\n",
      "2017-08-23T03:11:23.917829: step 404, loss 0.518052, acc 0.84\n",
      "current_step:  404\n",
      "2017-08-23T03:11:24.007794: step 405, loss 0.585155, acc 0.86\n",
      "current_step:  405\n",
      "2017-08-23T03:11:24.096682: step 406, loss 0.578024, acc 0.76\n",
      "current_step:  406\n",
      "2017-08-23T03:11:24.186093: step 407, loss 0.54188, acc 0.84\n",
      "current_step:  407\n",
      "2017-08-23T03:11:24.208071: step 408, loss 0.618066, acc 0.777778\n",
      "current_step:  408\n",
      "2017-08-23T03:11:24.299012: step 409, loss 0.328431, acc 0.92\n",
      "current_step:  409\n",
      "2017-08-23T03:11:24.391002: step 410, loss 0.397924, acc 0.9\n",
      "current_step:  410\n",
      "2017-08-23T03:11:24.484901: step 411, loss 0.359475, acc 0.92\n",
      "current_step:  411\n",
      "2017-08-23T03:11:24.575907: step 412, loss 0.41979, acc 0.88\n",
      "current_step:  412\n",
      "2017-08-23T03:11:24.666552: step 413, loss 0.401284, acc 0.92\n",
      "current_step:  413\n",
      "2017-08-23T03:11:24.758450: step 414, loss 0.346821, acc 0.96\n",
      "current_step:  414\n",
      "2017-08-23T03:11:24.849175: step 415, loss 0.424904, acc 0.86\n",
      "current_step:  415\n",
      "2017-08-23T03:11:24.939302: step 416, loss 0.437149, acc 0.94\n",
      "current_step:  416\n",
      "2017-08-23T03:11:25.029828: step 417, loss 0.375164, acc 0.92\n",
      "current_step:  417\n",
      "2017-08-23T03:11:25.120034: step 418, loss 0.396258, acc 0.92\n",
      "current_step:  418\n",
      "2017-08-23T03:11:25.211448: step 419, loss 0.420231, acc 0.9\n",
      "current_step:  419\n",
      "2017-08-23T03:11:25.301558: step 420, loss 0.261098, acc 0.96\n",
      "current_step:  420\n",
      "2017-08-23T03:11:25.390907: step 421, loss 0.345529, acc 0.94\n",
      "current_step:  421\n",
      "2017-08-23T03:11:25.481993: step 422, loss 0.516906, acc 0.82\n",
      "current_step:  422\n",
      "2017-08-23T03:11:25.572274: step 423, loss 0.370961, acc 0.94\n",
      "current_step:  423\n",
      "2017-08-23T03:11:25.662159: step 424, loss 0.365251, acc 0.86\n",
      "current_step:  424\n",
      "2017-08-23T03:11:25.752339: step 425, loss 0.394797, acc 0.86\n",
      "current_step:  425\n",
      "2017-08-23T03:11:25.842523: step 426, loss 0.405489, acc 0.92\n",
      "current_step:  426\n",
      "2017-08-23T03:11:25.932712: step 427, loss 0.419559, acc 0.88\n",
      "current_step:  427\n",
      "2017-08-23T03:11:26.021430: step 428, loss 0.473015, acc 0.86\n",
      "current_step:  428\n",
      "2017-08-23T03:11:26.112366: step 429, loss 0.492958, acc 0.86\n",
      "current_step:  429\n",
      "2017-08-23T03:11:26.203753: step 430, loss 0.493758, acc 0.9\n",
      "current_step:  430\n",
      "2017-08-23T03:11:26.294202: step 431, loss 0.357455, acc 0.96\n",
      "current_step:  431\n",
      "2017-08-23T03:11:26.384977: step 432, loss 0.272591, acc 0.98\n",
      "current_step:  432\n",
      "2017-08-23T03:11:26.476067: step 433, loss 0.331469, acc 0.96\n",
      "current_step:  433\n",
      "2017-08-23T03:11:26.568338: step 434, loss 0.366231, acc 0.94\n",
      "current_step:  434\n",
      "2017-08-23T03:11:26.661027: step 435, loss 0.300024, acc 0.92\n",
      "current_step:  435\n",
      "2017-08-23T03:11:26.751689: step 436, loss 0.36778, acc 0.94\n",
      "current_step:  436\n",
      "2017-08-23T03:11:26.843700: step 437, loss 0.30743, acc 0.92\n",
      "current_step:  437\n",
      "2017-08-23T03:11:26.934608: step 438, loss 0.413035, acc 0.86\n",
      "current_step:  438\n",
      "2017-08-23T03:11:27.024688: step 439, loss 0.402699, acc 0.9\n",
      "current_step:  439\n",
      "2017-08-23T03:11:27.117198: step 440, loss 0.352811, acc 0.94\n",
      "current_step:  440\n",
      "2017-08-23T03:11:27.207805: step 441, loss 0.399891, acc 0.84\n",
      "current_step:  441\n",
      "2017-08-23T03:11:27.298286: step 442, loss 0.360219, acc 0.86\n",
      "current_step:  442\n",
      "2017-08-23T03:11:27.393083: step 443, loss 0.245677, acc 0.96\n",
      "current_step:  443\n",
      "2017-08-23T03:11:27.483957: step 444, loss 0.409487, acc 0.92\n",
      "current_step:  444\n",
      "2017-08-23T03:11:27.574992: step 445, loss 0.421017, acc 0.88\n",
      "current_step:  445\n",
      "2017-08-23T03:11:27.665412: step 446, loss 0.302156, acc 0.94\n",
      "current_step:  446\n",
      "2017-08-23T03:11:27.756185: step 447, loss 0.316786, acc 0.94\n",
      "current_step:  447\n",
      "2017-08-23T03:11:27.845910: step 448, loss 0.288801, acc 0.98\n",
      "current_step:  448\n",
      "2017-08-23T03:11:27.936763: step 449, loss 0.404145, acc 0.9\n",
      "current_step:  449\n",
      "2017-08-23T03:11:28.027244: step 450, loss 0.390829, acc 0.92\n",
      "current_step:  450\n",
      "2017-08-23T03:11:28.117722: step 451, loss 0.389703, acc 0.88\n",
      "current_step:  451\n",
      "2017-08-23T03:11:28.206772: step 452, loss 0.401029, acc 0.9\n",
      "current_step:  452\n",
      "2017-08-23T03:11:28.296033: step 453, loss 0.382295, acc 0.9\n",
      "current_step:  453\n",
      "2017-08-23T03:11:28.385676: step 454, loss 0.437236, acc 0.94\n",
      "current_step:  454\n",
      "2017-08-23T03:11:28.476359: step 455, loss 0.358353, acc 0.94\n",
      "current_step:  455\n",
      "2017-08-23T03:11:28.567307: step 456, loss 0.293398, acc 0.96\n",
      "current_step:  456\n",
      "2017-08-23T03:11:28.658689: step 457, loss 0.495478, acc 0.8\n",
      "current_step:  457\n",
      "2017-08-23T03:11:28.748941: step 458, loss 0.421406, acc 0.9\n",
      "current_step:  458\n",
      "2017-08-23T03:11:28.840018: step 459, loss 0.381197, acc 0.92\n",
      "current_step:  459\n",
      "2017-08-23T03:11:28.930982: step 460, loss 0.350901, acc 0.94\n",
      "current_step:  460\n",
      "2017-08-23T03:11:29.021807: step 461, loss 0.484846, acc 0.82\n",
      "current_step:  461\n",
      "2017-08-23T03:11:29.111181: step 462, loss 0.460803, acc 0.86\n",
      "current_step:  462\n",
      "2017-08-23T03:11:29.201449: step 463, loss 0.486169, acc 0.9\n",
      "current_step:  463\n",
      "2017-08-23T03:11:29.290711: step 464, loss 0.275039, acc 0.92\n",
      "current_step:  464\n",
      "2017-08-23T03:11:29.380271: step 465, loss 0.424496, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-23T03:11:29.470852: step 466, loss 0.412677, acc 0.88\n",
      "current_step:  466\n",
      "2017-08-23T03:11:29.560593: step 467, loss 0.377243, acc 0.9\n",
      "current_step:  467\n",
      "2017-08-23T03:11:29.653113: step 468, loss 0.373693, acc 0.88\n",
      "current_step:  468\n",
      "2017-08-23T03:11:29.743634: step 469, loss 0.474356, acc 0.88\n",
      "current_step:  469\n",
      "2017-08-23T03:11:29.833160: step 470, loss 0.354425, acc 0.92\n",
      "current_step:  470\n",
      "2017-08-23T03:11:29.923266: step 471, loss 0.397021, acc 0.94\n",
      "current_step:  471\n",
      "2017-08-23T03:11:30.014140: step 472, loss 0.363969, acc 0.94\n",
      "current_step:  472\n",
      "2017-08-23T03:11:30.104729: step 473, loss 0.363668, acc 0.94\n",
      "current_step:  473\n",
      "2017-08-23T03:11:30.195346: step 474, loss 0.317981, acc 0.98\n",
      "current_step:  474\n",
      "2017-08-23T03:11:30.286547: step 475, loss 0.368463, acc 0.9\n",
      "current_step:  475\n",
      "2017-08-23T03:11:30.377161: step 476, loss 0.478312, acc 0.84\n",
      "current_step:  476\n",
      "2017-08-23T03:11:30.467971: step 477, loss 0.443156, acc 0.86\n",
      "current_step:  477\n",
      "2017-08-23T03:11:30.557800: step 478, loss 0.40902, acc 0.86\n",
      "current_step:  478\n",
      "2017-08-23T03:11:30.650813: step 479, loss 0.253194, acc 0.96\n",
      "current_step:  479\n",
      "2017-08-23T03:11:30.742900: step 480, loss 0.34423, acc 0.94\n",
      "current_step:  480\n",
      "2017-08-23T03:11:30.835213: step 481, loss 0.230002, acc 0.96\n",
      "current_step:  481\n",
      "2017-08-23T03:11:30.926411: step 482, loss 0.439838, acc 0.88\n",
      "current_step:  482\n",
      "2017-08-23T03:11:31.016420: step 483, loss 0.466437, acc 0.84\n",
      "current_step:  483\n",
      "2017-08-23T03:11:31.106728: step 484, loss 0.391509, acc 0.92\n",
      "current_step:  484\n",
      "2017-08-23T03:11:31.197558: step 485, loss 0.382258, acc 0.92\n",
      "current_step:  485\n",
      "2017-08-23T03:11:31.288816: step 486, loss 0.397297, acc 0.88\n",
      "current_step:  486\n",
      "2017-08-23T03:11:31.378611: step 487, loss 0.367296, acc 0.94\n",
      "current_step:  487\n",
      "2017-08-23T03:11:31.468169: step 488, loss 0.215271, acc 0.98\n",
      "current_step:  488\n",
      "2017-08-23T03:11:31.557883: step 489, loss 0.249365, acc 0.94\n",
      "current_step:  489\n",
      "2017-08-23T03:11:31.646523: step 490, loss 0.468456, acc 0.88\n",
      "current_step:  490\n",
      "2017-08-23T03:11:31.737009: step 491, loss 0.345714, acc 0.92\n",
      "current_step:  491\n",
      "2017-08-23T03:11:31.827488: step 492, loss 0.34358, acc 0.9\n",
      "current_step:  492\n",
      "2017-08-23T03:11:31.921612: step 493, loss 0.39053, acc 0.86\n",
      "current_step:  493\n",
      "2017-08-23T03:11:32.011657: step 494, loss 0.378502, acc 0.88\n",
      "current_step:  494\n",
      "2017-08-23T03:11:32.103578: step 495, loss 0.379191, acc 0.88\n",
      "current_step:  495\n",
      "2017-08-23T03:11:32.193752: step 496, loss 0.424009, acc 0.86\n",
      "current_step:  496\n",
      "2017-08-23T03:11:32.284456: step 497, loss 0.465592, acc 0.92\n",
      "current_step:  497\n",
      "2017-08-23T03:11:32.374246: step 498, loss 0.385921, acc 0.9\n",
      "current_step:  498\n",
      "2017-08-23T03:11:32.463837: step 499, loss 0.336704, acc 0.96\n",
      "current_step:  499\n",
      "2017-08-23T03:11:32.552894: step 500, loss 0.365164, acc 0.92\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:11:32.874235: step 500, loss 0.933153, acc 0.658363\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0310_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:11:33.022683: step 501, loss 0.293706, acc 0.98\n",
      "current_step:  501\n",
      "2017-08-23T03:11:33.111649: step 502, loss 0.296935, acc 0.96\n",
      "current_step:  502\n",
      "2017-08-23T03:11:33.208911: step 503, loss 0.345819, acc 0.92\n",
      "current_step:  503\n",
      "2017-08-23T03:11:33.298728: step 504, loss 0.408962, acc 0.92\n",
      "current_step:  504\n",
      "2017-08-23T03:11:33.388542: step 505, loss 0.324964, acc 0.94\n",
      "current_step:  505\n",
      "2017-08-23T03:11:33.478159: step 506, loss 0.339684, acc 0.9\n",
      "current_step:  506\n",
      "2017-08-23T03:11:33.566930: step 507, loss 0.350911, acc 0.94\n",
      "current_step:  507\n",
      "2017-08-23T03:11:33.656469: step 508, loss 0.490399, acc 0.82\n",
      "current_step:  508\n",
      "2017-08-23T03:11:33.745952: step 509, loss 0.387709, acc 0.88\n",
      "current_step:  509\n",
      "2017-08-23T03:11:33.768540: step 510, loss 0.344529, acc 1\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.524337\n",
      "Precision Score = 0.524579\n",
      "Recall Score = 0.587189\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.573079\n",
      "Precision Score = 0.563930\n",
      "Recall Score = 0.620996\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.598575\n",
      "Precision Score = 0.597327\n",
      "Recall Score = 0.633452\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.615160\n",
      "Precision Score = 0.612114\n",
      "Recall Score = 0.647687\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.626223\n",
      "Precision Score = 0.626623\n",
      "Recall Score = 0.647687\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v2 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 2, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  149\n",
      "Vocabulary Size: 13762\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170823_0311_UTC\n",
      "\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170823_0311_UTC\n",
      "2017-08-23T03:11:51.906305: step 1, loss 3.01969, acc 0.34\n",
      "current_step:  1\n",
      "2017-08-23T03:11:51.955529: step 2, loss 2.41527, acc 0.24\n",
      "current_step:  2\n",
      "2017-08-23T03:11:52.005605: step 3, loss 2.38905, acc 0.18\n",
      "current_step:  3\n",
      "2017-08-23T03:11:52.056179: step 4, loss 1.6626, acc 0.36\n",
      "current_step:  4\n",
      "2017-08-23T03:11:52.102722: step 5, loss 1.37136, acc 0.46\n",
      "current_step:  5\n",
      "2017-08-23T03:11:52.146711: step 6, loss 1.69469, acc 0.5\n",
      "current_step:  6\n",
      "2017-08-23T03:11:52.193618: step 7, loss 2.20259, acc 0.44\n",
      "current_step:  7\n",
      "2017-08-23T03:11:52.241457: step 8, loss 2.10866, acc 0.42\n",
      "current_step:  8\n",
      "2017-08-23T03:11:52.288928: step 9, loss 1.979, acc 0.46\n",
      "current_step:  9\n",
      "2017-08-23T03:11:52.335989: step 10, loss 1.27116, acc 0.6\n",
      "current_step:  10\n",
      "2017-08-23T03:11:52.381948: step 11, loss 2.00316, acc 0.38\n",
      "current_step:  11\n",
      "2017-08-23T03:11:52.430803: step 12, loss 1.18635, acc 0.56\n",
      "current_step:  12\n",
      "2017-08-23T03:11:52.479750: step 13, loss 1.68799, acc 0.32\n",
      "current_step:  13\n",
      "2017-08-23T03:11:52.531450: step 14, loss 1.26289, acc 0.52\n",
      "current_step:  14\n",
      "2017-08-23T03:11:52.580612: step 15, loss 1.71773, acc 0.32\n",
      "current_step:  15\n",
      "2017-08-23T03:11:52.631909: step 16, loss 1.58441, acc 0.3\n",
      "current_step:  16\n",
      "2017-08-23T03:11:52.677749: step 17, loss 1.44216, acc 0.5\n",
      "current_step:  17\n",
      "2017-08-23T03:11:52.725373: step 18, loss 1.61833, acc 0.38\n",
      "current_step:  18\n",
      "2017-08-23T03:11:52.772301: step 19, loss 1.47549, acc 0.4\n",
      "current_step:  19\n",
      "2017-08-23T03:11:52.823725: step 20, loss 1.48486, acc 0.4\n",
      "current_step:  20\n",
      "2017-08-23T03:11:52.870489: step 21, loss 1.76091, acc 0.34\n",
      "current_step:  21\n",
      "2017-08-23T03:11:52.916740: step 22, loss 1.26821, acc 0.5\n",
      "current_step:  22\n",
      "2017-08-23T03:11:52.962152: step 23, loss 1.41333, acc 0.48\n",
      "current_step:  23\n",
      "2017-08-23T03:11:53.007250: step 24, loss 1.11942, acc 0.56\n",
      "current_step:  24\n",
      "2017-08-23T03:11:53.055550: step 25, loss 1.21025, acc 0.56\n",
      "current_step:  25\n",
      "2017-08-23T03:11:53.102758: step 26, loss 1.01263, acc 0.66\n",
      "current_step:  26\n",
      "2017-08-23T03:11:53.156220: step 27, loss 1.14581, acc 0.56\n",
      "current_step:  27\n",
      "2017-08-23T03:11:53.203787: step 28, loss 1.33027, acc 0.5\n",
      "current_step:  28\n",
      "2017-08-23T03:11:53.252648: step 29, loss 1.33644, acc 0.48\n",
      "current_step:  29\n",
      "2017-08-23T03:11:53.301319: step 30, loss 1.42707, acc 0.44\n",
      "current_step:  30\n",
      "2017-08-23T03:11:53.351558: step 31, loss 1.25276, acc 0.5\n",
      "current_step:  31\n",
      "2017-08-23T03:11:53.397763: step 32, loss 1.50314, acc 0.54\n",
      "current_step:  32\n",
      "2017-08-23T03:11:53.447959: step 33, loss 1.01024, acc 0.56\n",
      "current_step:  33\n",
      "2017-08-23T03:11:53.500155: step 34, loss 1.13409, acc 0.52\n",
      "current_step:  34\n",
      "2017-08-23T03:11:53.549656: step 35, loss 1.02608, acc 0.52\n",
      "current_step:  35\n",
      "2017-08-23T03:11:53.596176: step 36, loss 1.43483, acc 0.54\n",
      "current_step:  36\n",
      "2017-08-23T03:11:53.643131: step 37, loss 1.67126, acc 0.44\n",
      "current_step:  37\n",
      "2017-08-23T03:11:53.689645: step 38, loss 1.33289, acc 0.46\n",
      "current_step:  38\n",
      "2017-08-23T03:11:53.746099: step 39, loss 1.33536, acc 0.42\n",
      "current_step:  39\n",
      "2017-08-23T03:11:53.812803: step 40, loss 1.37943, acc 0.48\n",
      "current_step:  40\n",
      "2017-08-23T03:11:53.879306: step 41, loss 0.975776, acc 0.7\n",
      "current_step:  41\n",
      "2017-08-23T03:11:53.946417: step 42, loss 1.12946, acc 0.6\n",
      "current_step:  42\n",
      "2017-08-23T03:11:54.014200: step 43, loss 1.11345, acc 0.6\n",
      "current_step:  43\n",
      "2017-08-23T03:11:54.082672: step 44, loss 1.32106, acc 0.48\n",
      "current_step:  44\n",
      "2017-08-23T03:11:54.148589: step 45, loss 1.3024, acc 0.56\n",
      "current_step:  45\n",
      "2017-08-23T03:11:54.215639: step 46, loss 0.891237, acc 0.72\n",
      "current_step:  46\n",
      "2017-08-23T03:11:54.282592: step 47, loss 1.32202, acc 0.44\n",
      "current_step:  47\n",
      "2017-08-23T03:11:54.348865: step 48, loss 1.32928, acc 0.5\n",
      "current_step:  48\n",
      "2017-08-23T03:11:54.418116: step 49, loss 1.32555, acc 0.46\n",
      "current_step:  49\n",
      "2017-08-23T03:11:54.484829: step 50, loss 1.2209, acc 0.42\n",
      "current_step:  50\n",
      "2017-08-23T03:11:54.551239: step 51, loss 1.20432, acc 0.6\n",
      "current_step:  51\n",
      "2017-08-23T03:11:54.617240: step 52, loss 1.17452, acc 0.48\n",
      "current_step:  52\n",
      "2017-08-23T03:11:54.684290: step 53, loss 1.16231, acc 0.56\n",
      "current_step:  53\n",
      "2017-08-23T03:11:54.751538: step 54, loss 0.911875, acc 0.68\n",
      "current_step:  54\n",
      "2017-08-23T03:11:54.817970: step 55, loss 1.21812, acc 0.56\n",
      "current_step:  55\n",
      "2017-08-23T03:11:54.884409: step 56, loss 1.23209, acc 0.48\n",
      "current_step:  56\n",
      "2017-08-23T03:11:54.952864: step 57, loss 1.07668, acc 0.54\n",
      "current_step:  57\n",
      "2017-08-23T03:11:55.019491: step 58, loss 1.17209, acc 0.56\n",
      "current_step:  58\n",
      "2017-08-23T03:11:55.085760: step 59, loss 1.48559, acc 0.44\n",
      "current_step:  59\n",
      "2017-08-23T03:11:55.152869: step 60, loss 1.08029, acc 0.6\n",
      "current_step:  60\n",
      "2017-08-23T03:11:55.219693: step 61, loss 1.19579, acc 0.54\n",
      "current_step:  61\n",
      "2017-08-23T03:11:55.286637: step 62, loss 1.46025, acc 0.54\n",
      "current_step:  62\n",
      "2017-08-23T03:11:55.354167: step 63, loss 1.38965, acc 0.36\n",
      "current_step:  63\n",
      "2017-08-23T03:11:55.421419: step 64, loss 1.23089, acc 0.48\n",
      "current_step:  64\n",
      "2017-08-23T03:11:55.489206: step 65, loss 0.933825, acc 0.62\n",
      "current_step:  65\n",
      "2017-08-23T03:11:55.556151: step 66, loss 1.10464, acc 0.6\n",
      "current_step:  66\n",
      "2017-08-23T03:11:55.623014: step 67, loss 1.04044, acc 0.58\n",
      "current_step:  67\n",
      "2017-08-23T03:11:55.690960: step 68, loss 1.33112, acc 0.56\n",
      "current_step:  68\n",
      "2017-08-23T03:11:55.758490: step 69, loss 1.04464, acc 0.54\n",
      "current_step:  69\n",
      "2017-08-23T03:11:55.825430: step 70, loss 1.09968, acc 0.56\n",
      "current_step:  70\n",
      "2017-08-23T03:11:55.891249: step 71, loss 1.30425, acc 0.5\n",
      "current_step:  71\n",
      "2017-08-23T03:11:55.958584: step 72, loss 1.12158, acc 0.5\n",
      "current_step:  72\n",
      "2017-08-23T03:11:56.025080: step 73, loss 1.07108, acc 0.68\n",
      "current_step:  73\n",
      "2017-08-23T03:11:56.091886: step 74, loss 1.35303, acc 0.4\n",
      "current_step:  74\n",
      "2017-08-23T03:11:56.159798: step 75, loss 0.988009, acc 0.58\n",
      "current_step:  75\n",
      "2017-08-23T03:11:56.226505: step 76, loss 1.11659, acc 0.62\n",
      "current_step:  76\n",
      "2017-08-23T03:11:56.293786: step 77, loss 1.09665, acc 0.64\n",
      "current_step:  77\n",
      "2017-08-23T03:11:56.359644: step 78, loss 1.18638, acc 0.54\n",
      "current_step:  78\n",
      "2017-08-23T03:11:56.426513: step 79, loss 1.04493, acc 0.64\n",
      "current_step:  79\n",
      "2017-08-23T03:11:56.493834: step 80, loss 1.23234, acc 0.56\n",
      "current_step:  80\n",
      "2017-08-23T03:11:56.560225: step 81, loss 1.37618, acc 0.52\n",
      "current_step:  81\n",
      "2017-08-23T03:11:56.627263: step 82, loss 1.02826, acc 0.72\n",
      "current_step:  82\n",
      "2017-08-23T03:11:56.694634: step 83, loss 1.29414, acc 0.54\n",
      "current_step:  83\n",
      "2017-08-23T03:11:56.760533: step 84, loss 1.14784, acc 0.58\n",
      "current_step:  84\n",
      "2017-08-23T03:11:56.827891: step 85, loss 1.01826, acc 0.64\n",
      "current_step:  85\n",
      "2017-08-23T03:11:56.895620: step 86, loss 0.896992, acc 0.66\n",
      "current_step:  86\n",
      "2017-08-23T03:11:56.963462: step 87, loss 1.17409, acc 0.5\n",
      "current_step:  87\n",
      "2017-08-23T03:11:57.030842: step 88, loss 0.993534, acc 0.68\n",
      "current_step:  88\n",
      "2017-08-23T03:11:57.102184: step 89, loss 1.10103, acc 0.62\n",
      "current_step:  89\n",
      "2017-08-23T03:11:57.168554: step 90, loss 1.11987, acc 0.6\n",
      "current_step:  90\n",
      "2017-08-23T03:11:57.236430: step 91, loss 0.909117, acc 0.7\n",
      "current_step:  91\n",
      "2017-08-23T03:11:57.303165: step 92, loss 1.21234, acc 0.62\n",
      "current_step:  92\n",
      "2017-08-23T03:11:57.370594: step 93, loss 0.826285, acc 0.66\n",
      "current_step:  93\n",
      "2017-08-23T03:11:57.437522: step 94, loss 1.12047, acc 0.58\n",
      "current_step:  94\n",
      "2017-08-23T03:11:57.505248: step 95, loss 1.20062, acc 0.58\n",
      "current_step:  95\n",
      "2017-08-23T03:11:57.572644: step 96, loss 1.28548, acc 0.52\n",
      "current_step:  96\n",
      "2017-08-23T03:11:57.641285: step 97, loss 1.35875, acc 0.52\n",
      "current_step:  97\n",
      "2017-08-23T03:11:57.707807: step 98, loss 1.16898, acc 0.6\n",
      "current_step:  98\n",
      "2017-08-23T03:11:57.775399: step 99, loss 1.02401, acc 0.58\n",
      "current_step:  99\n",
      "2017-08-23T03:11:57.841721: step 100, loss 1.15417, acc 0.58\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:11:58.081266: step 100, loss 1.09391, acc 0.578292\n",
      "\n",
      "2017-08-23T03:11:58.148500: step 101, loss 1.23153, acc 0.5\n",
      "current_step:  101\n",
      "2017-08-23T03:11:58.165279: step 102, loss 1.31126, acc 0.555556\n",
      "current_step:  102\n",
      "2017-08-23T03:11:58.232463: step 103, loss 0.980072, acc 0.64\n",
      "current_step:  103\n",
      "2017-08-23T03:11:58.299280: step 104, loss 1.184, acc 0.48\n",
      "current_step:  104\n",
      "2017-08-23T03:11:58.366206: step 105, loss 0.962454, acc 0.62\n",
      "current_step:  105\n",
      "2017-08-23T03:11:58.432620: step 106, loss 1.05231, acc 0.58\n",
      "current_step:  106\n",
      "2017-08-23T03:11:58.499124: step 107, loss 0.854845, acc 0.68\n",
      "current_step:  107\n",
      "2017-08-23T03:11:58.566038: step 108, loss 1.07901, acc 0.54\n",
      "current_step:  108\n",
      "2017-08-23T03:11:58.631594: step 109, loss 0.85444, acc 0.66\n",
      "current_step:  109\n",
      "2017-08-23T03:11:58.698318: step 110, loss 0.866274, acc 0.7\n",
      "current_step:  110\n",
      "2017-08-23T03:11:58.764437: step 111, loss 0.711317, acc 0.74\n",
      "current_step:  111\n",
      "2017-08-23T03:11:58.831765: step 112, loss 0.717249, acc 0.76\n",
      "current_step:  112\n",
      "2017-08-23T03:11:58.898789: step 113, loss 0.874094, acc 0.68\n",
      "current_step:  113\n",
      "2017-08-23T03:11:58.968289: step 114, loss 0.811245, acc 0.72\n",
      "current_step:  114\n",
      "2017-08-23T03:11:59.036428: step 115, loss 0.714163, acc 0.74\n",
      "current_step:  115\n",
      "2017-08-23T03:11:59.103747: step 116, loss 0.823577, acc 0.74\n",
      "current_step:  116\n",
      "2017-08-23T03:11:59.170562: step 117, loss 0.834595, acc 0.64\n",
      "current_step:  117\n",
      "2017-08-23T03:11:59.237486: step 118, loss 0.567182, acc 0.84\n",
      "current_step:  118\n",
      "2017-08-23T03:11:59.304126: step 119, loss 0.759339, acc 0.64\n",
      "current_step:  119\n",
      "2017-08-23T03:11:59.370782: step 120, loss 0.884338, acc 0.68\n",
      "current_step:  120\n",
      "2017-08-23T03:11:59.437169: step 121, loss 0.992676, acc 0.62\n",
      "current_step:  121\n",
      "2017-08-23T03:11:59.503677: step 122, loss 0.943612, acc 0.68\n",
      "current_step:  122\n",
      "2017-08-23T03:11:59.570494: step 123, loss 0.727925, acc 0.74\n",
      "current_step:  123\n",
      "2017-08-23T03:11:59.636567: step 124, loss 1.04201, acc 0.56\n",
      "current_step:  124\n",
      "2017-08-23T03:11:59.703118: step 125, loss 1.02113, acc 0.58\n",
      "current_step:  125\n",
      "2017-08-23T03:11:59.770386: step 126, loss 0.84394, acc 0.8\n",
      "current_step:  126\n",
      "2017-08-23T03:11:59.838249: step 127, loss 0.676859, acc 0.74\n",
      "current_step:  127\n",
      "2017-08-23T03:11:59.906720: step 128, loss 0.769138, acc 0.72\n",
      "current_step:  128\n",
      "2017-08-23T03:11:59.973814: step 129, loss 0.889837, acc 0.78\n",
      "current_step:  129\n",
      "2017-08-23T03:12:00.039862: step 130, loss 0.915762, acc 0.66\n",
      "current_step:  130\n",
      "2017-08-23T03:12:00.106157: step 131, loss 0.821886, acc 0.7\n",
      "current_step:  131\n",
      "2017-08-23T03:12:00.173091: step 132, loss 1.05469, acc 0.56\n",
      "current_step:  132\n",
      "2017-08-23T03:12:00.240908: step 133, loss 0.858462, acc 0.72\n",
      "current_step:  133\n",
      "2017-08-23T03:12:00.307375: step 134, loss 0.741007, acc 0.8\n",
      "current_step:  134\n",
      "2017-08-23T03:12:00.373871: step 135, loss 0.892316, acc 0.68\n",
      "current_step:  135\n",
      "2017-08-23T03:12:00.440479: step 136, loss 0.838682, acc 0.74\n",
      "current_step:  136\n",
      "2017-08-23T03:12:00.507587: step 137, loss 1.00448, acc 0.58\n",
      "current_step:  137\n",
      "2017-08-23T03:12:00.574245: step 138, loss 1.08372, acc 0.68\n",
      "current_step:  138\n",
      "2017-08-23T03:12:00.640756: step 139, loss 0.663134, acc 0.76\n",
      "current_step:  139\n",
      "2017-08-23T03:12:00.708788: step 140, loss 0.723158, acc 0.74\n",
      "current_step:  140\n",
      "2017-08-23T03:12:00.776775: step 141, loss 0.824366, acc 0.74\n",
      "current_step:  141\n",
      "2017-08-23T03:12:00.842945: step 142, loss 1.00323, acc 0.7\n",
      "current_step:  142\n",
      "2017-08-23T03:12:00.909022: step 143, loss 0.755231, acc 0.76\n",
      "current_step:  143\n",
      "2017-08-23T03:12:00.975847: step 144, loss 0.882004, acc 0.6\n",
      "current_step:  144\n",
      "2017-08-23T03:12:01.043188: step 145, loss 0.945966, acc 0.64\n",
      "current_step:  145\n",
      "2017-08-23T03:12:01.109386: step 146, loss 0.840085, acc 0.74\n",
      "current_step:  146\n",
      "2017-08-23T03:12:01.178143: step 147, loss 0.91001, acc 0.64\n",
      "current_step:  147\n",
      "2017-08-23T03:12:01.245382: step 148, loss 0.762563, acc 0.68\n",
      "current_step:  148\n",
      "2017-08-23T03:12:01.312177: step 149, loss 0.911446, acc 0.72\n",
      "current_step:  149\n",
      "2017-08-23T03:12:01.379493: step 150, loss 0.889455, acc 0.64\n",
      "current_step:  150\n",
      "2017-08-23T03:12:01.446871: step 151, loss 0.763259, acc 0.66\n",
      "current_step:  151\n",
      "2017-08-23T03:12:01.513318: step 152, loss 0.878586, acc 0.72\n",
      "current_step:  152\n",
      "2017-08-23T03:12:01.579399: step 153, loss 0.896304, acc 0.68\n",
      "current_step:  153\n",
      "2017-08-23T03:12:01.647315: step 154, loss 0.617293, acc 0.78\n",
      "current_step:  154\n",
      "2017-08-23T03:12:01.714406: step 155, loss 0.802055, acc 0.66\n",
      "current_step:  155\n",
      "2017-08-23T03:12:01.781563: step 156, loss 1.01898, acc 0.66\n",
      "current_step:  156\n",
      "2017-08-23T03:12:01.849275: step 157, loss 0.758351, acc 0.7\n",
      "current_step:  157\n",
      "2017-08-23T03:12:01.917847: step 158, loss 0.886325, acc 0.64\n",
      "current_step:  158\n",
      "2017-08-23T03:12:01.984972: step 159, loss 1.09038, acc 0.7\n",
      "current_step:  159\n",
      "2017-08-23T03:12:02.052963: step 160, loss 0.931096, acc 0.62\n",
      "current_step:  160\n",
      "2017-08-23T03:12:02.120214: step 161, loss 0.737958, acc 0.74\n",
      "current_step:  161\n",
      "2017-08-23T03:12:02.187920: step 162, loss 1.02846, acc 0.62\n",
      "current_step:  162\n",
      "2017-08-23T03:12:02.255782: step 163, loss 0.967474, acc 0.64\n",
      "current_step:  163\n",
      "2017-08-23T03:12:02.322818: step 164, loss 1.07033, acc 0.66\n",
      "current_step:  164\n",
      "2017-08-23T03:12:02.390782: step 165, loss 0.878683, acc 0.68\n",
      "current_step:  165\n",
      "2017-08-23T03:12:02.458240: step 166, loss 0.74475, acc 0.74\n",
      "current_step:  166\n",
      "2017-08-23T03:12:02.525116: step 167, loss 0.789258, acc 0.72\n",
      "current_step:  167\n",
      "2017-08-23T03:12:02.592008: step 168, loss 0.66091, acc 0.78\n",
      "current_step:  168\n",
      "2017-08-23T03:12:02.659315: step 169, loss 0.901328, acc 0.66\n",
      "current_step:  169\n",
      "2017-08-23T03:12:02.726661: step 170, loss 0.886177, acc 0.66\n",
      "current_step:  170\n",
      "2017-08-23T03:12:02.794360: step 171, loss 0.83745, acc 0.74\n",
      "current_step:  171\n",
      "2017-08-23T03:12:02.860423: step 172, loss 0.874755, acc 0.66\n",
      "current_step:  172\n",
      "2017-08-23T03:12:02.927630: step 173, loss 0.979549, acc 0.64\n",
      "current_step:  173\n",
      "2017-08-23T03:12:02.995635: step 174, loss 0.763177, acc 0.76\n",
      "current_step:  174\n",
      "2017-08-23T03:12:03.062413: step 175, loss 0.946794, acc 0.62\n",
      "current_step:  175\n",
      "2017-08-23T03:12:03.129153: step 176, loss 0.800059, acc 0.78\n",
      "current_step:  176\n",
      "2017-08-23T03:12:03.195975: step 177, loss 0.807874, acc 0.7\n",
      "current_step:  177\n",
      "2017-08-23T03:12:03.263461: step 178, loss 0.934828, acc 0.62\n",
      "current_step:  178\n",
      "2017-08-23T03:12:03.330021: step 179, loss 1.11222, acc 0.62\n",
      "current_step:  179\n",
      "2017-08-23T03:12:03.396717: step 180, loss 0.871775, acc 0.64\n",
      "current_step:  180\n",
      "2017-08-23T03:12:03.463899: step 181, loss 0.876878, acc 0.72\n",
      "current_step:  181\n",
      "2017-08-23T03:12:03.530748: step 182, loss 0.865955, acc 0.68\n",
      "current_step:  182\n",
      "2017-08-23T03:12:03.597111: step 183, loss 0.855376, acc 0.68\n",
      "current_step:  183\n",
      "2017-08-23T03:12:03.667172: step 184, loss 0.848438, acc 0.64\n",
      "current_step:  184\n",
      "2017-08-23T03:12:03.734824: step 185, loss 0.768467, acc 0.72\n",
      "current_step:  185\n",
      "2017-08-23T03:12:03.802326: step 186, loss 0.871776, acc 0.68\n",
      "current_step:  186\n",
      "2017-08-23T03:12:03.870153: step 187, loss 0.864865, acc 0.66\n",
      "current_step:  187\n",
      "2017-08-23T03:12:03.937008: step 188, loss 0.70501, acc 0.84\n",
      "current_step:  188\n",
      "2017-08-23T03:12:04.004879: step 189, loss 0.82014, acc 0.68\n",
      "current_step:  189\n",
      "2017-08-23T03:12:04.071836: step 190, loss 1.01281, acc 0.68\n",
      "current_step:  190\n",
      "2017-08-23T03:12:04.139053: step 191, loss 0.872158, acc 0.68\n",
      "current_step:  191\n",
      "2017-08-23T03:12:04.205760: step 192, loss 1.05742, acc 0.64\n",
      "current_step:  192\n",
      "2017-08-23T03:12:04.274350: step 193, loss 0.777807, acc 0.7\n",
      "current_step:  193\n",
      "2017-08-23T03:12:04.340927: step 194, loss 1.11407, acc 0.62\n",
      "current_step:  194\n",
      "2017-08-23T03:12:04.408893: step 195, loss 0.92554, acc 0.68\n",
      "current_step:  195\n",
      "2017-08-23T03:12:04.475524: step 196, loss 0.878087, acc 0.56\n",
      "current_step:  196\n",
      "2017-08-23T03:12:04.542588: step 197, loss 0.951968, acc 0.6\n",
      "current_step:  197\n",
      "2017-08-23T03:12:04.609492: step 198, loss 0.910356, acc 0.7\n",
      "current_step:  198\n",
      "2017-08-23T03:12:04.676149: step 199, loss 0.952891, acc 0.68\n",
      "current_step:  199\n",
      "2017-08-23T03:12:04.744262: step 200, loss 0.889642, acc 0.68\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:12:04.973241: step 200, loss 1.02139, acc 0.622776\n",
      "\n",
      "2017-08-23T03:12:05.041100: step 201, loss 1.10623, acc 0.7\n",
      "current_step:  201\n",
      "2017-08-23T03:12:05.108390: step 202, loss 0.931152, acc 0.58\n",
      "current_step:  202\n",
      "2017-08-23T03:12:05.176048: step 203, loss 0.897342, acc 0.76\n",
      "current_step:  203\n",
      "2017-08-23T03:12:05.192978: step 204, loss 0.491736, acc 0.777778\n",
      "current_step:  204\n",
      "2017-08-23T03:12:05.260958: step 205, loss 0.617435, acc 0.76\n",
      "current_step:  205\n",
      "2017-08-23T03:12:05.328227: step 206, loss 0.61013, acc 0.82\n",
      "current_step:  206\n",
      "2017-08-23T03:12:05.395045: step 207, loss 0.639355, acc 0.74\n",
      "current_step:  207\n",
      "2017-08-23T03:12:05.462364: step 208, loss 0.722844, acc 0.72\n",
      "current_step:  208\n",
      "2017-08-23T03:12:05.528484: step 209, loss 0.800957, acc 0.6\n",
      "current_step:  209\n",
      "2017-08-23T03:12:05.594311: step 210, loss 0.728083, acc 0.78\n",
      "current_step:  210\n",
      "2017-08-23T03:12:05.660526: step 211, loss 0.668555, acc 0.74\n",
      "current_step:  211\n",
      "2017-08-23T03:12:05.727911: step 212, loss 0.658015, acc 0.68\n",
      "current_step:  212\n",
      "2017-08-23T03:12:05.795918: step 213, loss 0.645367, acc 0.82\n",
      "current_step:  213\n",
      "2017-08-23T03:12:05.862819: step 214, loss 0.565673, acc 0.8\n",
      "current_step:  214\n",
      "2017-08-23T03:12:05.929808: step 215, loss 0.763265, acc 0.74\n",
      "current_step:  215\n",
      "2017-08-23T03:12:05.996383: step 216, loss 0.64693, acc 0.84\n",
      "current_step:  216\n",
      "2017-08-23T03:12:06.064058: step 217, loss 0.652786, acc 0.74\n",
      "current_step:  217\n",
      "2017-08-23T03:12:06.131021: step 218, loss 0.747603, acc 0.76\n",
      "current_step:  218\n",
      "2017-08-23T03:12:06.197611: step 219, loss 0.717267, acc 0.74\n",
      "current_step:  219\n",
      "2017-08-23T03:12:06.263995: step 220, loss 0.533883, acc 0.9\n",
      "current_step:  220\n",
      "2017-08-23T03:12:06.331504: step 221, loss 0.760271, acc 0.74\n",
      "current_step:  221\n",
      "2017-08-23T03:12:06.397375: step 222, loss 0.788217, acc 0.7\n",
      "current_step:  222\n",
      "2017-08-23T03:12:06.463224: step 223, loss 0.514677, acc 0.82\n",
      "current_step:  223\n",
      "2017-08-23T03:12:06.530054: step 224, loss 0.761525, acc 0.68\n",
      "current_step:  224\n",
      "2017-08-23T03:12:06.596828: step 225, loss 0.559825, acc 0.86\n",
      "current_step:  225\n",
      "2017-08-23T03:12:06.662975: step 226, loss 0.742316, acc 0.76\n",
      "current_step:  226\n",
      "2017-08-23T03:12:06.729053: step 227, loss 0.713576, acc 0.78\n",
      "current_step:  227\n",
      "2017-08-23T03:12:06.796683: step 228, loss 0.618473, acc 0.78\n",
      "current_step:  228\n",
      "2017-08-23T03:12:06.863868: step 229, loss 0.599325, acc 0.8\n",
      "current_step:  229\n",
      "2017-08-23T03:12:06.931369: step 230, loss 0.607868, acc 0.78\n",
      "current_step:  230\n",
      "2017-08-23T03:12:06.997105: step 231, loss 0.561679, acc 0.82\n",
      "current_step:  231\n",
      "2017-08-23T03:12:07.064049: step 232, loss 0.528281, acc 0.88\n",
      "current_step:  232\n",
      "2017-08-23T03:12:07.130840: step 233, loss 0.773091, acc 0.74\n",
      "current_step:  233\n",
      "2017-08-23T03:12:07.197613: step 234, loss 0.513266, acc 0.86\n",
      "current_step:  234\n",
      "2017-08-23T03:12:07.265486: step 235, loss 0.56967, acc 0.84\n",
      "current_step:  235\n",
      "2017-08-23T03:12:07.331839: step 236, loss 0.572722, acc 0.86\n",
      "current_step:  236\n",
      "2017-08-23T03:12:07.397864: step 237, loss 0.679626, acc 0.8\n",
      "current_step:  237\n",
      "2017-08-23T03:12:07.464415: step 238, loss 0.720986, acc 0.78\n",
      "current_step:  238\n",
      "2017-08-23T03:12:07.531947: step 239, loss 0.521392, acc 0.82\n",
      "current_step:  239\n",
      "2017-08-23T03:12:07.598581: step 240, loss 0.566955, acc 0.86\n",
      "current_step:  240\n",
      "2017-08-23T03:12:07.664702: step 241, loss 0.645884, acc 0.8\n",
      "current_step:  241\n",
      "2017-08-23T03:12:07.731729: step 242, loss 0.611668, acc 0.82\n",
      "current_step:  242\n",
      "2017-08-23T03:12:07.800090: step 243, loss 0.62428, acc 0.82\n",
      "current_step:  243\n",
      "2017-08-23T03:12:07.866848: step 244, loss 0.689356, acc 0.82\n",
      "current_step:  244\n",
      "2017-08-23T03:12:07.934205: step 245, loss 0.754881, acc 0.68\n",
      "current_step:  245\n",
      "2017-08-23T03:12:08.000996: step 246, loss 0.681937, acc 0.74\n",
      "current_step:  246\n",
      "2017-08-23T03:12:08.069295: step 247, loss 0.854972, acc 0.6\n",
      "current_step:  247\n",
      "2017-08-23T03:12:08.135566: step 248, loss 0.741865, acc 0.7\n",
      "current_step:  248\n",
      "2017-08-23T03:12:08.203531: step 249, loss 0.731756, acc 0.76\n",
      "current_step:  249\n",
      "2017-08-23T03:12:08.273981: step 250, loss 0.486022, acc 0.86\n",
      "current_step:  250\n",
      "2017-08-23T03:12:08.340487: step 251, loss 0.549822, acc 0.86\n",
      "current_step:  251\n",
      "2017-08-23T03:12:08.408187: step 252, loss 0.753105, acc 0.76\n",
      "current_step:  252\n",
      "2017-08-23T03:12:08.475874: step 253, loss 0.814486, acc 0.7\n",
      "current_step:  253\n",
      "2017-08-23T03:12:08.542992: step 254, loss 0.569715, acc 0.82\n",
      "current_step:  254\n",
      "2017-08-23T03:12:08.610527: step 255, loss 0.725291, acc 0.74\n",
      "current_step:  255\n",
      "2017-08-23T03:12:08.676637: step 256, loss 0.885837, acc 0.7\n",
      "current_step:  256\n",
      "2017-08-23T03:12:08.742250: step 257, loss 0.455519, acc 0.88\n",
      "current_step:  257\n",
      "2017-08-23T03:12:08.809083: step 258, loss 0.663531, acc 0.76\n",
      "current_step:  258\n",
      "2017-08-23T03:12:08.877014: step 259, loss 0.695284, acc 0.76\n",
      "current_step:  259\n",
      "2017-08-23T03:12:08.944236: step 260, loss 0.750609, acc 0.68\n",
      "current_step:  260\n",
      "2017-08-23T03:12:09.012624: step 261, loss 0.828695, acc 0.68\n",
      "current_step:  261\n",
      "2017-08-23T03:12:09.081536: step 262, loss 0.67852, acc 0.82\n",
      "current_step:  262\n",
      "2017-08-23T03:12:09.147830: step 263, loss 0.730409, acc 0.8\n",
      "current_step:  263\n",
      "2017-08-23T03:12:09.213392: step 264, loss 0.667455, acc 0.8\n",
      "current_step:  264\n",
      "2017-08-23T03:12:09.280587: step 265, loss 0.702647, acc 0.76\n",
      "current_step:  265\n",
      "2017-08-23T03:12:09.349489: step 266, loss 0.867906, acc 0.62\n",
      "current_step:  266\n",
      "2017-08-23T03:12:09.419669: step 267, loss 0.636557, acc 0.88\n",
      "current_step:  267\n",
      "2017-08-23T03:12:09.487795: step 268, loss 0.554514, acc 0.88\n",
      "current_step:  268\n",
      "2017-08-23T03:12:09.556368: step 269, loss 0.494346, acc 0.84\n",
      "current_step:  269\n",
      "2017-08-23T03:12:09.622510: step 270, loss 0.790801, acc 0.74\n",
      "current_step:  270\n",
      "2017-08-23T03:12:09.688877: step 271, loss 0.476888, acc 0.9\n",
      "current_step:  271\n",
      "2017-08-23T03:12:09.755346: step 272, loss 0.821436, acc 0.66\n",
      "current_step:  272\n",
      "2017-08-23T03:12:09.823931: step 273, loss 0.611122, acc 0.8\n",
      "current_step:  273\n",
      "2017-08-23T03:12:09.889848: step 274, loss 0.763503, acc 0.72\n",
      "current_step:  274\n",
      "2017-08-23T03:12:09.957500: step 275, loss 0.584194, acc 0.76\n",
      "current_step:  275\n",
      "2017-08-23T03:12:10.024000: step 276, loss 0.704366, acc 0.72\n",
      "current_step:  276\n",
      "2017-08-23T03:12:10.091962: step 277, loss 0.600378, acc 0.8\n",
      "current_step:  277\n",
      "2017-08-23T03:12:10.159456: step 278, loss 0.887941, acc 0.7\n",
      "current_step:  278\n",
      "2017-08-23T03:12:10.229063: step 279, loss 0.779044, acc 0.76\n",
      "current_step:  279\n",
      "2017-08-23T03:12:10.295344: step 280, loss 0.697574, acc 0.76\n",
      "current_step:  280\n",
      "2017-08-23T03:12:10.361089: step 281, loss 0.583338, acc 0.86\n",
      "current_step:  281\n",
      "2017-08-23T03:12:10.427601: step 282, loss 0.580305, acc 0.84\n",
      "current_step:  282\n",
      "2017-08-23T03:12:10.495338: step 283, loss 1.01555, acc 0.76\n",
      "current_step:  283\n",
      "2017-08-23T03:12:10.561777: step 284, loss 0.750737, acc 0.82\n",
      "current_step:  284\n",
      "2017-08-23T03:12:10.628048: step 285, loss 0.78283, acc 0.72\n",
      "current_step:  285\n",
      "2017-08-23T03:12:10.693755: step 286, loss 0.86598, acc 0.76\n",
      "current_step:  286\n",
      "2017-08-23T03:12:10.760425: step 287, loss 0.77856, acc 0.76\n",
      "current_step:  287\n",
      "2017-08-23T03:12:10.827526: step 288, loss 0.704673, acc 0.8\n",
      "current_step:  288\n",
      "2017-08-23T03:12:10.894135: step 289, loss 0.826803, acc 0.72\n",
      "current_step:  289\n",
      "2017-08-23T03:12:10.961029: step 290, loss 0.598152, acc 0.82\n",
      "current_step:  290\n",
      "2017-08-23T03:12:11.029357: step 291, loss 0.826327, acc 0.66\n",
      "current_step:  291\n",
      "2017-08-23T03:12:11.096533: step 292, loss 0.694712, acc 0.74\n",
      "current_step:  292\n",
      "2017-08-23T03:12:11.163485: step 293, loss 0.728869, acc 0.76\n",
      "current_step:  293\n",
      "2017-08-23T03:12:11.229364: step 294, loss 0.658792, acc 0.78\n",
      "current_step:  294\n",
      "2017-08-23T03:12:11.296262: step 295, loss 0.593624, acc 0.76\n",
      "current_step:  295\n",
      "2017-08-23T03:12:11.363282: step 296, loss 0.67997, acc 0.76\n",
      "current_step:  296\n",
      "2017-08-23T03:12:11.430968: step 297, loss 0.741306, acc 0.72\n",
      "current_step:  297\n",
      "2017-08-23T03:12:11.497364: step 298, loss 0.484887, acc 0.9\n",
      "current_step:  298\n",
      "2017-08-23T03:12:11.562872: step 299, loss 0.788835, acc 0.72\n",
      "current_step:  299\n",
      "2017-08-23T03:12:11.629842: step 300, loss 0.903241, acc 0.7\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:12:11.855496: step 300, loss 0.984603, acc 0.626335\n",
      "\n",
      "2017-08-23T03:12:11.922822: step 301, loss 0.581839, acc 0.82\n",
      "current_step:  301\n",
      "2017-08-23T03:12:11.989616: step 302, loss 0.587796, acc 0.8\n",
      "current_step:  302\n",
      "2017-08-23T03:12:12.056651: step 303, loss 0.62888, acc 0.84\n",
      "current_step:  303\n",
      "2017-08-23T03:12:12.123382: step 304, loss 0.718594, acc 0.68\n",
      "current_step:  304\n",
      "2017-08-23T03:12:12.188707: step 305, loss 0.634579, acc 0.76\n",
      "current_step:  305\n",
      "2017-08-23T03:12:12.205731: step 306, loss 0.971444, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-23T03:12:12.272929: step 307, loss 0.55237, acc 0.88\n",
      "current_step:  307\n",
      "2017-08-23T03:12:12.340752: step 308, loss 0.494545, acc 0.92\n",
      "current_step:  308\n",
      "2017-08-23T03:12:12.407873: step 309, loss 0.579077, acc 0.84\n",
      "current_step:  309\n",
      "2017-08-23T03:12:12.474847: step 310, loss 0.598579, acc 0.82\n",
      "current_step:  310\n",
      "2017-08-23T03:12:12.540284: step 311, loss 0.557986, acc 0.86\n",
      "current_step:  311\n",
      "2017-08-23T03:12:12.606653: step 312, loss 0.619624, acc 0.82\n",
      "current_step:  312\n",
      "2017-08-23T03:12:12.672856: step 313, loss 0.445297, acc 0.86\n",
      "current_step:  313\n",
      "2017-08-23T03:12:12.739456: step 314, loss 0.418376, acc 0.96\n",
      "current_step:  314\n",
      "2017-08-23T03:12:12.805308: step 315, loss 0.770114, acc 0.7\n",
      "current_step:  315\n",
      "2017-08-23T03:12:12.872540: step 316, loss 0.482588, acc 0.76\n",
      "current_step:  316\n",
      "2017-08-23T03:12:12.939815: step 317, loss 0.62092, acc 0.76\n",
      "current_step:  317\n",
      "2017-08-23T03:12:13.006929: step 318, loss 0.407421, acc 0.9\n",
      "current_step:  318\n",
      "2017-08-23T03:12:13.074062: step 319, loss 0.575114, acc 0.86\n",
      "current_step:  319\n",
      "2017-08-23T03:12:13.140716: step 320, loss 0.551186, acc 0.84\n",
      "current_step:  320\n",
      "2017-08-23T03:12:13.207753: step 321, loss 0.566517, acc 0.86\n",
      "current_step:  321\n",
      "2017-08-23T03:12:13.274333: step 322, loss 0.607603, acc 0.8\n",
      "current_step:  322\n",
      "2017-08-23T03:12:13.340332: step 323, loss 0.612202, acc 0.86\n",
      "current_step:  323\n",
      "2017-08-23T03:12:13.406653: step 324, loss 0.579685, acc 0.76\n",
      "current_step:  324\n",
      "2017-08-23T03:12:13.474413: step 325, loss 0.555435, acc 0.82\n",
      "current_step:  325\n",
      "2017-08-23T03:12:13.540548: step 326, loss 0.400034, acc 0.86\n",
      "current_step:  326\n",
      "2017-08-23T03:12:13.606294: step 327, loss 0.485002, acc 0.86\n",
      "current_step:  327\n",
      "2017-08-23T03:12:13.672313: step 328, loss 0.364735, acc 0.9\n",
      "current_step:  328\n",
      "2017-08-23T03:12:13.739289: step 329, loss 0.621369, acc 0.72\n",
      "current_step:  329\n",
      "2017-08-23T03:12:13.806034: step 330, loss 0.623976, acc 0.76\n",
      "current_step:  330\n",
      "2017-08-23T03:12:13.872655: step 331, loss 0.515114, acc 0.82\n",
      "current_step:  331\n",
      "2017-08-23T03:12:13.938956: step 332, loss 0.478857, acc 0.86\n",
      "current_step:  332\n",
      "2017-08-23T03:12:14.005839: step 333, loss 0.472607, acc 0.9\n",
      "current_step:  333\n",
      "2017-08-23T03:12:14.073693: step 334, loss 0.372955, acc 0.94\n",
      "current_step:  334\n",
      "2017-08-23T03:12:14.139659: step 335, loss 0.355017, acc 0.94\n",
      "current_step:  335\n",
      "2017-08-23T03:12:14.205442: step 336, loss 0.522651, acc 0.88\n",
      "current_step:  336\n",
      "2017-08-23T03:12:14.271393: step 337, loss 0.500826, acc 0.84\n",
      "current_step:  337\n",
      "2017-08-23T03:12:14.338134: step 338, loss 0.561646, acc 0.82\n",
      "current_step:  338\n",
      "2017-08-23T03:12:14.404359: step 339, loss 0.459211, acc 0.9\n",
      "current_step:  339\n",
      "2017-08-23T03:12:14.471302: step 340, loss 0.589803, acc 0.84\n",
      "current_step:  340\n",
      "2017-08-23T03:12:14.538582: step 341, loss 0.718221, acc 0.72\n",
      "current_step:  341\n",
      "2017-08-23T03:12:14.605086: step 342, loss 0.639453, acc 0.82\n",
      "current_step:  342\n",
      "2017-08-23T03:12:14.672102: step 343, loss 0.499973, acc 0.88\n",
      "current_step:  343\n",
      "2017-08-23T03:12:14.738849: step 344, loss 0.551755, acc 0.9\n",
      "current_step:  344\n",
      "2017-08-23T03:12:14.805586: step 345, loss 0.451906, acc 0.9\n",
      "current_step:  345\n",
      "2017-08-23T03:12:14.872337: step 346, loss 0.452693, acc 0.88\n",
      "current_step:  346\n",
      "2017-08-23T03:12:14.938992: step 347, loss 0.718567, acc 0.74\n",
      "current_step:  347\n",
      "2017-08-23T03:12:15.010901: step 348, loss 0.66778, acc 0.78\n",
      "current_step:  348\n",
      "2017-08-23T03:12:15.077766: step 349, loss 0.591335, acc 0.8\n",
      "current_step:  349\n",
      "2017-08-23T03:12:15.143720: step 350, loss 0.310079, acc 0.96\n",
      "current_step:  350\n",
      "2017-08-23T03:12:15.210331: step 351, loss 0.599602, acc 0.82\n",
      "current_step:  351\n",
      "2017-08-23T03:12:15.277981: step 352, loss 0.69173, acc 0.8\n",
      "current_step:  352\n",
      "2017-08-23T03:12:15.344788: step 353, loss 0.503263, acc 0.86\n",
      "current_step:  353\n",
      "2017-08-23T03:12:15.411138: step 354, loss 0.462556, acc 0.9\n",
      "current_step:  354\n",
      "2017-08-23T03:12:15.477430: step 355, loss 0.538743, acc 0.82\n",
      "current_step:  355\n",
      "2017-08-23T03:12:15.544765: step 356, loss 0.734787, acc 0.7\n",
      "current_step:  356\n",
      "2017-08-23T03:12:15.611637: step 357, loss 0.533312, acc 0.84\n",
      "current_step:  357\n",
      "2017-08-23T03:12:15.678360: step 358, loss 0.659755, acc 0.8\n",
      "current_step:  358\n",
      "2017-08-23T03:12:15.744681: step 359, loss 0.516967, acc 0.84\n",
      "current_step:  359\n",
      "2017-08-23T03:12:15.811817: step 360, loss 0.514413, acc 0.86\n",
      "current_step:  360\n",
      "2017-08-23T03:12:15.878590: step 361, loss 0.415449, acc 0.92\n",
      "current_step:  361\n",
      "2017-08-23T03:12:15.945828: step 362, loss 0.528681, acc 0.86\n",
      "current_step:  362\n",
      "2017-08-23T03:12:16.013138: step 363, loss 0.533542, acc 0.76\n",
      "current_step:  363\n",
      "2017-08-23T03:12:16.079951: step 364, loss 0.344912, acc 0.94\n",
      "current_step:  364\n",
      "2017-08-23T03:12:16.147470: step 365, loss 0.602711, acc 0.88\n",
      "current_step:  365\n",
      "2017-08-23T03:12:16.212810: step 366, loss 0.473023, acc 0.84\n",
      "current_step:  366\n",
      "2017-08-23T03:12:16.279375: step 367, loss 0.573048, acc 0.84\n",
      "current_step:  367\n",
      "2017-08-23T03:12:16.345787: step 368, loss 0.429854, acc 0.92\n",
      "current_step:  368\n",
      "2017-08-23T03:12:16.411799: step 369, loss 0.657213, acc 0.78\n",
      "current_step:  369\n",
      "2017-08-23T03:12:16.478354: step 370, loss 0.7004, acc 0.78\n",
      "current_step:  370\n",
      "2017-08-23T03:12:16.545029: step 371, loss 0.416073, acc 0.94\n",
      "current_step:  371\n",
      "2017-08-23T03:12:16.611433: step 372, loss 0.461301, acc 0.88\n",
      "current_step:  372\n",
      "2017-08-23T03:12:16.677631: step 373, loss 0.520481, acc 0.84\n",
      "current_step:  373\n",
      "2017-08-23T03:12:16.744254: step 374, loss 0.544278, acc 0.84\n",
      "current_step:  374\n",
      "2017-08-23T03:12:16.811487: step 375, loss 0.556684, acc 0.8\n",
      "current_step:  375\n",
      "2017-08-23T03:12:16.877695: step 376, loss 0.469109, acc 0.84\n",
      "current_step:  376\n",
      "2017-08-23T03:12:16.944220: step 377, loss 0.660149, acc 0.72\n",
      "current_step:  377\n",
      "2017-08-23T03:12:17.010895: step 378, loss 0.503397, acc 0.82\n",
      "current_step:  378\n",
      "2017-08-23T03:12:17.078531: step 379, loss 0.537322, acc 0.84\n",
      "current_step:  379\n",
      "2017-08-23T03:12:17.145074: step 380, loss 0.580559, acc 0.74\n",
      "current_step:  380\n",
      "2017-08-23T03:12:17.211205: step 381, loss 0.621907, acc 0.78\n",
      "current_step:  381\n",
      "2017-08-23T03:12:17.277551: step 382, loss 0.674333, acc 0.78\n",
      "current_step:  382\n",
      "2017-08-23T03:12:17.345162: step 383, loss 0.472073, acc 0.86\n",
      "current_step:  383\n",
      "2017-08-23T03:12:17.411648: step 384, loss 0.549653, acc 0.76\n",
      "current_step:  384\n",
      "2017-08-23T03:12:17.477121: step 385, loss 0.581179, acc 0.8\n",
      "current_step:  385\n",
      "2017-08-23T03:12:17.543536: step 386, loss 0.545728, acc 0.8\n",
      "current_step:  386\n",
      "2017-08-23T03:12:17.609263: step 387, loss 0.497154, acc 0.82\n",
      "current_step:  387\n",
      "2017-08-23T03:12:17.674947: step 388, loss 0.636837, acc 0.74\n",
      "current_step:  388\n",
      "2017-08-23T03:12:17.740707: step 389, loss 0.463234, acc 0.84\n",
      "current_step:  389\n",
      "2017-08-23T03:12:17.808536: step 390, loss 0.776914, acc 0.64\n",
      "current_step:  390\n",
      "2017-08-23T03:12:17.876096: step 391, loss 0.518451, acc 0.84\n",
      "current_step:  391\n",
      "2017-08-23T03:12:17.943010: step 392, loss 0.388009, acc 0.88\n",
      "current_step:  392\n",
      "2017-08-23T03:12:18.010519: step 393, loss 0.623446, acc 0.82\n",
      "current_step:  393\n",
      "2017-08-23T03:12:18.078018: step 394, loss 0.560236, acc 0.84\n",
      "current_step:  394\n",
      "2017-08-23T03:12:18.145014: step 395, loss 0.580222, acc 0.82\n",
      "current_step:  395\n",
      "2017-08-23T03:12:18.212134: step 396, loss 0.552105, acc 0.9\n",
      "current_step:  396\n",
      "2017-08-23T03:12:18.279223: step 397, loss 0.565803, acc 0.78\n",
      "current_step:  397\n",
      "2017-08-23T03:12:18.345764: step 398, loss 0.40757, acc 0.88\n",
      "current_step:  398\n",
      "2017-08-23T03:12:18.413414: step 399, loss 0.411191, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-23T03:12:18.480511: step 400, loss 0.414175, acc 0.86\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:12:18.711504: step 400, loss 0.954152, acc 0.674377\n",
      "\n",
      "2017-08-23T03:12:18.778165: step 401, loss 0.791392, acc 0.68\n",
      "current_step:  401\n",
      "2017-08-23T03:12:18.844630: step 402, loss 0.438362, acc 0.9\n",
      "current_step:  402\n",
      "2017-08-23T03:12:18.910526: step 403, loss 0.637632, acc 0.76\n",
      "current_step:  403\n",
      "2017-08-23T03:12:18.977165: step 404, loss 0.543057, acc 0.86\n",
      "current_step:  404\n",
      "2017-08-23T03:12:19.044104: step 405, loss 0.372748, acc 0.88\n",
      "current_step:  405\n",
      "2017-08-23T03:12:19.111043: step 406, loss 0.589454, acc 0.8\n",
      "current_step:  406\n",
      "2017-08-23T03:12:19.178335: step 407, loss 0.535123, acc 0.82\n",
      "current_step:  407\n",
      "2017-08-23T03:12:19.196115: step 408, loss 0.657041, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-23T03:12:19.263995: step 409, loss 0.381018, acc 0.92\n",
      "current_step:  409\n",
      "2017-08-23T03:12:19.329722: step 410, loss 0.368316, acc 0.9\n",
      "current_step:  410\n",
      "2017-08-23T03:12:19.396583: step 411, loss 0.432683, acc 0.84\n",
      "current_step:  411\n",
      "2017-08-23T03:12:19.463899: step 412, loss 0.335758, acc 0.92\n",
      "current_step:  412\n",
      "2017-08-23T03:12:19.530431: step 413, loss 0.594175, acc 0.88\n",
      "current_step:  413\n",
      "2017-08-23T03:12:19.596845: step 414, loss 0.374051, acc 0.9\n",
      "current_step:  414\n",
      "2017-08-23T03:12:19.663533: step 415, loss 0.362225, acc 0.9\n",
      "current_step:  415\n",
      "2017-08-23T03:12:19.729360: step 416, loss 0.339304, acc 0.94\n",
      "current_step:  416\n",
      "2017-08-23T03:12:19.795373: step 417, loss 0.271907, acc 0.94\n",
      "current_step:  417\n",
      "2017-08-23T03:12:19.861502: step 418, loss 0.394793, acc 0.96\n",
      "current_step:  418\n",
      "2017-08-23T03:12:19.928132: step 419, loss 0.660738, acc 0.76\n",
      "current_step:  419\n",
      "2017-08-23T03:12:19.995308: step 420, loss 0.351237, acc 0.96\n",
      "current_step:  420\n",
      "2017-08-23T03:12:20.062067: step 421, loss 0.380221, acc 0.9\n",
      "current_step:  421\n",
      "2017-08-23T03:12:20.129297: step 422, loss 0.42165, acc 0.88\n",
      "current_step:  422\n",
      "2017-08-23T03:12:20.195757: step 423, loss 0.382041, acc 0.88\n",
      "current_step:  423\n",
      "2017-08-23T03:12:20.263069: step 424, loss 0.441326, acc 0.88\n",
      "current_step:  424\n",
      "2017-08-23T03:12:20.330347: step 425, loss 0.355076, acc 0.94\n",
      "current_step:  425\n",
      "2017-08-23T03:12:20.397411: step 426, loss 0.423787, acc 0.92\n",
      "current_step:  426\n",
      "2017-08-23T03:12:20.463728: step 427, loss 0.401748, acc 0.9\n",
      "current_step:  427\n",
      "2017-08-23T03:12:20.531142: step 428, loss 0.389791, acc 0.9\n",
      "current_step:  428\n",
      "2017-08-23T03:12:20.597055: step 429, loss 0.382435, acc 0.88\n",
      "current_step:  429\n",
      "2017-08-23T03:12:20.662873: step 430, loss 0.426058, acc 0.9\n",
      "current_step:  430\n",
      "2017-08-23T03:12:20.730527: step 431, loss 0.449838, acc 0.88\n",
      "current_step:  431\n",
      "2017-08-23T03:12:20.796257: step 432, loss 0.34462, acc 0.92\n",
      "current_step:  432\n",
      "2017-08-23T03:12:20.863015: step 433, loss 0.330667, acc 0.92\n",
      "current_step:  433\n",
      "2017-08-23T03:12:20.929255: step 434, loss 0.347493, acc 0.94\n",
      "current_step:  434\n",
      "2017-08-23T03:12:20.996111: step 435, loss 0.374213, acc 0.92\n",
      "current_step:  435\n",
      "2017-08-23T03:12:21.062964: step 436, loss 0.666914, acc 0.78\n",
      "current_step:  436\n",
      "2017-08-23T03:12:21.128355: step 437, loss 0.304253, acc 0.96\n",
      "current_step:  437\n",
      "2017-08-23T03:12:21.194563: step 438, loss 0.45299, acc 0.9\n",
      "current_step:  438\n",
      "2017-08-23T03:12:21.262360: step 439, loss 0.418436, acc 0.88\n",
      "current_step:  439\n",
      "2017-08-23T03:12:21.329057: step 440, loss 0.421898, acc 0.88\n",
      "current_step:  440\n",
      "2017-08-23T03:12:21.395690: step 441, loss 0.436617, acc 0.92\n",
      "current_step:  441\n",
      "2017-08-23T03:12:21.462638: step 442, loss 0.458553, acc 0.86\n",
      "current_step:  442\n",
      "2017-08-23T03:12:21.529403: step 443, loss 0.478101, acc 0.86\n",
      "current_step:  443\n",
      "2017-08-23T03:12:21.595109: step 444, loss 0.336665, acc 0.96\n",
      "current_step:  444\n",
      "2017-08-23T03:12:21.661000: step 445, loss 0.368678, acc 0.94\n",
      "current_step:  445\n",
      "2017-08-23T03:12:21.727880: step 446, loss 0.514586, acc 0.84\n",
      "current_step:  446\n",
      "2017-08-23T03:12:21.795008: step 447, loss 0.425652, acc 0.84\n",
      "current_step:  447\n",
      "2017-08-23T03:12:21.862446: step 448, loss 0.324921, acc 0.96\n",
      "current_step:  448\n",
      "2017-08-23T03:12:21.930761: step 449, loss 0.424615, acc 0.86\n",
      "current_step:  449\n",
      "2017-08-23T03:12:21.996921: step 450, loss 0.564896, acc 0.82\n",
      "current_step:  450\n",
      "2017-08-23T03:12:22.064090: step 451, loss 0.377569, acc 0.9\n",
      "current_step:  451\n",
      "2017-08-23T03:12:22.131881: step 452, loss 0.350204, acc 0.96\n",
      "current_step:  452\n",
      "2017-08-23T03:12:22.198602: step 453, loss 0.451956, acc 0.84\n",
      "current_step:  453\n",
      "2017-08-23T03:12:22.268125: step 454, loss 0.4084, acc 0.86\n",
      "current_step:  454\n",
      "2017-08-23T03:12:22.336565: step 455, loss 0.399961, acc 0.88\n",
      "current_step:  455\n",
      "2017-08-23T03:12:22.403350: step 456, loss 0.295159, acc 0.94\n",
      "current_step:  456\n",
      "2017-08-23T03:12:22.470770: step 457, loss 0.502951, acc 0.82\n",
      "current_step:  457\n",
      "2017-08-23T03:12:22.538249: step 458, loss 0.473809, acc 0.92\n",
      "current_step:  458\n",
      "2017-08-23T03:12:22.604236: step 459, loss 0.466319, acc 0.88\n",
      "current_step:  459\n",
      "2017-08-23T03:12:22.671969: step 460, loss 0.354853, acc 0.92\n",
      "current_step:  460\n",
      "2017-08-23T03:12:22.739161: step 461, loss 0.40352, acc 0.9\n",
      "current_step:  461\n",
      "2017-08-23T03:12:22.806296: step 462, loss 0.403891, acc 0.94\n",
      "current_step:  462\n",
      "2017-08-23T03:12:22.873057: step 463, loss 0.338751, acc 0.9\n",
      "current_step:  463\n",
      "2017-08-23T03:12:22.940231: step 464, loss 0.385874, acc 0.92\n",
      "current_step:  464\n",
      "2017-08-23T03:12:23.007816: step 465, loss 0.392266, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-23T03:12:23.074570: step 466, loss 0.405298, acc 0.88\n",
      "current_step:  466\n",
      "2017-08-23T03:12:23.142100: step 467, loss 0.405828, acc 0.9\n",
      "current_step:  467\n",
      "2017-08-23T03:12:23.211231: step 468, loss 0.413454, acc 0.88\n",
      "current_step:  468\n",
      "2017-08-23T03:12:23.278373: step 469, loss 0.447291, acc 0.9\n",
      "current_step:  469\n",
      "2017-08-23T03:12:23.344764: step 470, loss 0.417481, acc 0.92\n",
      "current_step:  470\n",
      "2017-08-23T03:12:23.411676: step 471, loss 0.398988, acc 0.9\n",
      "current_step:  471\n",
      "2017-08-23T03:12:23.478379: step 472, loss 0.290587, acc 0.96\n",
      "current_step:  472\n",
      "2017-08-23T03:12:23.544609: step 473, loss 0.360602, acc 0.92\n",
      "current_step:  473\n",
      "2017-08-23T03:12:23.611960: step 474, loss 0.559479, acc 0.84\n",
      "current_step:  474\n",
      "2017-08-23T03:12:23.678812: step 475, loss 0.51987, acc 0.86\n",
      "current_step:  475\n",
      "2017-08-23T03:12:23.745584: step 476, loss 0.418478, acc 0.92\n",
      "current_step:  476\n",
      "2017-08-23T03:12:23.812790: step 477, loss 0.593146, acc 0.84\n",
      "current_step:  477\n",
      "2017-08-23T03:12:23.882568: step 478, loss 0.281127, acc 0.94\n",
      "current_step:  478\n",
      "2017-08-23T03:12:23.952102: step 479, loss 0.372853, acc 0.94\n",
      "current_step:  479\n",
      "2017-08-23T03:12:24.018858: step 480, loss 0.459775, acc 0.84\n",
      "current_step:  480\n",
      "2017-08-23T03:12:24.086675: step 481, loss 0.406571, acc 0.86\n",
      "current_step:  481\n",
      "2017-08-23T03:12:24.154299: step 482, loss 0.414443, acc 0.9\n",
      "current_step:  482\n",
      "2017-08-23T03:12:24.221118: step 483, loss 0.385464, acc 0.88\n",
      "current_step:  483\n",
      "2017-08-23T03:12:24.288003: step 484, loss 0.582774, acc 0.86\n",
      "current_step:  484\n",
      "2017-08-23T03:12:24.355170: step 485, loss 0.376216, acc 0.86\n",
      "current_step:  485\n",
      "2017-08-23T03:12:24.422447: step 486, loss 0.409274, acc 0.9\n",
      "current_step:  486\n",
      "2017-08-23T03:12:24.490669: step 487, loss 0.323298, acc 0.96\n",
      "current_step:  487\n",
      "2017-08-23T03:12:24.557619: step 488, loss 0.410152, acc 0.88\n",
      "current_step:  488\n",
      "2017-08-23T03:12:24.624516: step 489, loss 0.358217, acc 0.94\n",
      "current_step:  489\n",
      "2017-08-23T03:12:24.691063: step 490, loss 0.39177, acc 0.9\n",
      "current_step:  490\n",
      "2017-08-23T03:12:24.758268: step 491, loss 0.380424, acc 0.88\n",
      "current_step:  491\n",
      "2017-08-23T03:12:24.826632: step 492, loss 0.364706, acc 0.88\n",
      "current_step:  492\n",
      "2017-08-23T03:12:24.892151: step 493, loss 0.457871, acc 0.88\n",
      "current_step:  493\n",
      "2017-08-23T03:12:24.959446: step 494, loss 0.343395, acc 0.92\n",
      "current_step:  494\n",
      "2017-08-23T03:12:25.027272: step 495, loss 0.472405, acc 0.9\n",
      "current_step:  495\n",
      "2017-08-23T03:12:25.093982: step 496, loss 0.26443, acc 0.94\n",
      "current_step:  496\n",
      "2017-08-23T03:12:25.160792: step 497, loss 0.469022, acc 0.8\n",
      "current_step:  497\n",
      "2017-08-23T03:12:25.229147: step 498, loss 0.469354, acc 0.82\n",
      "current_step:  498\n",
      "2017-08-23T03:12:25.296400: step 499, loss 0.56743, acc 0.78\n",
      "current_step:  499\n",
      "2017-08-23T03:12:25.363814: step 500, loss 0.404243, acc 0.9\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-23T03:12:25.591117: step 500, loss 0.950437, acc 0.647687\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170823_0311_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-23T03:12:25.715949: step 501, loss 0.341019, acc 0.88\n",
      "current_step:  501\n",
      "2017-08-23T03:12:25.782241: step 502, loss 0.298323, acc 0.92\n",
      "current_step:  502\n",
      "2017-08-23T03:12:25.848764: step 503, loss 0.466738, acc 0.9\n",
      "current_step:  503\n",
      "2017-08-23T03:12:25.915284: step 504, loss 0.299728, acc 0.96\n",
      "current_step:  504\n",
      "2017-08-23T03:12:25.982746: step 505, loss 0.276431, acc 0.98\n",
      "current_step:  505\n",
      "2017-08-23T03:12:26.049318: step 506, loss 0.413296, acc 0.92\n",
      "current_step:  506\n",
      "2017-08-23T03:12:26.116398: step 507, loss 0.30264, acc 0.94\n",
      "current_step:  507\n",
      "2017-08-23T03:12:26.182255: step 508, loss 0.570919, acc 0.82\n",
      "current_step:  508\n",
      "2017-08-23T03:12:26.249100: step 509, loss 0.393461, acc 0.9\n",
      "current_step:  509\n",
      "2017-08-23T03:12:26.266024: step 510, loss 0.26466, acc 1\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.538636\n",
      "Precision Score = 0.522709\n",
      "Recall Score = 0.571174\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.584411\n",
      "Precision Score = 0.581154\n",
      "Recall Score = 0.615658\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.579130\n",
      "Precision Score = 0.571229\n",
      "Recall Score = 0.617438\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.630279\n",
      "Precision Score = 0.620294\n",
      "Recall Score = 0.665480\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.605545\n",
      "Precision Score = 0.594702\n",
      "Recall Score = 0.638790\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v3 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 3, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
