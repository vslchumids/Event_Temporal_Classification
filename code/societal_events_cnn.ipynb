{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Term Project: Event Temporal State Identification\n",
    "\n",
    "## CNN Model\n",
    "\n",
    "### John Chiang, Vincent Chu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "#import data_helpers\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# CNN\n",
    "from text_cnn import TextCNN\n",
    "from nlp_cnn import NLPCNN\n",
    "\n",
    "# Custom libraries\n",
    "import societal_data_processor as sdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Data Loading, CNN Model Ops and CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=50\n",
      "CHECKPOINT_EVERY=500\n",
      "DATA_DIR=/home/vslchu/w266/project/data/eventstatus_eng/\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=1\n",
      "EMBEDDING_DIM=50\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=5\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================================\n",
    "# Parameters\n",
    "#===========================================================================================================\n",
    "\n",
    "try:\n",
    "    # Data loading params\n",
    "    tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "    tf.flags.DEFINE_string(\"data_dir\", '/home/vslchu/w266/project/data/eventstatus_eng/', \"Directory for Annotated Societal Events Data\")\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    tf.flags.DEFINE_integer(\"embedding_dim\", 50, \"Dimensionality of character embedding (default: 128)\") \n",
    "    tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "    tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\") #50\n",
    "    tf.flags.DEFINE_float(\"dropout_keep_prob\", 1, \"Dropout keep probability (default: 1)\") #0.5\n",
    "    tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "    # Training parameters\n",
    "    tf.flags.DEFINE_integer(\"batch_size\", 50, \"Batch Size (default: 64)\")\n",
    "    tf.flags.DEFINE_integer(\"num_epochs\", 5, \"Number of training epochs (default: 200)\")\n",
    "    tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "    tf.flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
    "    tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "    \n",
    "    # Misc Parameters\n",
    "    tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "    tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "except:\n",
    "    print \"Tf Flags already defined\"\n",
    "\n",
    "params = tf.flags.FLAGS\n",
    "params._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(params.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#===========================================================================================================\n",
    "# Functions\n",
    "#===========================================================================================================\n",
    "\n",
    "def load_text_data(data_dir, \n",
    "                   processor_ver, \n",
    "                   remove_stopwords = False, \n",
    "                   replace_num = False, \n",
    "                   remove_non_alpha = False, \n",
    "                   to_lower = False, \n",
    "                   to_subwords = False):\n",
    "    \n",
    "    # Load data from the annotated files\n",
    "    print(\"Loading data...\")\n",
    "    (original_chunks, clean_chunks, clean_chunk_sents, temporal_states, event_files) = \\\n",
    "    sdp.get_chunks_n_annotations(data_dir, \n",
    "                                 processor_ver, \n",
    "                                 remove_stopwords, \n",
    "                                 replace_num, \n",
    "                                 remove_non_alpha, \n",
    "                                 to_lower, \n",
    "                                 to_subwords)\n",
    "\n",
    "    # Tranform annotations into lists of binaries\n",
    "    y = sdp.transform_annotations_to_binary(temporal_states)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_chunk_length = max([len(x.split(\" \")) for x in clean_chunks])\n",
    "    print \"max_chunk_length = \", max_chunk_length\n",
    "\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_chunk_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(clean_chunks)))\n",
    "\n",
    "    (x_train, x_test) = sdp.split_train_test_data(x, params.dev_sample_percentage)\n",
    "    (y_train, y_test) = sdp.split_train_test_data(y, params.dev_sample_percentage)\n",
    "    (y_orig_train, y_orig_test) = sdp.split_train_test_data(temporal_states, params.dev_sample_percentage)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split on data (x): {:d}/{:d}\".format(len(x_train), len(x_test)))\n",
    "    print(\"Train/Dev split on labels (y): {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
    "    \n",
    "    return (x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor)\n",
    "\n",
    "# A single training step on a batch from the training data set\n",
    "def train_step(sess, cnn, x_batch, y_batch, summaries_on = False):\n",
    "\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: params.dropout_keep_prob\n",
    "    }\n",
    "\n",
    "    if summaries_on:\n",
    "        _, step, summaries, loss, accuracy, predictions = sess.run(\n",
    "            [cnn.train_op, cnn.global_step, cnn.train_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "            feed_dict)\n",
    "        \n",
    "        train_summary_writer = tf.summary.FileWriter(cnn.train_summary_dir, sess.graph)        \n",
    "        train_summary_writer.add_summary(summaries, step)        \n",
    "    else:\n",
    "        _, step, loss, accuracy, predictions = sess.run(\n",
    "            [cnn.train_op, cnn.global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "            feed_dict)\n",
    "        \n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    return (loss, accuracy, predictions)\n",
    "\n",
    "# Evaluates CNN model on the test data set\n",
    "def test_step(sess, cnn, x_batch, y_batch, writer = None, summaries_on = False):\n",
    "\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    \n",
    "    if summaries_on:    \n",
    "        step, summaries, loss, accuracy, predictions = sess.run(\n",
    "            [cnn.global_step, cnn.test_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "            feed_dict)\n",
    "        \n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)       \n",
    "    else:\n",
    "        step, loss, accuracy, predictions = sess.run(\n",
    "            [cnn.global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "            feed_dict)\n",
    "        \n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    \n",
    "    return (loss, accuracy, predictions) \n",
    "\n",
    "def run_cnn(x_train, y_train, x_test, y_test, vocab_processor): \n",
    "    \n",
    "    train_preds = []\n",
    "    test_preds = []\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=params.allow_soft_placement,\n",
    "          log_device_placement=params.log_device_placement)\n",
    "\n",
    "        sess = tf.Session(config=session_conf)\n",
    "\n",
    "        with sess.as_default():\n",
    "            cnn = NLPCNN(sequence_length = x_train.shape[1],\n",
    "                         num_classes = y_train.shape[1],\n",
    "                         vocab_size = len(vocab_processor.vocabulary_),\n",
    "                         embedding_size = params.embedding_dim,\n",
    "                         filter_sizes = list(map(int, params.filter_sizes.split(\",\"))),\n",
    "                         num_filters = params.num_filters,\n",
    "                         l2_reg_lambda = params.l2_reg_lambda)\n",
    "            \n",
    "            cnn.build_core_graph()\n",
    "            cnn.build_train_test_graph()\n",
    "            \n",
    "            print \"cnn.out_dir = \", cnn.out_dir\n",
    "\n",
    "            # Write vocabulary\n",
    "            #vocab_processor.save(os.path.join(cnn.out_dir, \"vocab\"))   \n",
    "            \n",
    "            checkpoint_dir = os.path.abspath(os.path.join(cnn.out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep = cnn.num_checkpoints)            \n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())                     \n",
    "\n",
    "            # Generate batches\n",
    "            batches = sdp.batch_iter(\n",
    "                list(zip(x_train, y_train)), params.batch_size, params.num_epochs)\n",
    "\n",
    "            batch_count = 0\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                loss, accuracy, predictions = train_step(sess, cnn, x_batch, y_batch)\n",
    "\n",
    "                current_step = tf.train.global_step(sess, cnn.global_step)                \n",
    "                print \"current_step: \", current_step\n",
    "\n",
    "                for i in range(len(predictions)):\n",
    "                    train_preds.append(predictions[i])\n",
    "\n",
    "                if current_step % params.evaluate_every == 0:\n",
    "                    print \"\\nPredicting annotation for test data:\"\n",
    "                    #test_summary_writer = tf.summary.FileWriter(cnn.test_summary_dir, sess.graph)\n",
    "                    #loss, accuracy, predictions = test_step(sess, cnn, x_test, y_test, writer = test_summary_writer)\n",
    "                    loss, accuracy, predictions = test_step(sess, cnn, x_test, y_test, summaries_on = False)\n",
    "                    test_preds.append(list(predictions))\n",
    "                    print\n",
    "\n",
    "                if current_step % params.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step = current_step)\n",
    "                    print \"Saved model checkpoint to {}\\n\".format(path)\n",
    "\n",
    "                batch_count += 1\n",
    "                \n",
    "            #print \"\\nFinal round of predicting annotation for test data:\"\n",
    "            #test_summary_writer = tf.summary.FileWriter(cnn.test_summary_dir, sess.graph)\n",
    "            #loss, accuracy, predictions = test_step(sess, cnn, x_test, y_test, writer = test_summary_writer)\n",
    "            #loss, accuracy, predictions = test_step(sess, cnn, x_test, y_test, summaries_on = False)\n",
    "            #test_preds.append(list(predictions))\n",
    "\n",
    "            print \"\\nRan %d batches during training and created %d rounds of predictions\" % (batch_count, len(test_preds))\n",
    "\n",
    "    return test_preds\n",
    "\n",
    "############################################################################################################\n",
    "# Function Name: eval_preds\n",
    "# Description  :\n",
    "# Parameters   :\n",
    "#   test_preds_list: list of predictions from various evaluation checkpoint during the training cycle\n",
    "#   test_labels    : Labels for the test data set\n",
    "############################################################################################################\n",
    "\n",
    "def eval_preds(test_preds_list, test_labels):\n",
    "\n",
    "    #reload(sdp)\n",
    "    test_pred_annotations = []\n",
    "    test_pred_eval = []\n",
    "    \n",
    "    for i in range(len(test_preds_list)):\n",
    "        temp_test_pred_annotations = sdp.transform_digits_to_annotations(test_preds_list[i])\n",
    "        test_pred_annotations.append(temp_test_pred_annotations)\n",
    "                \n",
    "        ### Evaluate Performance of model        \n",
    "        f1 = f1_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        precision = precision_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        recall = recall_score(test_labels, temp_test_pred_annotations, average='weighted')\n",
    "        \n",
    "        print \"\\nPerformance Evaluation of CNN Model (i = %d):\" % i\n",
    "        print \"F1 Score = %f\" % f1\n",
    "        print \"Precision Score = %f\" % precision\n",
    "        print \"Recall Score = %f\" % recall \n",
    "\n",
    "        test_pred_eval.append((f1, precision, recall))\n",
    "        \n",
    "    return test_pred_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running various scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  643\n",
      "Vocabulary Size: 8522\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0515_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0515_UTC\n",
      "2017-08-22T05:15:16.328927: step 1, loss 3.67979, acc 0.04\n",
      "current_step:  1\n",
      "2017-08-22T05:15:16.657553: step 2, loss 2.43752, acc 0.04\n",
      "current_step:  2\n",
      "2017-08-22T05:15:16.981103: step 3, loss 1.47213, acc 0.38\n",
      "current_step:  3\n",
      "2017-08-22T05:15:17.305200: step 4, loss 1.65705, acc 0.5\n",
      "current_step:  4\n",
      "2017-08-22T05:15:17.619444: step 5, loss 1.96576, acc 0.32\n",
      "current_step:  5\n",
      "2017-08-22T05:15:18.135756: step 6, loss 1.24015, acc 0.62\n",
      "current_step:  6\n",
      "2017-08-22T05:15:18.661324: step 7, loss 1.60093, acc 0.42\n",
      "current_step:  7\n",
      "2017-08-22T05:15:19.182539: step 8, loss 1.34761, acc 0.4\n",
      "current_step:  8\n",
      "2017-08-22T05:15:19.707670: step 9, loss 1.66573, acc 0.44\n",
      "current_step:  9\n",
      "2017-08-22T05:15:20.230015: step 10, loss 1.55789, acc 0.46\n",
      "current_step:  10\n",
      "2017-08-22T05:15:20.750919: step 11, loss 1.60952, acc 0.4\n",
      "current_step:  11\n",
      "2017-08-22T05:15:21.273711: step 12, loss 1.28929, acc 0.58\n",
      "current_step:  12\n",
      "2017-08-22T05:15:21.799562: step 13, loss 1.3418, acc 0.52\n",
      "current_step:  13\n",
      "2017-08-22T05:15:22.319644: step 14, loss 1.28491, acc 0.52\n",
      "current_step:  14\n",
      "2017-08-22T05:15:22.843085: step 15, loss 1.28188, acc 0.52\n",
      "current_step:  15\n",
      "2017-08-22T05:15:23.360840: step 16, loss 1.48333, acc 0.56\n",
      "current_step:  16\n",
      "2017-08-22T05:15:23.883123: step 17, loss 1.44845, acc 0.46\n",
      "current_step:  17\n",
      "2017-08-22T05:15:24.406321: step 18, loss 1.0707, acc 0.52\n",
      "current_step:  18\n",
      "2017-08-22T05:15:24.935044: step 19, loss 1.392, acc 0.52\n",
      "current_step:  19\n",
      "2017-08-22T05:15:25.459732: step 20, loss 1.28207, acc 0.58\n",
      "current_step:  20\n",
      "2017-08-22T05:15:25.981023: step 21, loss 1.50713, acc 0.5\n",
      "current_step:  21\n",
      "2017-08-22T05:15:26.502098: step 22, loss 1.42273, acc 0.42\n",
      "current_step:  22\n",
      "2017-08-22T05:15:27.028251: step 23, loss 1.06635, acc 0.54\n",
      "current_step:  23\n",
      "2017-08-22T05:15:27.550823: step 24, loss 1.43887, acc 0.44\n",
      "current_step:  24\n",
      "2017-08-22T05:15:28.072073: step 25, loss 1.19546, acc 0.6\n",
      "current_step:  25\n",
      "2017-08-22T05:15:28.591143: step 26, loss 1.21417, acc 0.52\n",
      "current_step:  26\n",
      "2017-08-22T05:15:29.113859: step 27, loss 1.01707, acc 0.64\n",
      "current_step:  27\n",
      "2017-08-22T05:15:29.640689: step 28, loss 1.51595, acc 0.44\n",
      "current_step:  28\n",
      "2017-08-22T05:15:30.159594: step 29, loss 1.46602, acc 0.56\n",
      "current_step:  29\n",
      "2017-08-22T05:15:30.684684: step 30, loss 0.920285, acc 0.58\n",
      "current_step:  30\n",
      "2017-08-22T05:15:31.211056: step 31, loss 1.2345, acc 0.58\n",
      "current_step:  31\n",
      "2017-08-22T05:15:31.735685: step 32, loss 1.55903, acc 0.48\n",
      "current_step:  32\n",
      "2017-08-22T05:15:32.260158: step 33, loss 1.22303, acc 0.64\n",
      "current_step:  33\n",
      "2017-08-22T05:15:32.788415: step 34, loss 1.11163, acc 0.58\n",
      "current_step:  34\n",
      "2017-08-22T05:15:33.310301: step 35, loss 1.04273, acc 0.58\n",
      "current_step:  35\n",
      "2017-08-22T05:15:33.827495: step 36, loss 1.39019, acc 0.54\n",
      "current_step:  36\n",
      "2017-08-22T05:15:34.355692: step 37, loss 1.18715, acc 0.58\n",
      "current_step:  37\n",
      "2017-08-22T05:15:34.878077: step 38, loss 1.20231, acc 0.6\n",
      "current_step:  38\n",
      "2017-08-22T05:15:35.400377: step 39, loss 1.49124, acc 0.36\n",
      "current_step:  39\n",
      "2017-08-22T05:15:35.929420: step 40, loss 1.09218, acc 0.64\n",
      "current_step:  40\n",
      "2017-08-22T05:15:36.455067: step 41, loss 1.04413, acc 0.6\n",
      "current_step:  41\n",
      "2017-08-22T05:15:36.982213: step 42, loss 1.03578, acc 0.68\n",
      "current_step:  42\n",
      "2017-08-22T05:15:37.499807: step 43, loss 1.15809, acc 0.62\n",
      "current_step:  43\n",
      "2017-08-22T05:15:38.021085: step 44, loss 1.03482, acc 0.68\n",
      "current_step:  44\n",
      "2017-08-22T05:15:38.542647: step 45, loss 1.3357, acc 0.6\n",
      "current_step:  45\n",
      "2017-08-22T05:15:39.065567: step 46, loss 1.38323, acc 0.54\n",
      "current_step:  46\n",
      "2017-08-22T05:15:39.586045: step 47, loss 1.20275, acc 0.64\n",
      "current_step:  47\n",
      "2017-08-22T05:15:40.108500: step 48, loss 1.22837, acc 0.6\n",
      "current_step:  48\n",
      "2017-08-22T05:15:40.630289: step 49, loss 1.30259, acc 0.52\n",
      "current_step:  49\n",
      "2017-08-22T05:15:41.150849: step 50, loss 1.5116, acc 0.48\n",
      "current_step:  50\n",
      "2017-08-22T05:15:41.673585: step 51, loss 0.94775, acc 0.66\n",
      "current_step:  51\n",
      "2017-08-22T05:15:42.193495: step 52, loss 1.23112, acc 0.6\n",
      "current_step:  52\n",
      "2017-08-22T05:15:42.715362: step 53, loss 1.00321, acc 0.64\n",
      "current_step:  53\n",
      "2017-08-22T05:15:43.240846: step 54, loss 1.13948, acc 0.66\n",
      "current_step:  54\n",
      "2017-08-22T05:15:43.759622: step 55, loss 1.08648, acc 0.72\n",
      "current_step:  55\n",
      "2017-08-22T05:15:44.275149: step 56, loss 0.974921, acc 0.66\n",
      "current_step:  56\n",
      "2017-08-22T05:15:44.804803: step 57, loss 1.0584, acc 0.58\n",
      "current_step:  57\n",
      "2017-08-22T05:15:45.328921: step 58, loss 1.28755, acc 0.5\n",
      "current_step:  58\n",
      "2017-08-22T05:15:45.854681: step 59, loss 0.817109, acc 0.78\n",
      "current_step:  59\n",
      "2017-08-22T05:15:46.373406: step 60, loss 0.993839, acc 0.52\n",
      "current_step:  60\n",
      "2017-08-22T05:15:46.892997: step 61, loss 0.967325, acc 0.66\n",
      "current_step:  61\n",
      "2017-08-22T05:15:47.413091: step 62, loss 1.27453, acc 0.58\n",
      "current_step:  62\n",
      "2017-08-22T05:15:47.935649: step 63, loss 0.828679, acc 0.68\n",
      "current_step:  63\n",
      "2017-08-22T05:15:48.464343: step 64, loss 0.919443, acc 0.54\n",
      "current_step:  64\n",
      "2017-08-22T05:15:48.982232: step 65, loss 1.49661, acc 0.62\n",
      "current_step:  65\n",
      "2017-08-22T05:15:49.506979: step 66, loss 1.07155, acc 0.66\n",
      "current_step:  66\n",
      "2017-08-22T05:15:50.033244: step 67, loss 0.86741, acc 0.74\n",
      "current_step:  67\n",
      "2017-08-22T05:15:50.555399: step 68, loss 0.917886, acc 0.68\n",
      "current_step:  68\n",
      "2017-08-22T05:15:51.075135: step 69, loss 0.967188, acc 0.66\n",
      "current_step:  69\n",
      "2017-08-22T05:15:51.596157: step 70, loss 1.18994, acc 0.5\n",
      "current_step:  70\n",
      "2017-08-22T05:15:52.121430: step 71, loss 1.35836, acc 0.52\n",
      "current_step:  71\n",
      "2017-08-22T05:15:52.641265: step 72, loss 1.13967, acc 0.56\n",
      "current_step:  72\n",
      "2017-08-22T05:15:53.166709: step 73, loss 1.12427, acc 0.6\n",
      "current_step:  73\n",
      "2017-08-22T05:15:53.683457: step 74, loss 0.951466, acc 0.66\n",
      "current_step:  74\n",
      "2017-08-22T05:15:54.200786: step 75, loss 1.06887, acc 0.64\n",
      "current_step:  75\n",
      "2017-08-22T05:15:54.721502: step 76, loss 1.22951, acc 0.56\n",
      "current_step:  76\n",
      "2017-08-22T05:15:55.242739: step 77, loss 1.10022, acc 0.5\n",
      "current_step:  77\n",
      "2017-08-22T05:15:55.763844: step 78, loss 1.28627, acc 0.54\n",
      "current_step:  78\n",
      "2017-08-22T05:15:56.283649: step 79, loss 1.20707, acc 0.62\n",
      "current_step:  79\n",
      "2017-08-22T05:15:56.805227: step 80, loss 1.10151, acc 0.64\n",
      "current_step:  80\n",
      "2017-08-22T05:15:57.329891: step 81, loss 0.861418, acc 0.76\n",
      "current_step:  81\n",
      "2017-08-22T05:15:57.850666: step 82, loss 0.960665, acc 0.68\n",
      "current_step:  82\n",
      "2017-08-22T05:15:58.368733: step 83, loss 1.24698, acc 0.58\n",
      "current_step:  83\n",
      "2017-08-22T05:15:58.885433: step 84, loss 1.193, acc 0.52\n",
      "current_step:  84\n",
      "2017-08-22T05:15:59.405207: step 85, loss 1.04878, acc 0.66\n",
      "current_step:  85\n",
      "2017-08-22T05:15:59.931477: step 86, loss 0.956333, acc 0.66\n",
      "current_step:  86\n",
      "2017-08-22T05:16:00.460264: step 87, loss 1.06092, acc 0.68\n",
      "current_step:  87\n",
      "2017-08-22T05:16:00.985640: step 88, loss 1.12798, acc 0.62\n",
      "current_step:  88\n",
      "2017-08-22T05:16:01.512793: step 89, loss 0.827349, acc 0.8\n",
      "current_step:  89\n",
      "2017-08-22T05:16:02.041158: step 90, loss 1.16088, acc 0.6\n",
      "current_step:  90\n",
      "2017-08-22T05:16:02.562724: step 91, loss 1.01555, acc 0.6\n",
      "current_step:  91\n",
      "2017-08-22T05:16:03.087512: step 92, loss 1.12753, acc 0.58\n",
      "current_step:  92\n",
      "2017-08-22T05:16:03.609904: step 93, loss 1.09294, acc 0.58\n",
      "current_step:  93\n",
      "2017-08-22T05:16:04.128675: step 94, loss 1.25089, acc 0.54\n",
      "current_step:  94\n",
      "2017-08-22T05:16:04.649778: step 95, loss 0.902373, acc 0.72\n",
      "current_step:  95\n",
      "2017-08-22T05:16:05.172470: step 96, loss 1.28569, acc 0.6\n",
      "current_step:  96\n",
      "2017-08-22T05:16:05.698138: step 97, loss 1.03747, acc 0.54\n",
      "current_step:  97\n",
      "2017-08-22T05:16:06.221511: step 98, loss 0.853757, acc 0.68\n",
      "current_step:  98\n",
      "2017-08-22T05:16:06.743426: step 99, loss 1.47808, acc 0.56\n",
      "current_step:  99\n",
      "2017-08-22T05:16:07.269233: step 100, loss 1.25982, acc 0.6\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:16:09.237589: step 100, loss 0.965856, acc 0.69395\n",
      "\n",
      "2017-08-22T05:16:09.762876: step 101, loss 1.01065, acc 0.66\n",
      "current_step:  101\n",
      "2017-08-22T05:16:09.861076: step 102, loss 1.45725, acc 0.555556\n",
      "current_step:  102\n",
      "2017-08-22T05:16:10.380757: step 103, loss 0.956699, acc 0.66\n",
      "current_step:  103\n",
      "2017-08-22T05:16:10.903087: step 104, loss 0.740983, acc 0.78\n",
      "current_step:  104\n",
      "2017-08-22T05:16:11.424784: step 105, loss 0.784029, acc 0.74\n",
      "current_step:  105\n",
      "2017-08-22T05:16:11.947461: step 106, loss 0.912718, acc 0.72\n",
      "current_step:  106\n",
      "2017-08-22T05:16:12.471562: step 107, loss 0.649725, acc 0.86\n",
      "current_step:  107\n",
      "2017-08-22T05:16:12.992952: step 108, loss 0.67502, acc 0.86\n",
      "current_step:  108\n",
      "2017-08-22T05:16:13.508440: step 109, loss 0.733867, acc 0.78\n",
      "current_step:  109\n",
      "2017-08-22T05:16:14.030745: step 110, loss 0.775265, acc 0.72\n",
      "current_step:  110\n",
      "2017-08-22T05:16:14.548218: step 111, loss 0.814046, acc 0.76\n",
      "current_step:  111\n",
      "2017-08-22T05:16:15.075921: step 112, loss 0.746245, acc 0.68\n",
      "current_step:  112\n",
      "2017-08-22T05:16:15.603477: step 113, loss 1.11013, acc 0.54\n",
      "current_step:  113\n",
      "2017-08-22T05:16:16.128909: step 114, loss 0.595345, acc 0.84\n",
      "current_step:  114\n",
      "2017-08-22T05:16:16.647968: step 115, loss 0.971298, acc 0.7\n",
      "current_step:  115\n",
      "2017-08-22T05:16:17.167121: step 116, loss 0.764505, acc 0.74\n",
      "current_step:  116\n",
      "2017-08-22T05:16:17.683203: step 117, loss 0.753666, acc 0.8\n",
      "current_step:  117\n",
      "2017-08-22T05:16:18.204464: step 118, loss 0.988677, acc 0.64\n",
      "current_step:  118\n",
      "2017-08-22T05:16:18.725287: step 119, loss 0.882165, acc 0.68\n",
      "current_step:  119\n",
      "2017-08-22T05:16:19.243559: step 120, loss 0.902988, acc 0.72\n",
      "current_step:  120\n",
      "2017-08-22T05:16:19.763503: step 121, loss 0.714293, acc 0.78\n",
      "current_step:  121\n",
      "2017-08-22T05:16:20.288372: step 122, loss 0.744919, acc 0.74\n",
      "current_step:  122\n",
      "2017-08-22T05:16:20.813973: step 123, loss 0.8868, acc 0.7\n",
      "current_step:  123\n",
      "2017-08-22T05:16:21.336542: step 124, loss 0.645761, acc 0.76\n",
      "current_step:  124\n",
      "2017-08-22T05:16:21.857294: step 125, loss 0.867964, acc 0.78\n",
      "current_step:  125\n",
      "2017-08-22T05:16:22.372117: step 126, loss 0.687434, acc 0.74\n",
      "current_step:  126\n",
      "2017-08-22T05:16:22.893427: step 127, loss 0.753796, acc 0.74\n",
      "current_step:  127\n",
      "2017-08-22T05:16:23.416887: step 128, loss 0.775402, acc 0.78\n",
      "current_step:  128\n",
      "2017-08-22T05:16:23.939884: step 129, loss 0.626476, acc 0.72\n",
      "current_step:  129\n",
      "2017-08-22T05:16:24.464602: step 130, loss 0.70022, acc 0.76\n",
      "current_step:  130\n",
      "2017-08-22T05:16:24.991728: step 131, loss 0.784662, acc 0.74\n",
      "current_step:  131\n",
      "2017-08-22T05:16:25.510086: step 132, loss 0.779466, acc 0.74\n",
      "current_step:  132\n",
      "2017-08-22T05:16:26.042651: step 133, loss 0.72176, acc 0.74\n",
      "current_step:  133\n",
      "2017-08-22T05:16:26.556935: step 134, loss 0.998224, acc 0.56\n",
      "current_step:  134\n",
      "2017-08-22T05:16:27.076036: step 135, loss 0.813879, acc 0.66\n",
      "current_step:  135\n",
      "2017-08-22T05:16:27.598293: step 136, loss 0.758351, acc 0.74\n",
      "current_step:  136\n",
      "2017-08-22T05:16:28.119418: step 137, loss 0.916009, acc 0.66\n",
      "current_step:  137\n",
      "2017-08-22T05:16:28.638712: step 138, loss 0.657429, acc 0.78\n",
      "current_step:  138\n",
      "2017-08-22T05:16:29.164132: step 139, loss 0.747436, acc 0.74\n",
      "current_step:  139\n",
      "2017-08-22T05:16:29.689389: step 140, loss 0.794597, acc 0.72\n",
      "current_step:  140\n",
      "2017-08-22T05:16:30.207030: step 141, loss 0.660203, acc 0.84\n",
      "current_step:  141\n",
      "2017-08-22T05:16:30.732263: step 142, loss 0.860666, acc 0.7\n",
      "current_step:  142\n",
      "2017-08-22T05:16:31.257287: step 143, loss 0.61486, acc 0.72\n",
      "current_step:  143\n",
      "2017-08-22T05:16:31.794624: step 144, loss 0.848014, acc 0.7\n",
      "current_step:  144\n",
      "2017-08-22T05:16:32.312185: step 145, loss 0.892798, acc 0.74\n",
      "current_step:  145\n",
      "2017-08-22T05:16:32.833034: step 146, loss 0.613145, acc 0.84\n",
      "current_step:  146\n",
      "2017-08-22T05:16:33.360942: step 147, loss 0.809898, acc 0.74\n",
      "current_step:  147\n",
      "2017-08-22T05:16:33.885386: step 148, loss 0.771016, acc 0.74\n",
      "current_step:  148\n",
      "2017-08-22T05:16:34.409678: step 149, loss 0.727302, acc 0.76\n",
      "current_step:  149\n",
      "2017-08-22T05:16:34.931940: step 150, loss 0.664882, acc 0.86\n",
      "current_step:  150\n",
      "2017-08-22T05:16:35.452549: step 151, loss 0.94535, acc 0.68\n",
      "current_step:  151\n",
      "2017-08-22T05:16:35.975940: step 152, loss 0.564237, acc 0.8\n",
      "current_step:  152\n",
      "2017-08-22T05:16:36.496746: step 153, loss 0.812555, acc 0.74\n",
      "current_step:  153\n",
      "2017-08-22T05:16:37.021514: step 154, loss 0.712039, acc 0.74\n",
      "current_step:  154\n",
      "2017-08-22T05:16:37.549688: step 155, loss 0.661621, acc 0.8\n",
      "current_step:  155\n",
      "2017-08-22T05:16:38.074383: step 156, loss 0.68173, acc 0.76\n",
      "current_step:  156\n",
      "2017-08-22T05:16:38.594259: step 157, loss 0.691883, acc 0.8\n",
      "current_step:  157\n",
      "2017-08-22T05:16:39.115552: step 158, loss 0.969821, acc 0.7\n",
      "current_step:  158\n",
      "2017-08-22T05:16:39.640022: step 159, loss 0.835365, acc 0.68\n",
      "current_step:  159\n",
      "2017-08-22T05:16:40.162117: step 160, loss 0.976497, acc 0.64\n",
      "current_step:  160\n",
      "2017-08-22T05:16:40.688746: step 161, loss 0.94148, acc 0.62\n",
      "current_step:  161\n",
      "2017-08-22T05:16:41.213354: step 162, loss 1.0076, acc 0.68\n",
      "current_step:  162\n",
      "2017-08-22T05:16:41.740218: step 163, loss 0.664869, acc 0.8\n",
      "current_step:  163\n",
      "2017-08-22T05:16:42.261162: step 164, loss 0.702922, acc 0.8\n",
      "current_step:  164\n",
      "2017-08-22T05:16:42.788642: step 165, loss 0.790894, acc 0.68\n",
      "current_step:  165\n",
      "2017-08-22T05:16:43.310330: step 166, loss 0.73332, acc 0.8\n",
      "current_step:  166\n",
      "2017-08-22T05:16:43.830013: step 167, loss 0.910965, acc 0.72\n",
      "current_step:  167\n",
      "2017-08-22T05:16:44.354419: step 168, loss 0.669737, acc 0.86\n",
      "current_step:  168\n",
      "2017-08-22T05:16:44.881992: step 169, loss 0.860549, acc 0.68\n",
      "current_step:  169\n",
      "2017-08-22T05:16:45.403680: step 170, loss 0.689022, acc 0.84\n",
      "current_step:  170\n",
      "2017-08-22T05:16:45.922967: step 171, loss 0.86417, acc 0.7\n",
      "current_step:  171\n",
      "2017-08-22T05:16:46.446210: step 172, loss 0.981678, acc 0.66\n",
      "current_step:  172\n",
      "2017-08-22T05:16:46.962574: step 173, loss 0.724533, acc 0.74\n",
      "current_step:  173\n",
      "2017-08-22T05:16:47.482460: step 174, loss 0.604329, acc 0.82\n",
      "current_step:  174\n",
      "2017-08-22T05:16:48.007884: step 175, loss 0.79605, acc 0.7\n",
      "current_step:  175\n",
      "2017-08-22T05:16:48.527004: step 176, loss 0.764225, acc 0.72\n",
      "current_step:  176\n",
      "2017-08-22T05:16:49.052452: step 177, loss 0.720247, acc 0.76\n",
      "current_step:  177\n",
      "2017-08-22T05:16:49.578222: step 178, loss 0.659012, acc 0.76\n",
      "current_step:  178\n",
      "2017-08-22T05:16:50.102030: step 179, loss 0.710472, acc 0.82\n",
      "current_step:  179\n",
      "2017-08-22T05:16:50.625483: step 180, loss 1.02881, acc 0.64\n",
      "current_step:  180\n",
      "2017-08-22T05:16:51.144866: step 181, loss 0.510384, acc 0.84\n",
      "current_step:  181\n",
      "2017-08-22T05:16:51.664270: step 182, loss 0.890319, acc 0.74\n",
      "current_step:  182\n",
      "2017-08-22T05:16:52.187164: step 183, loss 0.944557, acc 0.72\n",
      "current_step:  183\n",
      "2017-08-22T05:16:52.707869: step 184, loss 0.631139, acc 0.76\n",
      "current_step:  184\n",
      "2017-08-22T05:16:53.232068: step 185, loss 0.700437, acc 0.74\n",
      "current_step:  185\n",
      "2017-08-22T05:16:53.750725: step 186, loss 0.70646, acc 0.72\n",
      "current_step:  186\n",
      "2017-08-22T05:16:54.274447: step 187, loss 0.859685, acc 0.7\n",
      "current_step:  187\n",
      "2017-08-22T05:16:54.805277: step 188, loss 0.689457, acc 0.76\n",
      "current_step:  188\n",
      "2017-08-22T05:16:55.332988: step 189, loss 0.937772, acc 0.6\n",
      "current_step:  189\n",
      "2017-08-22T05:16:55.862932: step 190, loss 0.669708, acc 0.8\n",
      "current_step:  190\n",
      "2017-08-22T05:16:56.391943: step 191, loss 0.718418, acc 0.72\n",
      "current_step:  191\n",
      "2017-08-22T05:16:56.925780: step 192, loss 0.502469, acc 0.86\n",
      "current_step:  192\n",
      "2017-08-22T05:16:57.462460: step 193, loss 0.890308, acc 0.64\n",
      "current_step:  193\n",
      "2017-08-22T05:16:57.991948: step 194, loss 0.860219, acc 0.68\n",
      "current_step:  194\n",
      "2017-08-22T05:16:58.521477: step 195, loss 0.824225, acc 0.7\n",
      "current_step:  195\n",
      "2017-08-22T05:16:59.053401: step 196, loss 0.849989, acc 0.74\n",
      "current_step:  196\n",
      "2017-08-22T05:16:59.589051: step 197, loss 0.756242, acc 0.72\n",
      "current_step:  197\n",
      "2017-08-22T05:17:00.122190: step 198, loss 0.91759, acc 0.68\n",
      "current_step:  198\n",
      "2017-08-22T05:17:00.649465: step 199, loss 0.982349, acc 0.62\n",
      "current_step:  199\n",
      "2017-08-22T05:17:01.184178: step 200, loss 0.877942, acc 0.72\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:17:03.133179: step 200, loss 0.886544, acc 0.701068\n",
      "\n",
      "2017-08-22T05:17:03.664253: step 201, loss 0.922834, acc 0.7\n",
      "current_step:  201\n",
      "2017-08-22T05:17:04.198086: step 202, loss 0.69262, acc 0.8\n",
      "current_step:  202\n",
      "2017-08-22T05:17:04.727731: step 203, loss 0.750648, acc 0.7\n",
      "current_step:  203\n",
      "2017-08-22T05:17:04.828050: step 204, loss 0.592251, acc 0.777778\n",
      "current_step:  204\n",
      "2017-08-22T05:17:05.352047: step 205, loss 0.558853, acc 0.84\n",
      "current_step:  205\n",
      "2017-08-22T05:17:05.874828: step 206, loss 0.495759, acc 0.82\n",
      "current_step:  206\n",
      "2017-08-22T05:17:06.397539: step 207, loss 0.661279, acc 0.8\n",
      "current_step:  207\n",
      "2017-08-22T05:17:06.922421: step 208, loss 0.480709, acc 0.84\n",
      "current_step:  208\n",
      "2017-08-22T05:17:07.444225: step 209, loss 0.649908, acc 0.82\n",
      "current_step:  209\n",
      "2017-08-22T05:17:07.966380: step 210, loss 0.689644, acc 0.76\n",
      "current_step:  210\n",
      "2017-08-22T05:17:08.491386: step 211, loss 0.53314, acc 0.88\n",
      "current_step:  211\n",
      "2017-08-22T05:17:09.017193: step 212, loss 0.644782, acc 0.82\n",
      "current_step:  212\n",
      "2017-08-22T05:17:09.536383: step 213, loss 0.462702, acc 0.92\n",
      "current_step:  213\n",
      "2017-08-22T05:17:10.054712: step 214, loss 0.583705, acc 0.84\n",
      "current_step:  214\n",
      "2017-08-22T05:17:10.576664: step 215, loss 0.507791, acc 0.8\n",
      "current_step:  215\n",
      "2017-08-22T05:17:11.103911: step 216, loss 0.642984, acc 0.82\n",
      "current_step:  216\n",
      "2017-08-22T05:17:11.627819: step 217, loss 0.448678, acc 0.88\n",
      "current_step:  217\n",
      "2017-08-22T05:17:12.147410: step 218, loss 0.464749, acc 0.88\n",
      "current_step:  218\n",
      "2017-08-22T05:17:12.669041: step 219, loss 0.627044, acc 0.78\n",
      "current_step:  219\n",
      "2017-08-22T05:17:13.192332: step 220, loss 0.60477, acc 0.78\n",
      "current_step:  220\n",
      "2017-08-22T05:17:13.718749: step 221, loss 0.496478, acc 0.82\n",
      "current_step:  221\n",
      "2017-08-22T05:17:14.240911: step 222, loss 0.472608, acc 0.86\n",
      "current_step:  222\n",
      "2017-08-22T05:17:14.757239: step 223, loss 0.598952, acc 0.8\n",
      "current_step:  223\n",
      "2017-08-22T05:17:15.276931: step 224, loss 0.782585, acc 0.74\n",
      "current_step:  224\n",
      "2017-08-22T05:17:15.806207: step 225, loss 0.68008, acc 0.8\n",
      "current_step:  225\n",
      "2017-08-22T05:17:16.330253: step 226, loss 0.426844, acc 0.88\n",
      "current_step:  226\n",
      "2017-08-22T05:17:16.854493: step 227, loss 0.733516, acc 0.78\n",
      "current_step:  227\n",
      "2017-08-22T05:17:17.371582: step 228, loss 0.622657, acc 0.86\n",
      "current_step:  228\n",
      "2017-08-22T05:17:17.892704: step 229, loss 0.521407, acc 0.84\n",
      "current_step:  229\n",
      "2017-08-22T05:17:18.416423: step 230, loss 0.598938, acc 0.82\n",
      "current_step:  230\n",
      "2017-08-22T05:17:18.941397: step 231, loss 0.638994, acc 0.78\n",
      "current_step:  231\n",
      "2017-08-22T05:17:19.467765: step 232, loss 0.577373, acc 0.88\n",
      "current_step:  232\n",
      "2017-08-22T05:17:19.996841: step 233, loss 0.456899, acc 0.94\n",
      "current_step:  233\n",
      "2017-08-22T05:17:20.519471: step 234, loss 0.48527, acc 0.82\n",
      "current_step:  234\n",
      "2017-08-22T05:17:21.044231: step 235, loss 0.489302, acc 0.82\n",
      "current_step:  235\n",
      "2017-08-22T05:17:21.569703: step 236, loss 0.809334, acc 0.76\n",
      "current_step:  236\n",
      "2017-08-22T05:17:22.092950: step 237, loss 0.658828, acc 0.72\n",
      "current_step:  237\n",
      "2017-08-22T05:17:22.616832: step 238, loss 0.485989, acc 0.84\n",
      "current_step:  238\n",
      "2017-08-22T05:17:23.141978: step 239, loss 0.598303, acc 0.88\n",
      "current_step:  239\n",
      "2017-08-22T05:17:23.664836: step 240, loss 0.521125, acc 0.88\n",
      "current_step:  240\n",
      "2017-08-22T05:17:24.195241: step 241, loss 0.560561, acc 0.86\n",
      "current_step:  241\n",
      "2017-08-22T05:17:24.726865: step 242, loss 0.503396, acc 0.86\n",
      "current_step:  242\n",
      "2017-08-22T05:17:25.249154: step 243, loss 0.573486, acc 0.84\n",
      "current_step:  243\n",
      "2017-08-22T05:17:25.770000: step 244, loss 0.733997, acc 0.72\n",
      "current_step:  244\n",
      "2017-08-22T05:17:26.289951: step 245, loss 0.615109, acc 0.76\n",
      "current_step:  245\n",
      "2017-08-22T05:17:26.815404: step 246, loss 0.600575, acc 0.86\n",
      "current_step:  246\n",
      "2017-08-22T05:17:27.340803: step 247, loss 0.53167, acc 0.82\n",
      "current_step:  247\n",
      "2017-08-22T05:17:27.863513: step 248, loss 0.53379, acc 0.8\n",
      "current_step:  248\n",
      "2017-08-22T05:17:28.382205: step 249, loss 0.493046, acc 0.84\n",
      "current_step:  249\n",
      "2017-08-22T05:17:28.907844: step 250, loss 0.412238, acc 0.84\n",
      "current_step:  250\n",
      "2017-08-22T05:17:29.427091: step 251, loss 0.550893, acc 0.82\n",
      "current_step:  251\n",
      "2017-08-22T05:17:29.948570: step 252, loss 0.533228, acc 0.86\n",
      "current_step:  252\n",
      "2017-08-22T05:17:30.474403: step 253, loss 0.570482, acc 0.84\n",
      "current_step:  253\n",
      "2017-08-22T05:17:31.001659: step 254, loss 0.794005, acc 0.72\n",
      "current_step:  254\n",
      "2017-08-22T05:17:31.524665: step 255, loss 0.544211, acc 0.78\n",
      "current_step:  255\n",
      "2017-08-22T05:17:32.053176: step 256, loss 0.4717, acc 0.86\n",
      "current_step:  256\n",
      "2017-08-22T05:17:32.573258: step 257, loss 0.677681, acc 0.8\n",
      "current_step:  257\n",
      "2017-08-22T05:17:33.092512: step 258, loss 0.541952, acc 0.84\n",
      "current_step:  258\n",
      "2017-08-22T05:17:33.618383: step 259, loss 0.488554, acc 0.76\n",
      "current_step:  259\n",
      "2017-08-22T05:17:34.145480: step 260, loss 0.568271, acc 0.76\n",
      "current_step:  260\n",
      "2017-08-22T05:17:34.669522: step 261, loss 0.618698, acc 0.82\n",
      "current_step:  261\n",
      "2017-08-22T05:17:35.190281: step 262, loss 0.693024, acc 0.78\n",
      "current_step:  262\n",
      "2017-08-22T05:17:35.713213: step 263, loss 0.704685, acc 0.7\n",
      "current_step:  263\n",
      "2017-08-22T05:17:36.236689: step 264, loss 0.581841, acc 0.76\n",
      "current_step:  264\n",
      "2017-08-22T05:17:36.763205: step 265, loss 0.607439, acc 0.84\n",
      "current_step:  265\n",
      "2017-08-22T05:17:37.286875: step 266, loss 0.580682, acc 0.82\n",
      "current_step:  266\n",
      "2017-08-22T05:17:37.807761: step 267, loss 0.379146, acc 0.88\n",
      "current_step:  267\n",
      "2017-08-22T05:17:38.329440: step 268, loss 0.700686, acc 0.8\n",
      "current_step:  268\n",
      "2017-08-22T05:17:38.855128: step 269, loss 0.57581, acc 0.78\n",
      "current_step:  269\n",
      "2017-08-22T05:17:39.372068: step 270, loss 0.705353, acc 0.74\n",
      "current_step:  270\n",
      "2017-08-22T05:17:39.891789: step 271, loss 0.763837, acc 0.76\n",
      "current_step:  271\n",
      "2017-08-22T05:17:40.409775: step 272, loss 0.650829, acc 0.8\n",
      "current_step:  272\n",
      "2017-08-22T05:17:40.928060: step 273, loss 0.444578, acc 0.88\n",
      "current_step:  273\n",
      "2017-08-22T05:17:41.448171: step 274, loss 0.529058, acc 0.84\n",
      "current_step:  274\n",
      "2017-08-22T05:17:41.974630: step 275, loss 0.512122, acc 0.86\n",
      "current_step:  275\n",
      "2017-08-22T05:17:42.499450: step 276, loss 0.499904, acc 0.88\n",
      "current_step:  276\n",
      "2017-08-22T05:17:43.013737: step 277, loss 0.340739, acc 0.98\n",
      "current_step:  277\n",
      "2017-08-22T05:17:43.538223: step 278, loss 0.370287, acc 0.88\n",
      "current_step:  278\n",
      "2017-08-22T05:17:44.061556: step 279, loss 0.566581, acc 0.76\n",
      "current_step:  279\n",
      "2017-08-22T05:17:44.577683: step 280, loss 0.9281, acc 0.68\n",
      "current_step:  280\n",
      "2017-08-22T05:17:45.102691: step 281, loss 0.768391, acc 0.68\n",
      "current_step:  281\n",
      "2017-08-22T05:17:45.627489: step 282, loss 0.735573, acc 0.76\n",
      "current_step:  282\n",
      "2017-08-22T05:17:46.147002: step 283, loss 0.648613, acc 0.84\n",
      "current_step:  283\n",
      "2017-08-22T05:17:46.671893: step 284, loss 0.521467, acc 0.88\n",
      "current_step:  284\n",
      "2017-08-22T05:17:47.190999: step 285, loss 0.624177, acc 0.84\n",
      "current_step:  285\n",
      "2017-08-22T05:17:47.712808: step 286, loss 0.658302, acc 0.8\n",
      "current_step:  286\n",
      "2017-08-22T05:17:48.231916: step 287, loss 0.746537, acc 0.74\n",
      "current_step:  287\n",
      "2017-08-22T05:17:48.761147: step 288, loss 0.557678, acc 0.86\n",
      "current_step:  288\n",
      "2017-08-22T05:17:49.286227: step 289, loss 0.62242, acc 0.88\n",
      "current_step:  289\n",
      "2017-08-22T05:17:49.812348: step 290, loss 0.567761, acc 0.74\n",
      "current_step:  290\n",
      "2017-08-22T05:17:50.338235: step 291, loss 0.750882, acc 0.78\n",
      "current_step:  291\n",
      "2017-08-22T05:17:50.860100: step 292, loss 0.633925, acc 0.78\n",
      "current_step:  292\n",
      "2017-08-22T05:17:51.383174: step 293, loss 0.586972, acc 0.84\n",
      "current_step:  293\n",
      "2017-08-22T05:17:51.907687: step 294, loss 0.698977, acc 0.78\n",
      "current_step:  294\n",
      "2017-08-22T05:17:52.426384: step 295, loss 0.619953, acc 0.72\n",
      "current_step:  295\n",
      "2017-08-22T05:17:52.946613: step 296, loss 0.660543, acc 0.84\n",
      "current_step:  296\n",
      "2017-08-22T05:17:53.467289: step 297, loss 0.562865, acc 0.82\n",
      "current_step:  297\n",
      "2017-08-22T05:17:53.995668: step 298, loss 0.608074, acc 0.8\n",
      "current_step:  298\n",
      "2017-08-22T05:17:54.518716: step 299, loss 0.534834, acc 0.82\n",
      "current_step:  299\n",
      "2017-08-22T05:17:55.044313: step 300, loss 0.530953, acc 0.8\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:17:56.965356: step 300, loss 0.845797, acc 0.711744\n",
      "\n",
      "2017-08-22T05:17:57.488782: step 301, loss 0.656243, acc 0.7\n",
      "current_step:  301\n",
      "2017-08-22T05:17:58.016948: step 302, loss 0.601393, acc 0.86\n",
      "current_step:  302\n",
      "2017-08-22T05:17:58.540546: step 303, loss 0.492912, acc 0.86\n",
      "current_step:  303\n",
      "2017-08-22T05:17:59.065255: step 304, loss 0.613768, acc 0.8\n",
      "current_step:  304\n",
      "2017-08-22T05:17:59.592842: step 305, loss 0.51575, acc 0.84\n",
      "current_step:  305\n",
      "2017-08-22T05:17:59.691833: step 306, loss 0.723665, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-22T05:18:00.215812: step 307, loss 0.483358, acc 0.9\n",
      "current_step:  307\n",
      "2017-08-22T05:18:00.741009: step 308, loss 0.426111, acc 0.92\n",
      "current_step:  308\n",
      "2017-08-22T05:18:01.261790: step 309, loss 0.622115, acc 0.8\n",
      "current_step:  309\n",
      "2017-08-22T05:18:01.788011: step 310, loss 0.464604, acc 0.88\n",
      "current_step:  310\n",
      "2017-08-22T05:18:02.314463: step 311, loss 0.421817, acc 0.94\n",
      "current_step:  311\n",
      "2017-08-22T05:18:02.835435: step 312, loss 0.392021, acc 0.94\n",
      "current_step:  312\n",
      "2017-08-22T05:18:03.359003: step 313, loss 0.389428, acc 0.92\n",
      "current_step:  313\n",
      "2017-08-22T05:18:03.879458: step 314, loss 0.400334, acc 0.92\n",
      "current_step:  314\n",
      "2017-08-22T05:18:04.401029: step 315, loss 0.495951, acc 0.86\n",
      "current_step:  315\n",
      "2017-08-22T05:18:04.930134: step 316, loss 0.359047, acc 0.94\n",
      "current_step:  316\n",
      "2017-08-22T05:18:05.452838: step 317, loss 0.476563, acc 0.82\n",
      "current_step:  317\n",
      "2017-08-22T05:18:05.979689: step 318, loss 0.402435, acc 0.9\n",
      "current_step:  318\n",
      "2017-08-22T05:18:06.505982: step 319, loss 0.494164, acc 0.88\n",
      "current_step:  319\n",
      "2017-08-22T05:18:07.036757: step 320, loss 0.520057, acc 0.82\n",
      "current_step:  320\n",
      "2017-08-22T05:18:07.558390: step 321, loss 0.325903, acc 0.9\n",
      "current_step:  321\n",
      "2017-08-22T05:18:08.088188: step 322, loss 0.432075, acc 0.82\n",
      "current_step:  322\n",
      "2017-08-22T05:18:08.613065: step 323, loss 0.377883, acc 0.9\n",
      "current_step:  323\n",
      "2017-08-22T05:18:09.137155: step 324, loss 0.514502, acc 0.88\n",
      "current_step:  324\n",
      "2017-08-22T05:18:09.660928: step 325, loss 0.669752, acc 0.72\n",
      "current_step:  325\n",
      "2017-08-22T05:18:10.186255: step 326, loss 0.410245, acc 0.9\n",
      "current_step:  326\n",
      "2017-08-22T05:18:10.709630: step 327, loss 0.370318, acc 0.92\n",
      "current_step:  327\n",
      "2017-08-22T05:18:11.235463: step 328, loss 0.326166, acc 0.96\n",
      "current_step:  328\n",
      "2017-08-22T05:18:11.756495: step 329, loss 0.459642, acc 0.94\n",
      "current_step:  329\n",
      "2017-08-22T05:18:12.277460: step 330, loss 0.792367, acc 0.72\n",
      "current_step:  330\n",
      "2017-08-22T05:18:12.798769: step 331, loss 0.424471, acc 0.92\n",
      "current_step:  331\n",
      "2017-08-22T05:18:13.318185: step 332, loss 0.35697, acc 0.96\n",
      "current_step:  332\n",
      "2017-08-22T05:18:13.841200: step 333, loss 0.365595, acc 0.94\n",
      "current_step:  333\n",
      "2017-08-22T05:18:14.365819: step 334, loss 0.42894, acc 0.9\n",
      "current_step:  334\n",
      "2017-08-22T05:18:14.886622: step 335, loss 0.400842, acc 0.84\n",
      "current_step:  335\n",
      "2017-08-22T05:18:15.408496: step 336, loss 0.422525, acc 0.9\n",
      "current_step:  336\n",
      "2017-08-22T05:18:15.931518: step 337, loss 0.454832, acc 0.86\n",
      "current_step:  337\n",
      "2017-08-22T05:18:16.448151: step 338, loss 0.375417, acc 0.92\n",
      "current_step:  338\n",
      "2017-08-22T05:18:16.968214: step 339, loss 0.397268, acc 0.94\n",
      "current_step:  339\n",
      "2017-08-22T05:18:17.491921: step 340, loss 0.400427, acc 0.86\n",
      "current_step:  340\n",
      "2017-08-22T05:18:18.016436: step 341, loss 0.440884, acc 0.88\n",
      "current_step:  341\n",
      "2017-08-22T05:18:18.541633: step 342, loss 0.33358, acc 0.9\n",
      "current_step:  342\n",
      "2017-08-22T05:18:19.071319: step 343, loss 0.573559, acc 0.86\n",
      "current_step:  343\n",
      "2017-08-22T05:18:19.590215: step 344, loss 0.432212, acc 0.92\n",
      "current_step:  344\n",
      "2017-08-22T05:18:20.106276: step 345, loss 0.344031, acc 0.92\n",
      "current_step:  345\n",
      "2017-08-22T05:18:20.633326: step 346, loss 0.471979, acc 0.84\n",
      "current_step:  346\n",
      "2017-08-22T05:18:21.151140: step 347, loss 0.339416, acc 0.94\n",
      "current_step:  347\n",
      "2017-08-22T05:18:21.671896: step 348, loss 0.392559, acc 0.92\n",
      "current_step:  348\n",
      "2017-08-22T05:18:22.200201: step 349, loss 0.353256, acc 0.9\n",
      "current_step:  349\n",
      "2017-08-22T05:18:22.722697: step 350, loss 0.371591, acc 0.9\n",
      "current_step:  350\n",
      "2017-08-22T05:18:23.241947: step 351, loss 0.32909, acc 0.9\n",
      "current_step:  351\n",
      "2017-08-22T05:18:23.762733: step 352, loss 0.500281, acc 0.82\n",
      "current_step:  352\n",
      "2017-08-22T05:18:24.286079: step 353, loss 0.276019, acc 0.96\n",
      "current_step:  353\n",
      "2017-08-22T05:18:24.805011: step 354, loss 0.541127, acc 0.82\n",
      "current_step:  354\n",
      "2017-08-22T05:18:25.327998: step 355, loss 0.41077, acc 0.88\n",
      "current_step:  355\n",
      "2017-08-22T05:18:25.846821: step 356, loss 0.436677, acc 0.9\n",
      "current_step:  356\n",
      "2017-08-22T05:18:26.366493: step 357, loss 0.298037, acc 0.94\n",
      "current_step:  357\n",
      "2017-08-22T05:18:26.890352: step 358, loss 0.479205, acc 0.88\n",
      "current_step:  358\n",
      "2017-08-22T05:18:27.421839: step 359, loss 0.265591, acc 0.98\n",
      "current_step:  359\n",
      "2017-08-22T05:18:27.945517: step 360, loss 0.413862, acc 0.92\n",
      "current_step:  360\n",
      "2017-08-22T05:18:28.466053: step 361, loss 0.537154, acc 0.92\n",
      "current_step:  361\n",
      "2017-08-22T05:18:28.990151: step 362, loss 0.323466, acc 0.96\n",
      "current_step:  362\n",
      "2017-08-22T05:18:29.511442: step 363, loss 0.416882, acc 0.88\n",
      "current_step:  363\n",
      "2017-08-22T05:18:30.040951: step 364, loss 0.511566, acc 0.82\n",
      "current_step:  364\n",
      "2017-08-22T05:18:30.556233: step 365, loss 0.443192, acc 0.88\n",
      "current_step:  365\n",
      "2017-08-22T05:18:31.088454: step 366, loss 0.536115, acc 0.84\n",
      "current_step:  366\n",
      "2017-08-22T05:18:31.608120: step 367, loss 0.482363, acc 0.84\n",
      "current_step:  367\n",
      "2017-08-22T05:18:32.129331: step 368, loss 0.443932, acc 0.86\n",
      "current_step:  368\n",
      "2017-08-22T05:18:32.657716: step 369, loss 0.471504, acc 0.84\n",
      "current_step:  369\n",
      "2017-08-22T05:18:33.178224: step 370, loss 0.467425, acc 0.9\n",
      "current_step:  370\n",
      "2017-08-22T05:18:33.703671: step 371, loss 0.410561, acc 0.82\n",
      "current_step:  371\n",
      "2017-08-22T05:18:34.223036: step 372, loss 0.499298, acc 0.86\n",
      "current_step:  372\n",
      "2017-08-22T05:18:34.743943: step 373, loss 0.487165, acc 0.88\n",
      "current_step:  373\n",
      "2017-08-22T05:18:35.269792: step 374, loss 0.333013, acc 0.98\n",
      "current_step:  374\n",
      "2017-08-22T05:18:35.796430: step 375, loss 0.55061, acc 0.78\n",
      "current_step:  375\n",
      "2017-08-22T05:18:36.322705: step 376, loss 0.46623, acc 0.86\n",
      "current_step:  376\n",
      "2017-08-22T05:18:36.849647: step 377, loss 0.522885, acc 0.88\n",
      "current_step:  377\n",
      "2017-08-22T05:18:37.377006: step 378, loss 0.503664, acc 0.88\n",
      "current_step:  378\n",
      "2017-08-22T05:18:37.906931: step 379, loss 0.428304, acc 0.86\n",
      "current_step:  379\n",
      "2017-08-22T05:18:38.433662: step 380, loss 0.571531, acc 0.76\n",
      "current_step:  380\n",
      "2017-08-22T05:18:38.958893: step 381, loss 0.541835, acc 0.82\n",
      "current_step:  381\n",
      "2017-08-22T05:18:39.482693: step 382, loss 0.677621, acc 0.76\n",
      "current_step:  382\n",
      "2017-08-22T05:18:40.004472: step 383, loss 0.465227, acc 0.88\n",
      "current_step:  383\n",
      "2017-08-22T05:18:40.523012: step 384, loss 0.511912, acc 0.9\n",
      "current_step:  384\n",
      "2017-08-22T05:18:41.045078: step 385, loss 0.39514, acc 0.92\n",
      "current_step:  385\n",
      "2017-08-22T05:18:41.563479: step 386, loss 0.580031, acc 0.84\n",
      "current_step:  386\n",
      "2017-08-22T05:18:42.084274: step 387, loss 0.388705, acc 0.92\n",
      "current_step:  387\n",
      "2017-08-22T05:18:42.608241: step 388, loss 0.389465, acc 0.88\n",
      "current_step:  388\n",
      "2017-08-22T05:18:43.132262: step 389, loss 0.461153, acc 0.84\n",
      "current_step:  389\n",
      "2017-08-22T05:18:43.650122: step 390, loss 0.489495, acc 0.88\n",
      "current_step:  390\n",
      "2017-08-22T05:18:44.168306: step 391, loss 0.385877, acc 0.84\n",
      "current_step:  391\n",
      "2017-08-22T05:18:44.700851: step 392, loss 0.485178, acc 0.88\n",
      "current_step:  392\n",
      "2017-08-22T05:18:45.221202: step 393, loss 0.591099, acc 0.78\n",
      "current_step:  393\n",
      "2017-08-22T05:18:45.740053: step 394, loss 0.41503, acc 0.92\n",
      "current_step:  394\n",
      "2017-08-22T05:18:46.265819: step 395, loss 0.433017, acc 0.9\n",
      "current_step:  395\n",
      "2017-08-22T05:18:46.789990: step 396, loss 0.373714, acc 0.96\n",
      "current_step:  396\n",
      "2017-08-22T05:18:47.309186: step 397, loss 0.379815, acc 0.84\n",
      "current_step:  397\n",
      "2017-08-22T05:18:47.838785: step 398, loss 0.573587, acc 0.86\n",
      "current_step:  398\n",
      "2017-08-22T05:18:48.357384: step 399, loss 0.538837, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-22T05:18:48.879335: step 400, loss 0.428707, acc 0.9\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:18:50.795486: step 400, loss 0.844348, acc 0.720641\n",
      "\n",
      "2017-08-22T05:18:51.319432: step 401, loss 0.427175, acc 0.86\n",
      "current_step:  401\n",
      "2017-08-22T05:18:51.840138: step 402, loss 0.549355, acc 0.8\n",
      "current_step:  402\n",
      "2017-08-22T05:18:52.358951: step 403, loss 0.359436, acc 0.92\n",
      "current_step:  403\n",
      "2017-08-22T05:18:52.882293: step 404, loss 0.462467, acc 0.8\n",
      "current_step:  404\n",
      "2017-08-22T05:18:53.403594: step 405, loss 0.620674, acc 0.8\n",
      "current_step:  405\n",
      "2017-08-22T05:18:53.930047: step 406, loss 0.330202, acc 0.92\n",
      "current_step:  406\n",
      "2017-08-22T05:18:54.452945: step 407, loss 0.54957, acc 0.82\n",
      "current_step:  407\n",
      "2017-08-22T05:18:54.554380: step 408, loss 0.487493, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-22T05:18:55.078119: step 409, loss 0.364394, acc 0.92\n",
      "current_step:  409\n",
      "2017-08-22T05:18:55.600379: step 410, loss 0.323627, acc 0.96\n",
      "current_step:  410\n",
      "2017-08-22T05:18:56.126128: step 411, loss 0.422115, acc 0.88\n",
      "current_step:  411\n",
      "2017-08-22T05:18:56.650341: step 412, loss 0.442484, acc 0.84\n",
      "current_step:  412\n",
      "2017-08-22T05:18:57.175570: step 413, loss 0.338414, acc 0.96\n",
      "current_step:  413\n",
      "2017-08-22T05:18:57.702153: step 414, loss 0.293788, acc 0.98\n",
      "current_step:  414\n",
      "2017-08-22T05:18:58.224536: step 415, loss 0.34215, acc 0.94\n",
      "current_step:  415\n",
      "2017-08-22T05:18:58.746942: step 416, loss 0.346308, acc 0.9\n",
      "current_step:  416\n",
      "2017-08-22T05:18:59.269156: step 417, loss 0.304025, acc 0.96\n",
      "current_step:  417\n",
      "2017-08-22T05:18:59.793692: step 418, loss 0.263739, acc 0.96\n",
      "current_step:  418\n",
      "2017-08-22T05:19:00.317952: step 419, loss 0.328565, acc 0.92\n",
      "current_step:  419\n",
      "2017-08-22T05:19:00.843686: step 420, loss 0.306613, acc 0.92\n",
      "current_step:  420\n",
      "2017-08-22T05:19:01.367765: step 421, loss 0.301156, acc 0.92\n",
      "current_step:  421\n",
      "2017-08-22T05:19:01.892342: step 422, loss 0.265304, acc 0.98\n",
      "current_step:  422\n",
      "2017-08-22T05:19:02.421712: step 423, loss 0.322027, acc 0.92\n",
      "current_step:  423\n",
      "2017-08-22T05:19:02.944397: step 424, loss 0.348676, acc 0.88\n",
      "current_step:  424\n",
      "2017-08-22T05:19:03.473042: step 425, loss 0.354873, acc 0.88\n",
      "current_step:  425\n",
      "2017-08-22T05:19:03.996535: step 426, loss 0.415264, acc 0.9\n",
      "current_step:  426\n",
      "2017-08-22T05:19:04.522036: step 427, loss 0.427685, acc 0.84\n",
      "current_step:  427\n",
      "2017-08-22T05:19:05.048071: step 428, loss 0.295824, acc 0.94\n",
      "current_step:  428\n",
      "2017-08-22T05:19:05.572442: step 429, loss 0.400265, acc 0.9\n",
      "current_step:  429\n",
      "2017-08-22T05:19:06.125438: step 430, loss 0.455191, acc 0.88\n",
      "current_step:  430\n",
      "2017-08-22T05:19:06.651381: step 431, loss 0.308309, acc 0.96\n",
      "current_step:  431\n",
      "2017-08-22T05:19:07.168185: step 432, loss 0.40783, acc 0.92\n",
      "current_step:  432\n",
      "2017-08-22T05:19:07.700349: step 433, loss 0.324799, acc 0.94\n",
      "current_step:  433\n",
      "2017-08-22T05:19:08.225067: step 434, loss 0.301759, acc 0.92\n",
      "current_step:  434\n",
      "2017-08-22T05:19:08.746603: step 435, loss 0.451998, acc 0.88\n",
      "current_step:  435\n",
      "2017-08-22T05:19:09.270754: step 436, loss 0.300033, acc 0.98\n",
      "current_step:  436\n",
      "2017-08-22T05:19:09.792424: step 437, loss 0.303452, acc 0.94\n",
      "current_step:  437\n",
      "2017-08-22T05:19:10.312168: step 438, loss 0.300363, acc 0.96\n",
      "current_step:  438\n",
      "2017-08-22T05:19:10.833445: step 439, loss 0.304664, acc 0.9\n",
      "current_step:  439\n",
      "2017-08-22T05:19:11.359124: step 440, loss 0.407717, acc 0.88\n",
      "current_step:  440\n",
      "2017-08-22T05:19:11.883472: step 441, loss 0.322639, acc 0.92\n",
      "current_step:  441\n",
      "2017-08-22T05:19:12.406204: step 442, loss 0.349414, acc 0.94\n",
      "current_step:  442\n",
      "2017-08-22T05:19:12.932550: step 443, loss 0.27945, acc 0.92\n",
      "current_step:  443\n",
      "2017-08-22T05:19:13.459398: step 444, loss 0.31814, acc 0.92\n",
      "current_step:  444\n",
      "2017-08-22T05:19:13.990450: step 445, loss 0.406625, acc 0.88\n",
      "current_step:  445\n",
      "2017-08-22T05:19:14.514140: step 446, loss 0.344225, acc 0.9\n",
      "current_step:  446\n",
      "2017-08-22T05:19:15.037704: step 447, loss 0.347596, acc 0.88\n",
      "current_step:  447\n",
      "2017-08-22T05:19:15.561636: step 448, loss 0.413799, acc 0.84\n",
      "current_step:  448\n",
      "2017-08-22T05:19:16.084217: step 449, loss 0.254201, acc 0.96\n",
      "current_step:  449\n",
      "2017-08-22T05:19:16.606751: step 450, loss 0.241683, acc 0.94\n",
      "current_step:  450\n",
      "2017-08-22T05:19:17.126724: step 451, loss 0.399368, acc 0.86\n",
      "current_step:  451\n",
      "2017-08-22T05:19:17.649898: step 452, loss 0.366549, acc 0.86\n",
      "current_step:  452\n",
      "2017-08-22T05:19:18.163960: step 453, loss 0.368688, acc 0.92\n",
      "current_step:  453\n",
      "2017-08-22T05:19:18.679685: step 454, loss 0.346882, acc 0.94\n",
      "current_step:  454\n",
      "2017-08-22T05:19:19.208935: step 455, loss 0.278689, acc 0.94\n",
      "current_step:  455\n",
      "2017-08-22T05:19:19.730984: step 456, loss 0.264445, acc 0.94\n",
      "current_step:  456\n",
      "2017-08-22T05:19:20.252940: step 457, loss 0.442231, acc 0.92\n",
      "current_step:  457\n",
      "2017-08-22T05:19:20.779063: step 458, loss 0.366525, acc 0.9\n",
      "current_step:  458\n",
      "2017-08-22T05:19:21.306276: step 459, loss 0.30231, acc 0.96\n",
      "current_step:  459\n",
      "2017-08-22T05:19:21.827689: step 460, loss 0.268379, acc 1\n",
      "current_step:  460\n",
      "2017-08-22T05:19:22.352752: step 461, loss 0.282296, acc 0.92\n",
      "current_step:  461\n",
      "2017-08-22T05:19:22.880569: step 462, loss 0.251076, acc 0.96\n",
      "current_step:  462\n",
      "2017-08-22T05:19:23.402274: step 463, loss 0.378873, acc 0.9\n",
      "current_step:  463\n",
      "2017-08-22T05:19:23.927729: step 464, loss 0.305182, acc 0.96\n",
      "current_step:  464\n",
      "2017-08-22T05:19:24.456667: step 465, loss 0.427751, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-22T05:19:24.982680: step 466, loss 0.343164, acc 0.92\n",
      "current_step:  466\n",
      "2017-08-22T05:19:25.511962: step 467, loss 0.335504, acc 0.9\n",
      "current_step:  467\n",
      "2017-08-22T05:19:26.042007: step 468, loss 0.343962, acc 0.92\n",
      "current_step:  468\n",
      "2017-08-22T05:19:26.564648: step 469, loss 0.253916, acc 0.94\n",
      "current_step:  469\n",
      "2017-08-22T05:19:27.084849: step 470, loss 0.435031, acc 0.84\n",
      "current_step:  470\n",
      "2017-08-22T05:19:27.609941: step 471, loss 0.314266, acc 0.9\n",
      "current_step:  471\n",
      "2017-08-22T05:19:28.134459: step 472, loss 0.391033, acc 0.86\n",
      "current_step:  472\n",
      "2017-08-22T05:19:28.665015: step 473, loss 0.214651, acc 0.98\n",
      "current_step:  473\n",
      "2017-08-22T05:19:29.196762: step 474, loss 0.322648, acc 0.9\n",
      "current_step:  474\n",
      "2017-08-22T05:19:29.726760: step 475, loss 0.431552, acc 0.88\n",
      "current_step:  475\n",
      "2017-08-22T05:19:30.246942: step 476, loss 0.338732, acc 0.9\n",
      "current_step:  476\n",
      "2017-08-22T05:19:30.768506: step 477, loss 0.387163, acc 0.9\n",
      "current_step:  477\n",
      "2017-08-22T05:19:31.296930: step 478, loss 0.423316, acc 0.88\n",
      "current_step:  478\n",
      "2017-08-22T05:19:31.817313: step 479, loss 0.265904, acc 0.96\n",
      "current_step:  479\n",
      "2017-08-22T05:19:32.341358: step 480, loss 0.306722, acc 0.94\n",
      "current_step:  480\n",
      "2017-08-22T05:19:32.865119: step 481, loss 0.335029, acc 0.94\n",
      "current_step:  481\n",
      "2017-08-22T05:19:33.394270: step 482, loss 0.401182, acc 0.86\n",
      "current_step:  482\n",
      "2017-08-22T05:19:33.914698: step 483, loss 0.260346, acc 0.98\n",
      "current_step:  483\n",
      "2017-08-22T05:19:34.433603: step 484, loss 0.299842, acc 0.94\n",
      "current_step:  484\n",
      "2017-08-22T05:19:34.956014: step 485, loss 0.265521, acc 0.96\n",
      "current_step:  485\n",
      "2017-08-22T05:19:35.476203: step 486, loss 0.544512, acc 0.78\n",
      "current_step:  486\n",
      "2017-08-22T05:19:36.005254: step 487, loss 0.413486, acc 0.92\n",
      "current_step:  487\n",
      "2017-08-22T05:19:36.528806: step 488, loss 0.299316, acc 0.94\n",
      "current_step:  488\n",
      "2017-08-22T05:19:37.055517: step 489, loss 0.356829, acc 0.9\n",
      "current_step:  489\n",
      "2017-08-22T05:19:37.580699: step 490, loss 0.324099, acc 0.92\n",
      "current_step:  490\n",
      "2017-08-22T05:19:38.104248: step 491, loss 0.405871, acc 0.88\n",
      "current_step:  491\n",
      "2017-08-22T05:19:38.630776: step 492, loss 0.397826, acc 0.9\n",
      "current_step:  492\n",
      "2017-08-22T05:19:39.152968: step 493, loss 0.248974, acc 0.96\n",
      "current_step:  493\n",
      "2017-08-22T05:19:39.678961: step 494, loss 0.306027, acc 0.94\n",
      "current_step:  494\n",
      "2017-08-22T05:19:40.195837: step 495, loss 0.344417, acc 0.9\n",
      "current_step:  495\n",
      "2017-08-22T05:19:40.720840: step 496, loss 0.411282, acc 0.92\n",
      "current_step:  496\n",
      "2017-08-22T05:19:41.243315: step 497, loss 0.47897, acc 0.84\n",
      "current_step:  497\n",
      "2017-08-22T05:19:41.757172: step 498, loss 0.432001, acc 0.92\n",
      "current_step:  498\n",
      "2017-08-22T05:19:42.279978: step 499, loss 0.475094, acc 0.88\n",
      "current_step:  499\n",
      "2017-08-22T05:19:42.798548: step 500, loss 0.451673, acc 0.84\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:19:44.716199: step 500, loss 0.822485, acc 0.729537\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0515_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:19:45.308553: step 501, loss 0.472064, acc 0.9\n",
      "current_step:  501\n",
      "2017-08-22T05:19:45.830686: step 502, loss 0.290308, acc 0.94\n",
      "current_step:  502\n",
      "2017-08-22T05:19:46.350468: step 503, loss 0.472773, acc 0.82\n",
      "current_step:  503\n",
      "2017-08-22T05:19:46.872575: step 504, loss 0.351932, acc 0.94\n",
      "current_step:  504\n",
      "2017-08-22T05:19:47.393299: step 505, loss 0.344564, acc 0.94\n",
      "current_step:  505\n",
      "2017-08-22T05:19:47.914447: step 506, loss 0.257322, acc 0.9\n",
      "current_step:  506\n",
      "2017-08-22T05:19:48.436624: step 507, loss 0.365922, acc 0.9\n",
      "current_step:  507\n",
      "2017-08-22T05:19:48.958697: step 508, loss 0.341132, acc 0.92\n",
      "current_step:  508\n",
      "2017-08-22T05:19:49.485635: step 509, loss 0.456376, acc 0.88\n",
      "current_step:  509\n",
      "2017-08-22T05:19:49.585683: step 510, loss 0.721917, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.630904\n",
      "Precision Score = 0.583786\n",
      "Recall Score = 0.692171\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.652809\n",
      "Precision Score = 0.658832\n",
      "Recall Score = 0.690391\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.671782\n",
      "Precision Score = 0.667583\n",
      "Recall Score = 0.702847\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.671778\n",
      "Precision Score = 0.676501\n",
      "Recall Score = 0.711744\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.689253\n",
      "Precision Score = 0.680363\n",
      "Recall Score = 0.720641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vslchu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vslchu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v1 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 1, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  411\n",
      "Vocabulary Size: 6361\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0520_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0520_UTC\n",
      "2017-08-22T05:20:13.514490: step 1, loss 3.15613, acc 0.1\n",
      "current_step:  1\n",
      "2017-08-22T05:20:13.711429: step 2, loss 3.20766, acc 0.18\n",
      "current_step:  2\n",
      "2017-08-22T05:20:13.903543: step 3, loss 2.27633, acc 0.32\n",
      "current_step:  3\n",
      "2017-08-22T05:20:14.096070: step 4, loss 1.7865, acc 0.24\n",
      "current_step:  4\n",
      "2017-08-22T05:20:14.289697: step 5, loss 1.50657, acc 0.46\n",
      "current_step:  5\n",
      "2017-08-22T05:20:14.506130: step 6, loss 1.26867, acc 0.5\n",
      "current_step:  6\n",
      "2017-08-22T05:20:14.705718: step 7, loss 1.54296, acc 0.52\n",
      "current_step:  7\n",
      "2017-08-22T05:20:14.899494: step 8, loss 1.33171, acc 0.56\n",
      "current_step:  8\n",
      "2017-08-22T05:20:15.093951: step 9, loss 1.26375, acc 0.62\n",
      "current_step:  9\n",
      "2017-08-22T05:20:15.291922: step 10, loss 1.41661, acc 0.54\n",
      "current_step:  10\n",
      "2017-08-22T05:20:15.491347: step 11, loss 1.30803, acc 0.5\n",
      "current_step:  11\n",
      "2017-08-22T05:20:15.720785: step 12, loss 1.3425, acc 0.56\n",
      "current_step:  12\n",
      "2017-08-22T05:20:16.053033: step 13, loss 1.79621, acc 0.48\n",
      "current_step:  13\n",
      "2017-08-22T05:20:16.382042: step 14, loss 1.22588, acc 0.56\n",
      "current_step:  14\n",
      "2017-08-22T05:20:16.715810: step 15, loss 1.61758, acc 0.46\n",
      "current_step:  15\n",
      "2017-08-22T05:20:17.049471: step 16, loss 1.12323, acc 0.56\n",
      "current_step:  16\n",
      "2017-08-22T05:20:17.382283: step 17, loss 1.39527, acc 0.46\n",
      "current_step:  17\n",
      "2017-08-22T05:20:17.714890: step 18, loss 1.50292, acc 0.44\n",
      "current_step:  18\n",
      "2017-08-22T05:20:18.046747: step 19, loss 1.26701, acc 0.46\n",
      "current_step:  19\n",
      "2017-08-22T05:20:18.379025: step 20, loss 1.26166, acc 0.5\n",
      "current_step:  20\n",
      "2017-08-22T05:20:18.713063: step 21, loss 1.80622, acc 0.48\n",
      "current_step:  21\n",
      "2017-08-22T05:20:19.046125: step 22, loss 1.02223, acc 0.62\n",
      "current_step:  22\n",
      "2017-08-22T05:20:19.378563: step 23, loss 1.22474, acc 0.58\n",
      "current_step:  23\n",
      "2017-08-22T05:20:19.714454: step 24, loss 1.12182, acc 0.64\n",
      "current_step:  24\n",
      "2017-08-22T05:20:20.049862: step 25, loss 1.29048, acc 0.56\n",
      "current_step:  25\n",
      "2017-08-22T05:20:20.380444: step 26, loss 1.18742, acc 0.58\n",
      "current_step:  26\n",
      "2017-08-22T05:20:20.716326: step 27, loss 1.55845, acc 0.54\n",
      "current_step:  27\n",
      "2017-08-22T05:20:21.048954: step 28, loss 1.11083, acc 0.56\n",
      "current_step:  28\n",
      "2017-08-22T05:20:21.382669: step 29, loss 1.34015, acc 0.58\n",
      "current_step:  29\n",
      "2017-08-22T05:20:21.717051: step 30, loss 1.28645, acc 0.58\n",
      "current_step:  30\n",
      "2017-08-22T05:20:22.055527: step 31, loss 1.03265, acc 0.64\n",
      "current_step:  31\n",
      "2017-08-22T05:20:22.389194: step 32, loss 1.22273, acc 0.5\n",
      "current_step:  32\n",
      "2017-08-22T05:20:22.722929: step 33, loss 1.02834, acc 0.62\n",
      "current_step:  33\n",
      "2017-08-22T05:20:23.056293: step 34, loss 1.50693, acc 0.48\n",
      "current_step:  34\n",
      "2017-08-22T05:20:23.388056: step 35, loss 1.53506, acc 0.46\n",
      "current_step:  35\n",
      "2017-08-22T05:20:23.721023: step 36, loss 0.964106, acc 0.66\n",
      "current_step:  36\n",
      "2017-08-22T05:20:24.055647: step 37, loss 0.964483, acc 0.66\n",
      "current_step:  37\n",
      "2017-08-22T05:20:24.390296: step 38, loss 1.24019, acc 0.6\n",
      "current_step:  38\n",
      "2017-08-22T05:20:24.722640: step 39, loss 0.960221, acc 0.68\n",
      "current_step:  39\n",
      "2017-08-22T05:20:25.053285: step 40, loss 1.54186, acc 0.58\n",
      "current_step:  40\n",
      "2017-08-22T05:20:25.385654: step 41, loss 1.071, acc 0.6\n",
      "current_step:  41\n",
      "2017-08-22T05:20:25.720140: step 42, loss 1.13386, acc 0.5\n",
      "current_step:  42\n",
      "2017-08-22T05:20:26.051114: step 43, loss 1.09376, acc 0.48\n",
      "current_step:  43\n",
      "2017-08-22T05:20:26.381258: step 44, loss 1.14369, acc 0.54\n",
      "current_step:  44\n",
      "2017-08-22T05:20:26.714272: step 45, loss 0.891434, acc 0.64\n",
      "current_step:  45\n",
      "2017-08-22T05:20:27.045898: step 46, loss 1.09838, acc 0.58\n",
      "current_step:  46\n",
      "2017-08-22T05:20:27.382111: step 47, loss 1.21223, acc 0.52\n",
      "current_step:  47\n",
      "2017-08-22T05:20:27.714301: step 48, loss 1.20708, acc 0.58\n",
      "current_step:  48\n",
      "2017-08-22T05:20:28.046746: step 49, loss 1.1861, acc 0.56\n",
      "current_step:  49\n",
      "2017-08-22T05:20:28.380440: step 50, loss 1.22717, acc 0.58\n",
      "current_step:  50\n",
      "2017-08-22T05:20:28.715938: step 51, loss 1.69795, acc 0.44\n",
      "current_step:  51\n",
      "2017-08-22T05:20:29.050373: step 52, loss 1.14441, acc 0.6\n",
      "current_step:  52\n",
      "2017-08-22T05:20:29.382636: step 53, loss 1.0507, acc 0.62\n",
      "current_step:  53\n",
      "2017-08-22T05:20:29.718930: step 54, loss 1.11044, acc 0.58\n",
      "current_step:  54\n",
      "2017-08-22T05:20:30.049241: step 55, loss 1.36904, acc 0.46\n",
      "current_step:  55\n",
      "2017-08-22T05:20:30.382401: step 56, loss 1.13013, acc 0.64\n",
      "current_step:  56\n",
      "2017-08-22T05:20:30.715934: step 57, loss 0.946254, acc 0.7\n",
      "current_step:  57\n",
      "2017-08-22T05:20:31.049753: step 58, loss 0.931719, acc 0.68\n",
      "current_step:  58\n",
      "2017-08-22T05:20:31.382690: step 59, loss 0.809391, acc 0.8\n",
      "current_step:  59\n",
      "2017-08-22T05:20:31.730516: step 60, loss 1.1381, acc 0.64\n",
      "current_step:  60\n",
      "2017-08-22T05:20:32.060495: step 61, loss 1.05996, acc 0.6\n",
      "current_step:  61\n",
      "2017-08-22T05:20:32.393235: step 62, loss 1.10476, acc 0.62\n",
      "current_step:  62\n",
      "2017-08-22T05:20:32.725129: step 63, loss 1.2571, acc 0.6\n",
      "current_step:  63\n",
      "2017-08-22T05:20:33.056741: step 64, loss 1.23083, acc 0.56\n",
      "current_step:  64\n",
      "2017-08-22T05:20:33.388555: step 65, loss 1.139, acc 0.56\n",
      "current_step:  65\n",
      "2017-08-22T05:20:33.720966: step 66, loss 0.94344, acc 0.66\n",
      "current_step:  66\n",
      "2017-08-22T05:20:34.051184: step 67, loss 1.14275, acc 0.56\n",
      "current_step:  67\n",
      "2017-08-22T05:20:34.380307: step 68, loss 1.27914, acc 0.56\n",
      "current_step:  68\n",
      "2017-08-22T05:20:34.713466: step 69, loss 1.40171, acc 0.5\n",
      "current_step:  69\n",
      "2017-08-22T05:20:35.044754: step 70, loss 1.12934, acc 0.66\n",
      "current_step:  70\n",
      "2017-08-22T05:20:35.376214: step 71, loss 1.14663, acc 0.54\n",
      "current_step:  71\n",
      "2017-08-22T05:20:35.707148: step 72, loss 1.00469, acc 0.66\n",
      "current_step:  72\n",
      "2017-08-22T05:20:36.039253: step 73, loss 0.98362, acc 0.68\n",
      "current_step:  73\n",
      "2017-08-22T05:20:36.370240: step 74, loss 0.922082, acc 0.62\n",
      "current_step:  74\n",
      "2017-08-22T05:20:36.703534: step 75, loss 1.16065, acc 0.54\n",
      "current_step:  75\n",
      "2017-08-22T05:20:37.035332: step 76, loss 0.976761, acc 0.6\n",
      "current_step:  76\n",
      "2017-08-22T05:20:37.365682: step 77, loss 1.06414, acc 0.58\n",
      "current_step:  77\n",
      "2017-08-22T05:20:37.697799: step 78, loss 0.960155, acc 0.68\n",
      "current_step:  78\n",
      "2017-08-22T05:20:38.031077: step 79, loss 1.12562, acc 0.56\n",
      "current_step:  79\n",
      "2017-08-22T05:20:38.362468: step 80, loss 1.15667, acc 0.66\n",
      "current_step:  80\n",
      "2017-08-22T05:20:38.693883: step 81, loss 0.997856, acc 0.66\n",
      "current_step:  81\n",
      "2017-08-22T05:20:39.024502: step 82, loss 1.07535, acc 0.58\n",
      "current_step:  82\n",
      "2017-08-22T05:20:39.355999: step 83, loss 1.02518, acc 0.58\n",
      "current_step:  83\n",
      "2017-08-22T05:20:39.689444: step 84, loss 1.09896, acc 0.7\n",
      "current_step:  84\n",
      "2017-08-22T05:20:40.020586: step 85, loss 1.10371, acc 0.62\n",
      "current_step:  85\n",
      "2017-08-22T05:20:40.352305: step 86, loss 1.16441, acc 0.52\n",
      "current_step:  86\n",
      "2017-08-22T05:20:40.682054: step 87, loss 1.19597, acc 0.54\n",
      "current_step:  87\n",
      "2017-08-22T05:20:41.013913: step 88, loss 0.88323, acc 0.7\n",
      "current_step:  88\n",
      "2017-08-22T05:20:41.348195: step 89, loss 1.14225, acc 0.64\n",
      "current_step:  89\n",
      "2017-08-22T05:20:41.679174: step 90, loss 0.894844, acc 0.76\n",
      "current_step:  90\n",
      "2017-08-22T05:20:42.011807: step 91, loss 1.17271, acc 0.68\n",
      "current_step:  91\n",
      "2017-08-22T05:20:42.342964: step 92, loss 0.77446, acc 0.76\n",
      "current_step:  92\n",
      "2017-08-22T05:20:42.673617: step 93, loss 0.975239, acc 0.66\n",
      "current_step:  93\n",
      "2017-08-22T05:20:43.007122: step 94, loss 0.868006, acc 0.66\n",
      "current_step:  94\n",
      "2017-08-22T05:20:43.336785: step 95, loss 1.05893, acc 0.6\n",
      "current_step:  95\n",
      "2017-08-22T05:20:43.668873: step 96, loss 0.99968, acc 0.62\n",
      "current_step:  96\n",
      "2017-08-22T05:20:44.001144: step 97, loss 1.13268, acc 0.6\n",
      "current_step:  97\n",
      "2017-08-22T05:20:44.331541: step 98, loss 1.23253, acc 0.54\n",
      "current_step:  98\n",
      "2017-08-22T05:20:44.664313: step 99, loss 1.04541, acc 0.66\n",
      "current_step:  99\n",
      "2017-08-22T05:20:44.997899: step 100, loss 1.136, acc 0.58\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:20:46.237977: step 100, loss 0.968429, acc 0.683274\n",
      "\n",
      "2017-08-22T05:20:46.569274: step 101, loss 1.06512, acc 0.62\n",
      "current_step:  101\n",
      "2017-08-22T05:20:46.633967: step 102, loss 1.16166, acc 0.666667\n",
      "current_step:  102\n",
      "2017-08-22T05:20:46.965419: step 103, loss 0.88103, acc 0.66\n",
      "current_step:  103\n",
      "2017-08-22T05:20:47.298391: step 104, loss 0.85805, acc 0.66\n",
      "current_step:  104\n",
      "2017-08-22T05:20:47.630517: step 105, loss 0.87681, acc 0.68\n",
      "current_step:  105\n",
      "2017-08-22T05:20:47.962551: step 106, loss 1.06723, acc 0.64\n",
      "current_step:  106\n",
      "2017-08-22T05:20:48.292423: step 107, loss 0.824815, acc 0.68\n",
      "current_step:  107\n",
      "2017-08-22T05:20:48.624197: step 108, loss 0.791884, acc 0.72\n",
      "current_step:  108\n",
      "2017-08-22T05:20:48.959929: step 109, loss 0.82692, acc 0.7\n",
      "current_step:  109\n",
      "2017-08-22T05:20:49.292415: step 110, loss 0.945812, acc 0.66\n",
      "current_step:  110\n",
      "2017-08-22T05:20:49.625465: step 111, loss 0.675844, acc 0.8\n",
      "current_step:  111\n",
      "2017-08-22T05:20:49.959192: step 112, loss 0.793444, acc 0.68\n",
      "current_step:  112\n",
      "2017-08-22T05:20:50.291580: step 113, loss 0.955872, acc 0.66\n",
      "current_step:  113\n",
      "2017-08-22T05:20:50.622356: step 114, loss 0.776627, acc 0.8\n",
      "current_step:  114\n",
      "2017-08-22T05:20:50.953283: step 115, loss 0.62854, acc 0.78\n",
      "current_step:  115\n",
      "2017-08-22T05:20:51.285679: step 116, loss 0.74982, acc 0.76\n",
      "current_step:  116\n",
      "2017-08-22T05:20:51.620691: step 117, loss 0.909459, acc 0.7\n",
      "current_step:  117\n",
      "2017-08-22T05:20:51.951116: step 118, loss 1.18194, acc 0.5\n",
      "current_step:  118\n",
      "2017-08-22T05:20:52.283161: step 119, loss 0.968881, acc 0.66\n",
      "current_step:  119\n",
      "2017-08-22T05:20:52.615179: step 120, loss 0.877132, acc 0.74\n",
      "current_step:  120\n",
      "2017-08-22T05:20:52.944483: step 121, loss 0.781497, acc 0.74\n",
      "current_step:  121\n",
      "2017-08-22T05:20:53.276011: step 122, loss 0.832563, acc 0.7\n",
      "current_step:  122\n",
      "2017-08-22T05:20:53.608078: step 123, loss 0.601136, acc 0.78\n",
      "current_step:  123\n",
      "2017-08-22T05:20:53.940687: step 124, loss 0.651366, acc 0.82\n",
      "current_step:  124\n",
      "2017-08-22T05:20:54.274983: step 125, loss 0.804622, acc 0.7\n",
      "current_step:  125\n",
      "2017-08-22T05:20:54.608078: step 126, loss 0.787289, acc 0.72\n",
      "current_step:  126\n",
      "2017-08-22T05:20:54.940055: step 127, loss 0.805912, acc 0.72\n",
      "current_step:  127\n",
      "2017-08-22T05:20:55.272513: step 128, loss 0.74645, acc 0.72\n",
      "current_step:  128\n",
      "2017-08-22T05:20:55.604734: step 129, loss 0.924568, acc 0.7\n",
      "current_step:  129\n",
      "2017-08-22T05:20:55.938215: step 130, loss 0.736533, acc 0.7\n",
      "current_step:  130\n",
      "2017-08-22T05:20:56.269086: step 131, loss 0.809965, acc 0.72\n",
      "current_step:  131\n",
      "2017-08-22T05:20:56.599715: step 132, loss 0.877914, acc 0.74\n",
      "current_step:  132\n",
      "2017-08-22T05:20:56.932306: step 133, loss 0.615238, acc 0.82\n",
      "current_step:  133\n",
      "2017-08-22T05:20:57.264805: step 134, loss 0.67093, acc 0.76\n",
      "current_step:  134\n",
      "2017-08-22T05:20:57.597700: step 135, loss 0.7843, acc 0.78\n",
      "current_step:  135\n",
      "2017-08-22T05:20:57.927323: step 136, loss 0.587008, acc 0.8\n",
      "current_step:  136\n",
      "2017-08-22T05:20:58.257835: step 137, loss 0.74921, acc 0.72\n",
      "current_step:  137\n",
      "2017-08-22T05:20:58.590792: step 138, loss 0.76553, acc 0.76\n",
      "current_step:  138\n",
      "2017-08-22T05:20:58.924520: step 139, loss 0.70297, acc 0.7\n",
      "current_step:  139\n",
      "2017-08-22T05:20:59.256119: step 140, loss 0.775022, acc 0.72\n",
      "current_step:  140\n",
      "2017-08-22T05:20:59.588648: step 141, loss 1.01854, acc 0.48\n",
      "current_step:  141\n",
      "2017-08-22T05:20:59.922556: step 142, loss 1.02411, acc 0.68\n",
      "current_step:  142\n",
      "2017-08-22T05:21:00.256025: step 143, loss 0.725974, acc 0.82\n",
      "current_step:  143\n",
      "2017-08-22T05:21:00.588895: step 144, loss 0.875648, acc 0.68\n",
      "current_step:  144\n",
      "2017-08-22T05:21:00.923259: step 145, loss 0.816123, acc 0.72\n",
      "current_step:  145\n",
      "2017-08-22T05:21:01.256304: step 146, loss 0.966154, acc 0.66\n",
      "current_step:  146\n",
      "2017-08-22T05:21:01.589127: step 147, loss 0.893826, acc 0.7\n",
      "current_step:  147\n",
      "2017-08-22T05:21:01.923931: step 148, loss 1.05485, acc 0.56\n",
      "current_step:  148\n",
      "2017-08-22T05:21:02.258107: step 149, loss 1.00235, acc 0.64\n",
      "current_step:  149\n",
      "2017-08-22T05:21:02.590057: step 150, loss 0.815942, acc 0.7\n",
      "current_step:  150\n",
      "2017-08-22T05:21:02.926186: step 151, loss 0.883553, acc 0.68\n",
      "current_step:  151\n",
      "2017-08-22T05:21:03.259929: step 152, loss 0.787715, acc 0.8\n",
      "current_step:  152\n",
      "2017-08-22T05:21:03.592493: step 153, loss 0.684783, acc 0.74\n",
      "current_step:  153\n",
      "2017-08-22T05:21:03.924165: step 154, loss 0.932749, acc 0.66\n",
      "current_step:  154\n",
      "2017-08-22T05:21:04.255754: step 155, loss 0.975697, acc 0.62\n",
      "current_step:  155\n",
      "2017-08-22T05:21:04.585922: step 156, loss 0.723291, acc 0.76\n",
      "current_step:  156\n",
      "2017-08-22T05:21:04.917984: step 157, loss 0.72243, acc 0.8\n",
      "current_step:  157\n",
      "2017-08-22T05:21:05.249704: step 158, loss 0.747259, acc 0.76\n",
      "current_step:  158\n",
      "2017-08-22T05:21:05.580983: step 159, loss 0.803113, acc 0.66\n",
      "current_step:  159\n",
      "2017-08-22T05:21:05.915322: step 160, loss 0.883496, acc 0.74\n",
      "current_step:  160\n",
      "2017-08-22T05:21:06.244943: step 161, loss 0.697118, acc 0.76\n",
      "current_step:  161\n",
      "2017-08-22T05:21:06.575659: step 162, loss 0.686331, acc 0.8\n",
      "current_step:  162\n",
      "2017-08-22T05:21:06.905514: step 163, loss 0.910005, acc 0.68\n",
      "current_step:  163\n",
      "2017-08-22T05:21:07.238283: step 164, loss 0.647435, acc 0.76\n",
      "current_step:  164\n",
      "2017-08-22T05:21:07.571272: step 165, loss 0.777474, acc 0.74\n",
      "current_step:  165\n",
      "2017-08-22T05:21:07.903697: step 166, loss 0.710055, acc 0.74\n",
      "current_step:  166\n",
      "2017-08-22T05:21:08.234061: step 167, loss 0.989523, acc 0.62\n",
      "current_step:  167\n",
      "2017-08-22T05:21:08.565309: step 168, loss 0.669933, acc 0.76\n",
      "current_step:  168\n",
      "2017-08-22T05:21:08.898754: step 169, loss 0.852776, acc 0.7\n",
      "current_step:  169\n",
      "2017-08-22T05:21:09.229371: step 170, loss 0.903294, acc 0.66\n",
      "current_step:  170\n",
      "2017-08-22T05:21:09.561090: step 171, loss 0.707706, acc 0.72\n",
      "current_step:  171\n",
      "2017-08-22T05:21:09.896561: step 172, loss 0.673875, acc 0.76\n",
      "current_step:  172\n",
      "2017-08-22T05:21:10.226951: step 173, loss 0.791216, acc 0.72\n",
      "current_step:  173\n",
      "2017-08-22T05:21:10.557222: step 174, loss 0.85188, acc 0.64\n",
      "current_step:  174\n",
      "2017-08-22T05:21:10.890072: step 175, loss 0.912371, acc 0.78\n",
      "current_step:  175\n",
      "2017-08-22T05:21:11.221453: step 176, loss 0.625023, acc 0.86\n",
      "current_step:  176\n",
      "2017-08-22T05:21:11.552391: step 177, loss 0.719616, acc 0.78\n",
      "current_step:  177\n",
      "2017-08-22T05:21:11.885837: step 178, loss 0.985451, acc 0.72\n",
      "current_step:  178\n",
      "2017-08-22T05:21:12.219936: step 179, loss 0.615411, acc 0.82\n",
      "current_step:  179\n",
      "2017-08-22T05:21:12.551380: step 180, loss 0.533724, acc 0.82\n",
      "current_step:  180\n",
      "2017-08-22T05:21:12.884160: step 181, loss 0.683258, acc 0.8\n",
      "current_step:  181\n",
      "2017-08-22T05:21:13.214796: step 182, loss 0.806267, acc 0.7\n",
      "current_step:  182\n",
      "2017-08-22T05:21:13.544472: step 183, loss 0.677899, acc 0.74\n",
      "current_step:  183\n",
      "2017-08-22T05:21:13.876595: step 184, loss 0.661884, acc 0.72\n",
      "current_step:  184\n",
      "2017-08-22T05:21:14.207921: step 185, loss 0.92014, acc 0.66\n",
      "current_step:  185\n",
      "2017-08-22T05:21:14.538809: step 186, loss 0.839303, acc 0.68\n",
      "current_step:  186\n",
      "2017-08-22T05:21:14.872662: step 187, loss 0.856763, acc 0.72\n",
      "current_step:  187\n",
      "2017-08-22T05:21:15.203410: step 188, loss 0.798228, acc 0.78\n",
      "current_step:  188\n",
      "2017-08-22T05:21:15.537191: step 189, loss 1.02971, acc 0.64\n",
      "current_step:  189\n",
      "2017-08-22T05:21:15.869053: step 190, loss 0.636653, acc 0.86\n",
      "current_step:  190\n",
      "2017-08-22T05:21:16.200863: step 191, loss 0.722748, acc 0.76\n",
      "current_step:  191\n",
      "2017-08-22T05:21:16.531401: step 192, loss 0.713748, acc 0.82\n",
      "current_step:  192\n",
      "2017-08-22T05:21:16.864146: step 193, loss 0.72916, acc 0.74\n",
      "current_step:  193\n",
      "2017-08-22T05:21:17.195482: step 194, loss 0.683843, acc 0.74\n",
      "current_step:  194\n",
      "2017-08-22T05:21:17.526189: step 195, loss 0.722969, acc 0.76\n",
      "current_step:  195\n",
      "2017-08-22T05:21:17.860935: step 196, loss 0.851026, acc 0.76\n",
      "current_step:  196\n",
      "2017-08-22T05:21:18.193694: step 197, loss 0.717208, acc 0.7\n",
      "current_step:  197\n",
      "2017-08-22T05:21:18.525186: step 198, loss 1.08805, acc 0.56\n",
      "current_step:  198\n",
      "2017-08-22T05:21:18.856610: step 199, loss 0.929682, acc 0.7\n",
      "current_step:  199\n",
      "2017-08-22T05:21:19.190477: step 200, loss 0.833211, acc 0.68\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:21:20.420219: step 200, loss 0.894123, acc 0.709964\n",
      "\n",
      "2017-08-22T05:21:20.754315: step 201, loss 0.760754, acc 0.7\n",
      "current_step:  201\n",
      "2017-08-22T05:21:21.087114: step 202, loss 0.746646, acc 0.74\n",
      "current_step:  202\n",
      "2017-08-22T05:21:21.415071: step 203, loss 1.01029, acc 0.56\n",
      "current_step:  203\n",
      "2017-08-22T05:21:21.480073: step 204, loss 1.07049, acc 0.666667\n",
      "current_step:  204\n",
      "2017-08-22T05:21:21.813077: step 205, loss 0.622514, acc 0.9\n",
      "current_step:  205\n",
      "2017-08-22T05:21:22.144256: step 206, loss 0.485718, acc 0.94\n",
      "current_step:  206\n",
      "2017-08-22T05:21:22.480743: step 207, loss 0.718926, acc 0.78\n",
      "current_step:  207\n",
      "2017-08-22T05:21:22.811422: step 208, loss 0.573205, acc 0.84\n",
      "current_step:  208\n",
      "2017-08-22T05:21:23.143365: step 209, loss 0.479831, acc 0.9\n",
      "current_step:  209\n",
      "2017-08-22T05:21:23.473653: step 210, loss 0.700859, acc 0.74\n",
      "current_step:  210\n",
      "2017-08-22T05:21:23.806327: step 211, loss 0.677512, acc 0.8\n",
      "current_step:  211\n",
      "2017-08-22T05:21:24.137831: step 212, loss 0.546757, acc 0.84\n",
      "current_step:  212\n",
      "2017-08-22T05:21:24.469713: step 213, loss 0.496548, acc 0.8\n",
      "current_step:  213\n",
      "2017-08-22T05:21:24.801793: step 214, loss 0.519496, acc 0.88\n",
      "current_step:  214\n",
      "2017-08-22T05:21:25.133781: step 215, loss 0.818751, acc 0.76\n",
      "current_step:  215\n",
      "2017-08-22T05:21:25.463169: step 216, loss 0.799783, acc 0.74\n",
      "current_step:  216\n",
      "2017-08-22T05:21:25.794685: step 217, loss 0.733213, acc 0.76\n",
      "current_step:  217\n",
      "2017-08-22T05:21:26.125773: step 218, loss 0.476836, acc 0.84\n",
      "current_step:  218\n",
      "2017-08-22T05:21:26.456183: step 219, loss 0.510772, acc 0.84\n",
      "current_step:  219\n",
      "2017-08-22T05:21:26.786985: step 220, loss 0.591769, acc 0.8\n",
      "current_step:  220\n",
      "2017-08-22T05:21:27.116981: step 221, loss 0.620554, acc 0.8\n",
      "current_step:  221\n",
      "2017-08-22T05:21:27.448321: step 222, loss 0.522057, acc 0.86\n",
      "current_step:  222\n",
      "2017-08-22T05:21:27.782781: step 223, loss 0.549213, acc 0.76\n",
      "current_step:  223\n",
      "2017-08-22T05:21:28.115148: step 224, loss 0.673183, acc 0.76\n",
      "current_step:  224\n",
      "2017-08-22T05:21:28.446699: step 225, loss 0.404555, acc 0.94\n",
      "current_step:  225\n",
      "2017-08-22T05:21:28.778570: step 226, loss 0.592222, acc 0.84\n",
      "current_step:  226\n",
      "2017-08-22T05:21:29.110330: step 227, loss 0.567845, acc 0.84\n",
      "current_step:  227\n",
      "2017-08-22T05:21:29.442255: step 228, loss 0.684351, acc 0.78\n",
      "current_step:  228\n",
      "2017-08-22T05:21:29.776341: step 229, loss 0.446645, acc 0.92\n",
      "current_step:  229\n",
      "2017-08-22T05:21:30.107921: step 230, loss 0.672702, acc 0.76\n",
      "current_step:  230\n",
      "2017-08-22T05:21:30.440901: step 231, loss 0.605477, acc 0.82\n",
      "current_step:  231\n",
      "2017-08-22T05:21:30.772985: step 232, loss 0.773234, acc 0.68\n",
      "current_step:  232\n",
      "2017-08-22T05:21:31.103869: step 233, loss 0.549338, acc 0.82\n",
      "current_step:  233\n",
      "2017-08-22T05:21:31.435086: step 234, loss 0.533684, acc 0.88\n",
      "current_step:  234\n",
      "2017-08-22T05:21:31.767052: step 235, loss 0.496437, acc 0.88\n",
      "current_step:  235\n",
      "2017-08-22T05:21:32.098054: step 236, loss 0.609256, acc 0.84\n",
      "current_step:  236\n",
      "2017-08-22T05:21:32.428685: step 237, loss 0.538612, acc 0.82\n",
      "current_step:  237\n",
      "2017-08-22T05:21:32.761312: step 238, loss 0.442422, acc 0.88\n",
      "current_step:  238\n",
      "2017-08-22T05:21:33.091058: step 239, loss 0.618469, acc 0.78\n",
      "current_step:  239\n",
      "2017-08-22T05:21:33.422056: step 240, loss 0.642, acc 0.76\n",
      "current_step:  240\n",
      "2017-08-22T05:21:33.755073: step 241, loss 0.45924, acc 0.88\n",
      "current_step:  241\n",
      "2017-08-22T05:21:34.086942: step 242, loss 0.689346, acc 0.72\n",
      "current_step:  242\n",
      "2017-08-22T05:21:34.418795: step 243, loss 0.528409, acc 0.8\n",
      "current_step:  243\n",
      "2017-08-22T05:21:34.750691: step 244, loss 0.505033, acc 0.88\n",
      "current_step:  244\n",
      "2017-08-22T05:21:35.081945: step 245, loss 0.635374, acc 0.78\n",
      "current_step:  245\n",
      "2017-08-22T05:21:35.411867: step 246, loss 0.448901, acc 0.82\n",
      "current_step:  246\n",
      "2017-08-22T05:21:35.742941: step 247, loss 0.756176, acc 0.68\n",
      "current_step:  247\n",
      "2017-08-22T05:21:36.074465: step 248, loss 0.677395, acc 0.7\n",
      "current_step:  248\n",
      "2017-08-22T05:21:36.405390: step 249, loss 0.633718, acc 0.8\n",
      "current_step:  249\n",
      "2017-08-22T05:21:36.736672: step 250, loss 0.732127, acc 0.72\n",
      "current_step:  250\n",
      "2017-08-22T05:21:37.067989: step 251, loss 0.592171, acc 0.78\n",
      "current_step:  251\n",
      "2017-08-22T05:21:37.400006: step 252, loss 0.777639, acc 0.7\n",
      "current_step:  252\n",
      "2017-08-22T05:21:37.732290: step 253, loss 0.821492, acc 0.68\n",
      "current_step:  253\n",
      "2017-08-22T05:21:38.064762: step 254, loss 0.721157, acc 0.74\n",
      "current_step:  254\n",
      "2017-08-22T05:21:38.394808: step 255, loss 0.552678, acc 0.84\n",
      "current_step:  255\n",
      "2017-08-22T05:21:38.725559: step 256, loss 0.49866, acc 0.86\n",
      "current_step:  256\n",
      "2017-08-22T05:21:39.059226: step 257, loss 0.776615, acc 0.74\n",
      "current_step:  257\n",
      "2017-08-22T05:21:39.392959: step 258, loss 0.693276, acc 0.72\n",
      "current_step:  258\n",
      "2017-08-22T05:21:39.726963: step 259, loss 0.697881, acc 0.82\n",
      "current_step:  259\n",
      "2017-08-22T05:21:40.057385: step 260, loss 0.812084, acc 0.68\n",
      "current_step:  260\n",
      "2017-08-22T05:21:40.387636: step 261, loss 0.671944, acc 0.74\n",
      "current_step:  261\n",
      "2017-08-22T05:21:40.719454: step 262, loss 0.517823, acc 0.82\n",
      "current_step:  262\n",
      "2017-08-22T05:21:41.049453: step 263, loss 0.524287, acc 0.8\n",
      "current_step:  263\n",
      "2017-08-22T05:21:41.408972: step 264, loss 0.43361, acc 0.9\n",
      "current_step:  264\n",
      "2017-08-22T05:21:41.740815: step 265, loss 0.651402, acc 0.78\n",
      "current_step:  265\n",
      "2017-08-22T05:21:42.069710: step 266, loss 0.64214, acc 0.78\n",
      "current_step:  266\n",
      "2017-08-22T05:21:42.400652: step 267, loss 0.630435, acc 0.86\n",
      "current_step:  267\n",
      "2017-08-22T05:21:42.733334: step 268, loss 0.623512, acc 0.78\n",
      "current_step:  268\n",
      "2017-08-22T05:21:43.063835: step 269, loss 0.646435, acc 0.74\n",
      "current_step:  269\n",
      "2017-08-22T05:21:43.392476: step 270, loss 0.553113, acc 0.82\n",
      "current_step:  270\n",
      "2017-08-22T05:21:43.721926: step 271, loss 0.607016, acc 0.76\n",
      "current_step:  271\n",
      "2017-08-22T05:21:44.054675: step 272, loss 0.643448, acc 0.76\n",
      "current_step:  272\n",
      "2017-08-22T05:21:44.383753: step 273, loss 0.514752, acc 0.88\n",
      "current_step:  273\n",
      "2017-08-22T05:21:44.717719: step 274, loss 0.708143, acc 0.82\n",
      "current_step:  274\n",
      "2017-08-22T05:21:45.049482: step 275, loss 0.607317, acc 0.8\n",
      "current_step:  275\n",
      "2017-08-22T05:21:45.379826: step 276, loss 0.705856, acc 0.78\n",
      "current_step:  276\n",
      "2017-08-22T05:21:45.715264: step 277, loss 0.632582, acc 0.84\n",
      "current_step:  277\n",
      "2017-08-22T05:21:46.046881: step 278, loss 0.532839, acc 0.86\n",
      "current_step:  278\n",
      "2017-08-22T05:21:46.376818: step 279, loss 0.638143, acc 0.72\n",
      "current_step:  279\n",
      "2017-08-22T05:21:46.707989: step 280, loss 0.708602, acc 0.78\n",
      "current_step:  280\n",
      "2017-08-22T05:21:47.040927: step 281, loss 0.734917, acc 0.84\n",
      "current_step:  281\n",
      "2017-08-22T05:21:47.372962: step 282, loss 0.67943, acc 0.8\n",
      "current_step:  282\n",
      "2017-08-22T05:21:47.704388: step 283, loss 0.449314, acc 0.86\n",
      "current_step:  283\n",
      "2017-08-22T05:21:48.035137: step 284, loss 0.623515, acc 0.86\n",
      "current_step:  284\n",
      "2017-08-22T05:21:48.365239: step 285, loss 0.636659, acc 0.8\n",
      "current_step:  285\n",
      "2017-08-22T05:21:48.700436: step 286, loss 0.606952, acc 0.86\n",
      "current_step:  286\n",
      "2017-08-22T05:21:49.032400: step 287, loss 0.946316, acc 0.64\n",
      "current_step:  287\n",
      "2017-08-22T05:21:49.364291: step 288, loss 0.635691, acc 0.76\n",
      "current_step:  288\n",
      "2017-08-22T05:21:49.698389: step 289, loss 0.607718, acc 0.82\n",
      "current_step:  289\n",
      "2017-08-22T05:21:50.030165: step 290, loss 0.50566, acc 0.86\n",
      "current_step:  290\n",
      "2017-08-22T05:21:50.361517: step 291, loss 0.622138, acc 0.76\n",
      "current_step:  291\n",
      "2017-08-22T05:21:50.693978: step 292, loss 0.723981, acc 0.8\n",
      "current_step:  292\n",
      "2017-08-22T05:21:51.027309: step 293, loss 0.535496, acc 0.84\n",
      "current_step:  293\n",
      "2017-08-22T05:21:51.358081: step 294, loss 0.624751, acc 0.82\n",
      "current_step:  294\n",
      "2017-08-22T05:21:51.688476: step 295, loss 0.751643, acc 0.72\n",
      "current_step:  295\n",
      "2017-08-22T05:21:52.020099: step 296, loss 0.685423, acc 0.72\n",
      "current_step:  296\n",
      "2017-08-22T05:21:52.351955: step 297, loss 0.680125, acc 0.8\n",
      "current_step:  297\n",
      "2017-08-22T05:21:52.683328: step 298, loss 0.510138, acc 0.84\n",
      "current_step:  298\n",
      "2017-08-22T05:21:53.015887: step 299, loss 0.725049, acc 0.7\n",
      "current_step:  299\n",
      "2017-08-22T05:21:53.347711: step 300, loss 0.504718, acc 0.86\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:21:54.577833: step 300, loss 0.898419, acc 0.718861\n",
      "\n",
      "2017-08-22T05:21:54.919506: step 301, loss 0.970131, acc 0.66\n",
      "current_step:  301\n",
      "2017-08-22T05:21:55.260538: step 302, loss 0.802458, acc 0.78\n",
      "current_step:  302\n",
      "2017-08-22T05:21:55.598535: step 303, loss 0.786209, acc 0.72\n",
      "current_step:  303\n",
      "2017-08-22T05:21:55.938429: step 304, loss 0.599989, acc 0.8\n",
      "current_step:  304\n",
      "2017-08-22T05:21:56.276925: step 305, loss 0.755831, acc 0.7\n",
      "current_step:  305\n",
      "2017-08-22T05:21:56.342386: step 306, loss 0.704196, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-22T05:21:56.684014: step 307, loss 0.367323, acc 0.88\n",
      "current_step:  307\n",
      "2017-08-22T05:21:57.025582: step 308, loss 0.413163, acc 0.92\n",
      "current_step:  308\n",
      "2017-08-22T05:21:57.365272: step 309, loss 0.448044, acc 0.88\n",
      "current_step:  309\n",
      "2017-08-22T05:21:57.706247: step 310, loss 0.642436, acc 0.76\n",
      "current_step:  310\n",
      "2017-08-22T05:21:58.046553: step 311, loss 0.540788, acc 0.88\n",
      "current_step:  311\n",
      "2017-08-22T05:21:58.386527: step 312, loss 0.442027, acc 0.92\n",
      "current_step:  312\n",
      "2017-08-22T05:21:58.729257: step 313, loss 0.473797, acc 0.88\n",
      "current_step:  313\n",
      "2017-08-22T05:21:59.086273: step 314, loss 0.424763, acc 0.86\n",
      "current_step:  314\n",
      "2017-08-22T05:21:59.426892: step 315, loss 0.575371, acc 0.84\n",
      "current_step:  315\n",
      "2017-08-22T05:21:59.769039: step 316, loss 0.500603, acc 0.88\n",
      "current_step:  316\n",
      "2017-08-22T05:22:00.108049: step 317, loss 0.44726, acc 0.86\n",
      "current_step:  317\n",
      "2017-08-22T05:22:00.447271: step 318, loss 0.561708, acc 0.8\n",
      "current_step:  318\n",
      "2017-08-22T05:22:00.788263: step 319, loss 0.435906, acc 0.84\n",
      "current_step:  319\n",
      "2017-08-22T05:22:01.127907: step 320, loss 0.424931, acc 0.84\n",
      "current_step:  320\n",
      "2017-08-22T05:22:01.468908: step 321, loss 0.478375, acc 0.86\n",
      "current_step:  321\n",
      "2017-08-22T05:22:01.807665: step 322, loss 0.412979, acc 0.88\n",
      "current_step:  322\n",
      "2017-08-22T05:22:02.147065: step 323, loss 0.417312, acc 0.86\n",
      "current_step:  323\n",
      "2017-08-22T05:22:02.487593: step 324, loss 0.440617, acc 0.82\n",
      "current_step:  324\n",
      "2017-08-22T05:22:02.825413: step 325, loss 0.453386, acc 0.86\n",
      "current_step:  325\n",
      "2017-08-22T05:22:03.165174: step 326, loss 0.480074, acc 0.84\n",
      "current_step:  326\n",
      "2017-08-22T05:22:03.503810: step 327, loss 0.609254, acc 0.82\n",
      "current_step:  327\n",
      "2017-08-22T05:22:03.844832: step 328, loss 0.57358, acc 0.84\n",
      "current_step:  328\n",
      "2017-08-22T05:22:04.184463: step 329, loss 0.448098, acc 0.86\n",
      "current_step:  329\n",
      "2017-08-22T05:22:04.522191: step 330, loss 0.484798, acc 0.88\n",
      "current_step:  330\n",
      "2017-08-22T05:22:04.853804: step 331, loss 0.515706, acc 0.82\n",
      "current_step:  331\n",
      "2017-08-22T05:22:05.184535: step 332, loss 0.405976, acc 0.9\n",
      "current_step:  332\n",
      "2017-08-22T05:22:05.516557: step 333, loss 0.509877, acc 0.86\n",
      "current_step:  333\n",
      "2017-08-22T05:22:05.851227: step 334, loss 0.478436, acc 0.86\n",
      "current_step:  334\n",
      "2017-08-22T05:22:06.182402: step 335, loss 0.548809, acc 0.84\n",
      "current_step:  335\n",
      "2017-08-22T05:22:06.513396: step 336, loss 0.603117, acc 0.8\n",
      "current_step:  336\n",
      "2017-08-22T05:22:06.846553: step 337, loss 0.329302, acc 0.96\n",
      "current_step:  337\n",
      "2017-08-22T05:22:07.176186: step 338, loss 0.557022, acc 0.82\n",
      "current_step:  338\n",
      "2017-08-22T05:22:07.509409: step 339, loss 0.330986, acc 0.98\n",
      "current_step:  339\n",
      "2017-08-22T05:22:07.841394: step 340, loss 0.661834, acc 0.8\n",
      "current_step:  340\n",
      "2017-08-22T05:22:08.172641: step 341, loss 0.508956, acc 0.88\n",
      "current_step:  341\n",
      "2017-08-22T05:22:08.502226: step 342, loss 0.511853, acc 0.8\n",
      "current_step:  342\n",
      "2017-08-22T05:22:08.834124: step 343, loss 0.432237, acc 0.9\n",
      "current_step:  343\n",
      "2017-08-22T05:22:09.164965: step 344, loss 0.453756, acc 0.84\n",
      "current_step:  344\n",
      "2017-08-22T05:22:09.499637: step 345, loss 0.459993, acc 0.9\n",
      "current_step:  345\n",
      "2017-08-22T05:22:09.831714: step 346, loss 0.432119, acc 0.84\n",
      "current_step:  346\n",
      "2017-08-22T05:22:10.164704: step 347, loss 0.778711, acc 0.72\n",
      "current_step:  347\n",
      "2017-08-22T05:22:10.497987: step 348, loss 0.556263, acc 0.86\n",
      "current_step:  348\n",
      "2017-08-22T05:22:10.830359: step 349, loss 0.542041, acc 0.9\n",
      "current_step:  349\n",
      "2017-08-22T05:22:11.162033: step 350, loss 0.362552, acc 0.92\n",
      "current_step:  350\n",
      "2017-08-22T05:22:11.492178: step 351, loss 0.400465, acc 0.92\n",
      "current_step:  351\n",
      "2017-08-22T05:22:11.823688: step 352, loss 0.5823, acc 0.8\n",
      "current_step:  352\n",
      "2017-08-22T05:22:12.154154: step 353, loss 0.443677, acc 0.92\n",
      "current_step:  353\n",
      "2017-08-22T05:22:12.485057: step 354, loss 0.429321, acc 0.9\n",
      "current_step:  354\n",
      "2017-08-22T05:22:12.817998: step 355, loss 0.599075, acc 0.78\n",
      "current_step:  355\n",
      "2017-08-22T05:22:13.149234: step 356, loss 0.419966, acc 0.86\n",
      "current_step:  356\n",
      "2017-08-22T05:22:13.480508: step 357, loss 0.391296, acc 0.9\n",
      "current_step:  357\n",
      "2017-08-22T05:22:13.811262: step 358, loss 0.636299, acc 0.76\n",
      "current_step:  358\n",
      "2017-08-22T05:22:14.142692: step 359, loss 0.43232, acc 0.92\n",
      "current_step:  359\n",
      "2017-08-22T05:22:14.473243: step 360, loss 0.32681, acc 0.96\n",
      "current_step:  360\n",
      "2017-08-22T05:22:14.807765: step 361, loss 0.549557, acc 0.8\n",
      "current_step:  361\n",
      "2017-08-22T05:22:15.138428: step 362, loss 0.432037, acc 0.86\n",
      "current_step:  362\n",
      "2017-08-22T05:22:15.469248: step 363, loss 0.635192, acc 0.78\n",
      "current_step:  363\n",
      "2017-08-22T05:22:15.802180: step 364, loss 0.541622, acc 0.84\n",
      "current_step:  364\n",
      "2017-08-22T05:22:16.134705: step 365, loss 0.380079, acc 0.94\n",
      "current_step:  365\n",
      "2017-08-22T05:22:16.466732: step 366, loss 0.396811, acc 0.86\n",
      "current_step:  366\n",
      "2017-08-22T05:22:16.799681: step 367, loss 0.446895, acc 0.82\n",
      "current_step:  367\n",
      "2017-08-22T05:22:17.130528: step 368, loss 0.464204, acc 0.82\n",
      "current_step:  368\n",
      "2017-08-22T05:22:17.461384: step 369, loss 0.563381, acc 0.78\n",
      "current_step:  369\n",
      "2017-08-22T05:22:17.793238: step 370, loss 0.487416, acc 0.84\n",
      "current_step:  370\n",
      "2017-08-22T05:22:18.123746: step 371, loss 0.612186, acc 0.82\n",
      "current_step:  371\n",
      "2017-08-22T05:22:18.455196: step 372, loss 0.531601, acc 0.82\n",
      "current_step:  372\n",
      "2017-08-22T05:22:18.786788: step 373, loss 0.474716, acc 0.88\n",
      "current_step:  373\n",
      "2017-08-22T05:22:19.116900: step 374, loss 0.481798, acc 0.9\n",
      "current_step:  374\n",
      "2017-08-22T05:22:19.446686: step 375, loss 0.4379, acc 0.88\n",
      "current_step:  375\n",
      "2017-08-22T05:22:19.779325: step 376, loss 0.383302, acc 0.92\n",
      "current_step:  376\n",
      "2017-08-22T05:22:20.109727: step 377, loss 0.520901, acc 0.84\n",
      "current_step:  377\n",
      "2017-08-22T05:22:20.443575: step 378, loss 0.574978, acc 0.78\n",
      "current_step:  378\n",
      "2017-08-22T05:22:20.777143: step 379, loss 0.442565, acc 0.82\n",
      "current_step:  379\n",
      "2017-08-22T05:22:21.107721: step 380, loss 0.321858, acc 0.94\n",
      "current_step:  380\n",
      "2017-08-22T05:22:21.440959: step 381, loss 0.57648, acc 0.8\n",
      "current_step:  381\n",
      "2017-08-22T05:22:21.774469: step 382, loss 0.56837, acc 0.78\n",
      "current_step:  382\n",
      "2017-08-22T05:22:22.105095: step 383, loss 0.547902, acc 0.82\n",
      "current_step:  383\n",
      "2017-08-22T05:22:22.436854: step 384, loss 0.401615, acc 0.88\n",
      "current_step:  384\n",
      "2017-08-22T05:22:22.769367: step 385, loss 0.505675, acc 0.8\n",
      "current_step:  385\n",
      "2017-08-22T05:22:23.101053: step 386, loss 0.318815, acc 0.98\n",
      "current_step:  386\n",
      "2017-08-22T05:22:23.430772: step 387, loss 0.445281, acc 0.84\n",
      "current_step:  387\n",
      "2017-08-22T05:22:23.763256: step 388, loss 0.506997, acc 0.84\n",
      "current_step:  388\n",
      "2017-08-22T05:22:24.095635: step 389, loss 0.424581, acc 0.94\n",
      "current_step:  389\n",
      "2017-08-22T05:22:24.427354: step 390, loss 0.605045, acc 0.82\n",
      "current_step:  390\n",
      "2017-08-22T05:22:24.761555: step 391, loss 0.547325, acc 0.82\n",
      "current_step:  391\n",
      "2017-08-22T05:22:25.093033: step 392, loss 0.56907, acc 0.9\n",
      "current_step:  392\n",
      "2017-08-22T05:22:25.424823: step 393, loss 0.521125, acc 0.84\n",
      "current_step:  393\n",
      "2017-08-22T05:22:25.759535: step 394, loss 0.512626, acc 0.86\n",
      "current_step:  394\n",
      "2017-08-22T05:22:26.090471: step 395, loss 0.567448, acc 0.82\n",
      "current_step:  395\n",
      "2017-08-22T05:22:26.424073: step 396, loss 0.561454, acc 0.84\n",
      "current_step:  396\n",
      "2017-08-22T05:22:26.756805: step 397, loss 0.545043, acc 0.8\n",
      "current_step:  397\n",
      "2017-08-22T05:22:27.088744: step 398, loss 0.641555, acc 0.78\n",
      "current_step:  398\n",
      "2017-08-22T05:22:27.419069: step 399, loss 0.550029, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-22T05:22:27.751115: step 400, loss 0.516075, acc 0.9\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:22:28.973487: step 400, loss 0.862211, acc 0.724199\n",
      "\n",
      "2017-08-22T05:22:29.306597: step 401, loss 0.559149, acc 0.8\n",
      "current_step:  401\n",
      "2017-08-22T05:22:29.636318: step 402, loss 0.380083, acc 0.94\n",
      "current_step:  402\n",
      "2017-08-22T05:22:29.965628: step 403, loss 0.525669, acc 0.84\n",
      "current_step:  403\n",
      "2017-08-22T05:22:30.297902: step 404, loss 0.491501, acc 0.88\n",
      "current_step:  404\n",
      "2017-08-22T05:22:30.630533: step 405, loss 0.586582, acc 0.76\n",
      "current_step:  405\n",
      "2017-08-22T05:22:30.961488: step 406, loss 0.480019, acc 0.86\n",
      "current_step:  406\n",
      "2017-08-22T05:22:31.291081: step 407, loss 0.382912, acc 0.9\n",
      "current_step:  407\n",
      "2017-08-22T05:22:31.356212: step 408, loss 0.185735, acc 1\n",
      "current_step:  408\n",
      "2017-08-22T05:22:31.704036: step 409, loss 0.353407, acc 0.84\n",
      "current_step:  409\n",
      "2017-08-22T05:22:32.035070: step 410, loss 0.405666, acc 0.9\n",
      "current_step:  410\n",
      "2017-08-22T05:22:32.364115: step 411, loss 0.205151, acc 0.98\n",
      "current_step:  411\n",
      "2017-08-22T05:22:32.695309: step 412, loss 0.390313, acc 0.88\n",
      "current_step:  412\n",
      "2017-08-22T05:22:33.025327: step 413, loss 0.356109, acc 0.92\n",
      "current_step:  413\n",
      "2017-08-22T05:22:33.358207: step 414, loss 0.370375, acc 0.88\n",
      "current_step:  414\n",
      "2017-08-22T05:22:33.694181: step 415, loss 0.356151, acc 0.88\n",
      "current_step:  415\n",
      "2017-08-22T05:22:34.026328: step 416, loss 0.332816, acc 0.92\n",
      "current_step:  416\n",
      "2017-08-22T05:22:34.356197: step 417, loss 0.450999, acc 0.9\n",
      "current_step:  417\n",
      "2017-08-22T05:22:34.690187: step 418, loss 0.336119, acc 0.94\n",
      "current_step:  418\n",
      "2017-08-22T05:22:35.020864: step 419, loss 0.420941, acc 0.94\n",
      "current_step:  419\n",
      "2017-08-22T05:22:35.354431: step 420, loss 0.366572, acc 0.9\n",
      "current_step:  420\n",
      "2017-08-22T05:22:35.685871: step 421, loss 0.317024, acc 0.94\n",
      "current_step:  421\n",
      "2017-08-22T05:22:36.018932: step 422, loss 0.370246, acc 0.96\n",
      "current_step:  422\n",
      "2017-08-22T05:22:36.350313: step 423, loss 0.358661, acc 0.88\n",
      "current_step:  423\n",
      "2017-08-22T05:22:36.681023: step 424, loss 0.396707, acc 0.92\n",
      "current_step:  424\n",
      "2017-08-22T05:22:37.011901: step 425, loss 0.503397, acc 0.9\n",
      "current_step:  425\n",
      "2017-08-22T05:22:37.343768: step 426, loss 0.360641, acc 0.92\n",
      "current_step:  426\n",
      "2017-08-22T05:22:37.675542: step 427, loss 0.334298, acc 0.94\n",
      "current_step:  427\n",
      "2017-08-22T05:22:38.007906: step 428, loss 0.454645, acc 0.86\n",
      "current_step:  428\n",
      "2017-08-22T05:22:38.337087: step 429, loss 0.268319, acc 0.94\n",
      "current_step:  429\n",
      "2017-08-22T05:22:38.668290: step 430, loss 0.438275, acc 0.88\n",
      "current_step:  430\n",
      "2017-08-22T05:22:38.998864: step 431, loss 0.478927, acc 0.88\n",
      "current_step:  431\n",
      "2017-08-22T05:22:39.329892: step 432, loss 0.55172, acc 0.82\n",
      "current_step:  432\n",
      "2017-08-22T05:22:39.662164: step 433, loss 0.286043, acc 0.96\n",
      "current_step:  433\n",
      "2017-08-22T05:22:39.992888: step 434, loss 0.387019, acc 0.9\n",
      "current_step:  434\n",
      "2017-08-22T05:22:40.322683: step 435, loss 0.277337, acc 0.98\n",
      "current_step:  435\n",
      "2017-08-22T05:22:40.652849: step 436, loss 0.34861, acc 0.9\n",
      "current_step:  436\n",
      "2017-08-22T05:22:40.984202: step 437, loss 0.321449, acc 0.92\n",
      "current_step:  437\n",
      "2017-08-22T05:22:41.312633: step 438, loss 0.378716, acc 0.86\n",
      "current_step:  438\n",
      "2017-08-22T05:22:41.644161: step 439, loss 0.434276, acc 0.86\n",
      "current_step:  439\n",
      "2017-08-22T05:22:41.975021: step 440, loss 0.458955, acc 0.86\n",
      "current_step:  440\n",
      "2017-08-22T05:22:42.306726: step 441, loss 0.339393, acc 0.9\n",
      "current_step:  441\n",
      "2017-08-22T05:22:42.638675: step 442, loss 0.438263, acc 0.86\n",
      "current_step:  442\n",
      "2017-08-22T05:22:42.968201: step 443, loss 0.353801, acc 0.92\n",
      "current_step:  443\n",
      "2017-08-22T05:22:43.299655: step 444, loss 0.426002, acc 0.84\n",
      "current_step:  444\n",
      "2017-08-22T05:22:43.631219: step 445, loss 0.355278, acc 0.92\n",
      "current_step:  445\n",
      "2017-08-22T05:22:43.963527: step 446, loss 0.439306, acc 0.82\n",
      "current_step:  446\n",
      "2017-08-22T05:22:44.295066: step 447, loss 0.379801, acc 0.9\n",
      "current_step:  447\n",
      "2017-08-22T05:22:44.626932: step 448, loss 0.495326, acc 0.84\n",
      "current_step:  448\n",
      "2017-08-22T05:22:44.959910: step 449, loss 0.253306, acc 0.98\n",
      "current_step:  449\n",
      "2017-08-22T05:22:45.290542: step 450, loss 0.263405, acc 0.98\n",
      "current_step:  450\n",
      "2017-08-22T05:22:45.623381: step 451, loss 0.31775, acc 0.94\n",
      "current_step:  451\n",
      "2017-08-22T05:22:45.958021: step 452, loss 0.400172, acc 0.9\n",
      "current_step:  452\n",
      "2017-08-22T05:22:46.289728: step 453, loss 0.454959, acc 0.88\n",
      "current_step:  453\n",
      "2017-08-22T05:22:46.620464: step 454, loss 0.520636, acc 0.88\n",
      "current_step:  454\n",
      "2017-08-22T05:22:46.951291: step 455, loss 0.291942, acc 0.98\n",
      "current_step:  455\n",
      "2017-08-22T05:22:47.281643: step 456, loss 0.418589, acc 0.92\n",
      "current_step:  456\n",
      "2017-08-22T05:22:47.612230: step 457, loss 0.371728, acc 0.94\n",
      "current_step:  457\n",
      "2017-08-22T05:22:47.942725: step 458, loss 0.442539, acc 0.92\n",
      "current_step:  458\n",
      "2017-08-22T05:22:48.273348: step 459, loss 0.321421, acc 0.92\n",
      "current_step:  459\n",
      "2017-08-22T05:22:48.604888: step 460, loss 0.429174, acc 0.86\n",
      "current_step:  460\n",
      "2017-08-22T05:22:48.937833: step 461, loss 0.410081, acc 0.88\n",
      "current_step:  461\n",
      "2017-08-22T05:22:49.271660: step 462, loss 0.348204, acc 0.9\n",
      "current_step:  462\n",
      "2017-08-22T05:22:49.601816: step 463, loss 0.348053, acc 0.9\n",
      "current_step:  463\n",
      "2017-08-22T05:22:49.934480: step 464, loss 0.355825, acc 0.92\n",
      "current_step:  464\n",
      "2017-08-22T05:22:50.268214: step 465, loss 0.429458, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-22T05:22:50.599272: step 466, loss 0.369104, acc 0.92\n",
      "current_step:  466\n",
      "2017-08-22T05:22:50.932460: step 467, loss 0.321352, acc 0.92\n",
      "current_step:  467\n",
      "2017-08-22T05:22:51.264061: step 468, loss 0.487023, acc 0.84\n",
      "current_step:  468\n",
      "2017-08-22T05:22:51.593902: step 469, loss 0.336525, acc 0.94\n",
      "current_step:  469\n",
      "2017-08-22T05:22:51.927153: step 470, loss 0.343892, acc 0.94\n",
      "current_step:  470\n",
      "2017-08-22T05:22:52.258903: step 471, loss 0.359559, acc 0.9\n",
      "current_step:  471\n",
      "2017-08-22T05:22:52.592011: step 472, loss 0.406098, acc 0.88\n",
      "current_step:  472\n",
      "2017-08-22T05:22:52.923452: step 473, loss 0.271481, acc 0.96\n",
      "current_step:  473\n",
      "2017-08-22T05:22:53.255307: step 474, loss 0.428828, acc 0.86\n",
      "current_step:  474\n",
      "2017-08-22T05:22:53.588422: step 475, loss 0.386038, acc 0.88\n",
      "current_step:  475\n",
      "2017-08-22T05:22:53.918132: step 476, loss 0.384948, acc 0.9\n",
      "current_step:  476\n",
      "2017-08-22T05:22:54.250150: step 477, loss 0.373562, acc 0.92\n",
      "current_step:  477\n",
      "2017-08-22T05:22:54.581615: step 478, loss 0.409451, acc 0.86\n",
      "current_step:  478\n",
      "2017-08-22T05:22:54.915618: step 479, loss 0.338417, acc 0.94\n",
      "current_step:  479\n",
      "2017-08-22T05:22:55.247200: step 480, loss 0.38063, acc 0.96\n",
      "current_step:  480\n",
      "2017-08-22T05:22:55.581653: step 481, loss 0.48216, acc 0.88\n",
      "current_step:  481\n",
      "2017-08-22T05:22:55.912175: step 482, loss 0.266945, acc 0.98\n",
      "current_step:  482\n",
      "2017-08-22T05:22:56.241416: step 483, loss 0.404737, acc 0.92\n",
      "current_step:  483\n",
      "2017-08-22T05:22:56.571872: step 484, loss 0.611841, acc 0.78\n",
      "current_step:  484\n",
      "2017-08-22T05:22:56.904804: step 485, loss 0.326941, acc 0.96\n",
      "current_step:  485\n",
      "2017-08-22T05:22:57.234943: step 486, loss 0.331367, acc 0.98\n",
      "current_step:  486\n",
      "2017-08-22T05:22:57.568825: step 487, loss 0.308575, acc 0.92\n",
      "current_step:  487\n",
      "2017-08-22T05:22:57.900934: step 488, loss 0.358538, acc 0.92\n",
      "current_step:  488\n",
      "2017-08-22T05:22:58.230341: step 489, loss 0.293894, acc 0.96\n",
      "current_step:  489\n",
      "2017-08-22T05:22:58.560807: step 490, loss 0.343567, acc 0.94\n",
      "current_step:  490\n",
      "2017-08-22T05:22:58.892896: step 491, loss 0.335615, acc 0.9\n",
      "current_step:  491\n",
      "2017-08-22T05:22:59.224660: step 492, loss 0.491633, acc 0.86\n",
      "current_step:  492\n",
      "2017-08-22T05:22:59.557443: step 493, loss 0.321923, acc 0.94\n",
      "current_step:  493\n",
      "2017-08-22T05:22:59.891058: step 494, loss 0.479035, acc 0.78\n",
      "current_step:  494\n",
      "2017-08-22T05:23:00.219305: step 495, loss 0.265231, acc 0.94\n",
      "current_step:  495\n",
      "2017-08-22T05:23:00.548167: step 496, loss 0.554421, acc 0.78\n",
      "current_step:  496\n",
      "2017-08-22T05:23:00.882552: step 497, loss 0.395757, acc 0.9\n",
      "current_step:  497\n",
      "2017-08-22T05:23:01.214575: step 498, loss 0.379736, acc 0.86\n",
      "current_step:  498\n",
      "2017-08-22T05:23:01.546418: step 499, loss 0.226167, acc 0.96\n",
      "current_step:  499\n",
      "2017-08-22T05:23:01.879318: step 500, loss 0.354759, acc 0.92\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:23:03.102079: step 500, loss 0.84429, acc 0.724199\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0520_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:23:03.497836: step 501, loss 0.365423, acc 0.9\n",
      "current_step:  501\n",
      "2017-08-22T05:23:03.830968: step 502, loss 0.336354, acc 0.92\n",
      "current_step:  502\n",
      "2017-08-22T05:23:04.161762: step 503, loss 0.381111, acc 0.9\n",
      "current_step:  503\n",
      "2017-08-22T05:23:04.493506: step 504, loss 0.511696, acc 0.82\n",
      "current_step:  504\n",
      "2017-08-22T05:23:04.826750: step 505, loss 0.359344, acc 0.94\n",
      "current_step:  505\n",
      "2017-08-22T05:23:05.159218: step 506, loss 0.413371, acc 0.9\n",
      "current_step:  506\n",
      "2017-08-22T05:23:05.490055: step 507, loss 0.306314, acc 0.92\n",
      "current_step:  507\n",
      "2017-08-22T05:23:05.823990: step 508, loss 0.381616, acc 0.9\n",
      "current_step:  508\n",
      "2017-08-22T05:23:06.156578: step 509, loss 0.405737, acc 0.96\n",
      "current_step:  509\n",
      "2017-08-22T05:23:06.220418: step 510, loss 0.330464, acc 1\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.635739\n",
      "Precision Score = 0.616988\n",
      "Recall Score = 0.672598\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.648244\n",
      "Precision Score = 0.638311\n",
      "Recall Score = 0.701068\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.668601\n",
      "Precision Score = 0.663833\n",
      "Recall Score = 0.713523\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.691914\n",
      "Precision Score = 0.676597\n",
      "Recall Score = 0.715302\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.683810\n",
      "Precision Score = 0.680049\n",
      "Recall Score = 0.715302\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v2 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 2, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  264\n",
      "Vocabulary Size: 6361\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0523_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0523_UTC\n",
      "2017-08-22T05:23:29.083129: step 1, loss 1.81808, acc 0.2\n",
      "current_step:  1\n",
      "2017-08-22T05:23:29.218174: step 2, loss 1.34052, acc 0.4\n",
      "current_step:  2\n",
      "2017-08-22T05:23:29.351141: step 3, loss 1.30688, acc 0.54\n",
      "current_step:  3\n",
      "2017-08-22T05:23:29.480881: step 4, loss 1.58922, acc 0.52\n",
      "current_step:  4\n",
      "2017-08-22T05:23:29.610732: step 5, loss 1.64245, acc 0.46\n",
      "current_step:  5\n",
      "2017-08-22T05:23:29.740156: step 6, loss 1.57429, acc 0.42\n",
      "current_step:  6\n",
      "2017-08-22T05:23:29.869275: step 7, loss 1.33345, acc 0.5\n",
      "current_step:  7\n",
      "2017-08-22T05:23:29.999285: step 8, loss 1.35292, acc 0.6\n",
      "current_step:  8\n",
      "2017-08-22T05:23:30.128618: step 9, loss 1.39272, acc 0.56\n",
      "current_step:  9\n",
      "2017-08-22T05:23:30.256840: step 10, loss 1.38766, acc 0.56\n",
      "current_step:  10\n",
      "2017-08-22T05:23:30.385287: step 11, loss 1.04091, acc 0.58\n",
      "current_step:  11\n",
      "2017-08-22T05:23:30.515245: step 12, loss 1.47107, acc 0.46\n",
      "current_step:  12\n",
      "2017-08-22T05:23:30.646971: step 13, loss 1.12267, acc 0.58\n",
      "current_step:  13\n",
      "2017-08-22T05:23:30.859121: step 14, loss 1.41829, acc 0.42\n",
      "current_step:  14\n",
      "2017-08-22T05:23:31.075047: step 15, loss 1.28058, acc 0.56\n",
      "current_step:  15\n",
      "2017-08-22T05:23:31.291381: step 16, loss 1.21912, acc 0.56\n",
      "current_step:  16\n",
      "2017-08-22T05:23:31.505503: step 17, loss 1.37997, acc 0.48\n",
      "current_step:  17\n",
      "2017-08-22T05:23:31.720385: step 18, loss 1.36843, acc 0.52\n",
      "current_step:  18\n",
      "2017-08-22T05:23:31.933565: step 19, loss 1.16285, acc 0.58\n",
      "current_step:  19\n",
      "2017-08-22T05:23:32.149141: step 20, loss 1.59614, acc 0.56\n",
      "current_step:  20\n",
      "2017-08-22T05:23:32.364624: step 21, loss 1.24183, acc 0.58\n",
      "current_step:  21\n",
      "2017-08-22T05:23:32.577904: step 22, loss 1.23732, acc 0.52\n",
      "current_step:  22\n",
      "2017-08-22T05:23:32.793382: step 23, loss 1.00257, acc 0.62\n",
      "current_step:  23\n",
      "2017-08-22T05:23:33.008302: step 24, loss 1.42914, acc 0.52\n",
      "current_step:  24\n",
      "2017-08-22T05:23:33.223003: step 25, loss 1.25014, acc 0.56\n",
      "current_step:  25\n",
      "2017-08-22T05:23:33.438899: step 26, loss 1.2061, acc 0.5\n",
      "current_step:  26\n",
      "2017-08-22T05:23:33.652920: step 27, loss 1.32253, acc 0.56\n",
      "current_step:  27\n",
      "2017-08-22T05:23:33.868735: step 28, loss 1.15137, acc 0.62\n",
      "current_step:  28\n",
      "2017-08-22T05:23:34.083007: step 29, loss 1.09106, acc 0.54\n",
      "current_step:  29\n",
      "2017-08-22T05:23:34.297358: step 30, loss 1.09872, acc 0.66\n",
      "current_step:  30\n",
      "2017-08-22T05:23:34.511849: step 31, loss 1.27725, acc 0.6\n",
      "current_step:  31\n",
      "2017-08-22T05:23:34.727394: step 32, loss 1.24408, acc 0.56\n",
      "current_step:  32\n",
      "2017-08-22T05:23:34.941376: step 33, loss 1.2209, acc 0.6\n",
      "current_step:  33\n",
      "2017-08-22T05:23:35.154394: step 34, loss 1.20999, acc 0.48\n",
      "current_step:  34\n",
      "2017-08-22T05:23:35.368045: step 35, loss 0.772938, acc 0.76\n",
      "current_step:  35\n",
      "2017-08-22T05:23:35.582499: step 36, loss 1.26994, acc 0.62\n",
      "current_step:  36\n",
      "2017-08-22T05:23:35.798903: step 37, loss 0.877806, acc 0.78\n",
      "current_step:  37\n",
      "2017-08-22T05:23:36.012977: step 38, loss 1.16436, acc 0.64\n",
      "current_step:  38\n",
      "2017-08-22T05:23:36.225901: step 39, loss 0.862311, acc 0.72\n",
      "current_step:  39\n",
      "2017-08-22T05:23:36.442572: step 40, loss 1.63428, acc 0.4\n",
      "current_step:  40\n",
      "2017-08-22T05:23:36.658112: step 41, loss 1.22551, acc 0.54\n",
      "current_step:  41\n",
      "2017-08-22T05:23:36.873693: step 42, loss 0.691103, acc 0.78\n",
      "current_step:  42\n",
      "2017-08-22T05:23:37.088505: step 43, loss 1.02555, acc 0.56\n",
      "current_step:  43\n",
      "2017-08-22T05:23:37.304688: step 44, loss 1.00613, acc 0.6\n",
      "current_step:  44\n",
      "2017-08-22T05:23:37.518944: step 45, loss 1.24418, acc 0.58\n",
      "current_step:  45\n",
      "2017-08-22T05:23:37.735535: step 46, loss 1.44376, acc 0.42\n",
      "current_step:  46\n",
      "2017-08-22T05:23:37.949311: step 47, loss 1.05438, acc 0.64\n",
      "current_step:  47\n",
      "2017-08-22T05:23:38.163438: step 48, loss 1.02948, acc 0.64\n",
      "current_step:  48\n",
      "2017-08-22T05:23:38.378658: step 49, loss 1.207, acc 0.62\n",
      "current_step:  49\n",
      "2017-08-22T05:23:38.593051: step 50, loss 1.00796, acc 0.58\n",
      "current_step:  50\n",
      "2017-08-22T05:23:38.806912: step 51, loss 1.25045, acc 0.64\n",
      "current_step:  51\n",
      "2017-08-22T05:23:39.020280: step 52, loss 0.887167, acc 0.72\n",
      "current_step:  52\n",
      "2017-08-22T05:23:39.234825: step 53, loss 1.18362, acc 0.56\n",
      "current_step:  53\n",
      "2017-08-22T05:23:39.448702: step 54, loss 1.14362, acc 0.64\n",
      "current_step:  54\n",
      "2017-08-22T05:23:39.662690: step 55, loss 0.996504, acc 0.64\n",
      "current_step:  55\n",
      "2017-08-22T05:23:39.878968: step 56, loss 0.868141, acc 0.7\n",
      "current_step:  56\n",
      "2017-08-22T05:23:40.094914: step 57, loss 1.38923, acc 0.52\n",
      "current_step:  57\n",
      "2017-08-22T05:23:40.310449: step 58, loss 1.2574, acc 0.5\n",
      "current_step:  58\n",
      "2017-08-22T05:23:40.524676: step 59, loss 1.19148, acc 0.56\n",
      "current_step:  59\n",
      "2017-08-22T05:23:40.739499: step 60, loss 1.00324, acc 0.64\n",
      "current_step:  60\n",
      "2017-08-22T05:23:40.953921: step 61, loss 1.32955, acc 0.56\n",
      "current_step:  61\n",
      "2017-08-22T05:23:41.168420: step 62, loss 1.31394, acc 0.6\n",
      "current_step:  62\n",
      "2017-08-22T05:23:41.383102: step 63, loss 0.860021, acc 0.78\n",
      "current_step:  63\n",
      "2017-08-22T05:23:41.597255: step 64, loss 0.867003, acc 0.7\n",
      "current_step:  64\n",
      "2017-08-22T05:23:41.814032: step 65, loss 1.60339, acc 0.52\n",
      "current_step:  65\n",
      "2017-08-22T05:23:42.027891: step 66, loss 1.1955, acc 0.56\n",
      "current_step:  66\n",
      "2017-08-22T05:23:42.242491: step 67, loss 1.44626, acc 0.5\n",
      "current_step:  67\n",
      "2017-08-22T05:23:42.456745: step 68, loss 1.16054, acc 0.58\n",
      "current_step:  68\n",
      "2017-08-22T05:23:42.673032: step 69, loss 1.13432, acc 0.58\n",
      "current_step:  69\n",
      "2017-08-22T05:23:42.886668: step 70, loss 1.318, acc 0.48\n",
      "current_step:  70\n",
      "2017-08-22T05:23:43.103379: step 71, loss 1.15209, acc 0.54\n",
      "current_step:  71\n",
      "2017-08-22T05:23:43.318888: step 72, loss 1.21886, acc 0.56\n",
      "current_step:  72\n",
      "2017-08-22T05:23:43.534230: step 73, loss 1.03264, acc 0.62\n",
      "current_step:  73\n",
      "2017-08-22T05:23:43.750139: step 74, loss 1.29136, acc 0.56\n",
      "current_step:  74\n",
      "2017-08-22T05:23:43.965321: step 75, loss 1.02108, acc 0.68\n",
      "current_step:  75\n",
      "2017-08-22T05:23:44.182048: step 76, loss 1.0784, acc 0.56\n",
      "current_step:  76\n",
      "2017-08-22T05:23:44.398474: step 77, loss 1.24561, acc 0.56\n",
      "current_step:  77\n",
      "2017-08-22T05:23:44.614784: step 78, loss 0.9683, acc 0.64\n",
      "current_step:  78\n",
      "2017-08-22T05:23:44.829883: step 79, loss 1.12211, acc 0.58\n",
      "current_step:  79\n",
      "2017-08-22T05:23:45.044341: step 80, loss 0.942733, acc 0.66\n",
      "current_step:  80\n",
      "2017-08-22T05:23:45.260690: step 81, loss 1.0394, acc 0.54\n",
      "current_step:  81\n",
      "2017-08-22T05:23:45.476414: step 82, loss 1.08073, acc 0.6\n",
      "current_step:  82\n",
      "2017-08-22T05:23:45.691698: step 83, loss 0.863276, acc 0.72\n",
      "current_step:  83\n",
      "2017-08-22T05:23:45.905685: step 84, loss 1.43155, acc 0.58\n",
      "current_step:  84\n",
      "2017-08-22T05:23:46.119393: step 85, loss 0.93242, acc 0.66\n",
      "current_step:  85\n",
      "2017-08-22T05:23:46.333914: step 86, loss 1.12308, acc 0.6\n",
      "current_step:  86\n",
      "2017-08-22T05:23:46.550537: step 87, loss 1.07559, acc 0.6\n",
      "current_step:  87\n",
      "2017-08-22T05:23:46.766029: step 88, loss 1.05651, acc 0.68\n",
      "current_step:  88\n",
      "2017-08-22T05:23:46.980989: step 89, loss 1.11169, acc 0.62\n",
      "current_step:  89\n",
      "2017-08-22T05:23:47.195959: step 90, loss 0.880345, acc 0.72\n",
      "current_step:  90\n",
      "2017-08-22T05:23:47.411811: step 91, loss 1.20307, acc 0.56\n",
      "current_step:  91\n",
      "2017-08-22T05:23:47.627810: step 92, loss 0.711467, acc 0.74\n",
      "current_step:  92\n",
      "2017-08-22T05:23:47.845134: step 93, loss 1.12022, acc 0.6\n",
      "current_step:  93\n",
      "2017-08-22T05:23:48.060274: step 94, loss 0.983199, acc 0.6\n",
      "current_step:  94\n",
      "2017-08-22T05:23:48.275883: step 95, loss 0.814732, acc 0.68\n",
      "current_step:  95\n",
      "2017-08-22T05:23:48.490808: step 96, loss 0.815312, acc 0.74\n",
      "current_step:  96\n",
      "2017-08-22T05:23:48.708908: step 97, loss 1.17889, acc 0.58\n",
      "current_step:  97\n",
      "2017-08-22T05:23:48.923259: step 98, loss 1.29983, acc 0.6\n",
      "current_step:  98\n",
      "2017-08-22T05:23:49.138347: step 99, loss 1.00173, acc 0.64\n",
      "current_step:  99\n",
      "2017-08-22T05:23:49.354560: step 100, loss 0.970776, acc 0.72\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:23:50.145564: step 100, loss 0.971733, acc 0.672598\n",
      "\n",
      "2017-08-22T05:23:50.359857: step 101, loss 1.0666, acc 0.58\n",
      "current_step:  101\n",
      "2017-08-22T05:23:50.402266: step 102, loss 0.73198, acc 0.666667\n",
      "current_step:  102\n",
      "2017-08-22T05:23:50.619218: step 103, loss 0.640525, acc 0.78\n",
      "current_step:  103\n",
      "2017-08-22T05:23:50.832601: step 104, loss 0.829793, acc 0.68\n",
      "current_step:  104\n",
      "2017-08-22T05:23:51.047901: step 105, loss 0.76556, acc 0.66\n",
      "current_step:  105\n",
      "2017-08-22T05:23:51.263082: step 106, loss 0.866073, acc 0.66\n",
      "current_step:  106\n",
      "2017-08-22T05:23:51.477997: step 107, loss 0.850957, acc 0.64\n",
      "current_step:  107\n",
      "2017-08-22T05:23:51.694946: step 108, loss 0.82574, acc 0.68\n",
      "current_step:  108\n",
      "2017-08-22T05:23:51.908252: step 109, loss 0.809068, acc 0.68\n",
      "current_step:  109\n",
      "2017-08-22T05:23:52.123138: step 110, loss 0.784871, acc 0.8\n",
      "current_step:  110\n",
      "2017-08-22T05:23:52.338988: step 111, loss 0.913009, acc 0.72\n",
      "current_step:  111\n",
      "2017-08-22T05:23:52.554003: step 112, loss 0.939448, acc 0.7\n",
      "current_step:  112\n",
      "2017-08-22T05:23:52.770628: step 113, loss 0.871058, acc 0.7\n",
      "current_step:  113\n",
      "2017-08-22T05:23:52.986440: step 114, loss 0.819829, acc 0.72\n",
      "current_step:  114\n",
      "2017-08-22T05:23:53.200344: step 115, loss 0.628974, acc 0.74\n",
      "current_step:  115\n",
      "2017-08-22T05:23:53.415395: step 116, loss 0.666937, acc 0.76\n",
      "current_step:  116\n",
      "2017-08-22T05:23:53.629945: step 117, loss 0.856439, acc 0.7\n",
      "current_step:  117\n",
      "2017-08-22T05:23:53.843958: step 118, loss 0.8853, acc 0.7\n",
      "current_step:  118\n",
      "2017-08-22T05:23:54.058534: step 119, loss 0.808666, acc 0.64\n",
      "current_step:  119\n",
      "2017-08-22T05:23:54.274538: step 120, loss 0.719838, acc 0.78\n",
      "current_step:  120\n",
      "2017-08-22T05:23:54.490457: step 121, loss 1.08131, acc 0.68\n",
      "current_step:  121\n",
      "2017-08-22T05:23:54.707052: step 122, loss 0.819965, acc 0.72\n",
      "current_step:  122\n",
      "2017-08-22T05:23:54.922799: step 123, loss 0.782936, acc 0.68\n",
      "current_step:  123\n",
      "2017-08-22T05:23:55.137056: step 124, loss 0.849394, acc 0.7\n",
      "current_step:  124\n",
      "2017-08-22T05:23:55.351021: step 125, loss 0.917598, acc 0.72\n",
      "current_step:  125\n",
      "2017-08-22T05:23:55.565366: step 126, loss 0.885044, acc 0.7\n",
      "current_step:  126\n",
      "2017-08-22T05:23:55.783075: step 127, loss 0.90914, acc 0.7\n",
      "current_step:  127\n",
      "2017-08-22T05:23:55.997702: step 128, loss 1.0578, acc 0.62\n",
      "current_step:  128\n",
      "2017-08-22T05:23:56.213694: step 129, loss 0.778333, acc 0.76\n",
      "current_step:  129\n",
      "2017-08-22T05:23:56.428195: step 130, loss 0.609586, acc 0.84\n",
      "current_step:  130\n",
      "2017-08-22T05:23:56.643833: step 131, loss 0.726048, acc 0.72\n",
      "current_step:  131\n",
      "2017-08-22T05:23:56.859662: step 132, loss 0.749405, acc 0.76\n",
      "current_step:  132\n",
      "2017-08-22T05:23:57.075196: step 133, loss 0.903524, acc 0.64\n",
      "current_step:  133\n",
      "2017-08-22T05:23:57.288773: step 134, loss 0.800639, acc 0.76\n",
      "current_step:  134\n",
      "2017-08-22T05:23:57.505260: step 135, loss 0.817341, acc 0.72\n",
      "current_step:  135\n",
      "2017-08-22T05:23:57.721015: step 136, loss 0.80888, acc 0.7\n",
      "current_step:  136\n",
      "2017-08-22T05:23:57.936038: step 137, loss 0.756712, acc 0.76\n",
      "current_step:  137\n",
      "2017-08-22T05:23:58.150491: step 138, loss 0.704604, acc 0.7\n",
      "current_step:  138\n",
      "2017-08-22T05:23:58.365272: step 139, loss 0.845427, acc 0.7\n",
      "current_step:  139\n",
      "2017-08-22T05:23:58.580812: step 140, loss 0.876841, acc 0.68\n",
      "current_step:  140\n",
      "2017-08-22T05:23:58.795551: step 141, loss 0.741156, acc 0.8\n",
      "current_step:  141\n",
      "2017-08-22T05:23:59.010139: step 142, loss 0.868946, acc 0.72\n",
      "current_step:  142\n",
      "2017-08-22T05:23:59.224637: step 143, loss 1.03114, acc 0.64\n",
      "current_step:  143\n",
      "2017-08-22T05:23:59.441187: step 144, loss 0.790975, acc 0.66\n",
      "current_step:  144\n",
      "2017-08-22T05:23:59.657585: step 145, loss 0.857966, acc 0.7\n",
      "current_step:  145\n",
      "2017-08-22T05:23:59.873269: step 146, loss 0.614454, acc 0.76\n",
      "current_step:  146\n",
      "2017-08-22T05:24:00.087483: step 147, loss 0.853069, acc 0.62\n",
      "current_step:  147\n",
      "2017-08-22T05:24:00.302789: step 148, loss 0.804458, acc 0.72\n",
      "current_step:  148\n",
      "2017-08-22T05:24:00.517558: step 149, loss 0.729331, acc 0.7\n",
      "current_step:  149\n",
      "2017-08-22T05:24:00.733200: step 150, loss 0.754725, acc 0.74\n",
      "current_step:  150\n",
      "2017-08-22T05:24:00.949232: step 151, loss 1.22938, acc 0.62\n",
      "current_step:  151\n",
      "2017-08-22T05:24:01.164255: step 152, loss 0.80779, acc 0.6\n",
      "current_step:  152\n",
      "2017-08-22T05:24:01.380031: step 153, loss 0.885631, acc 0.68\n",
      "current_step:  153\n",
      "2017-08-22T05:24:01.595388: step 154, loss 0.730589, acc 0.86\n",
      "current_step:  154\n",
      "2017-08-22T05:24:01.812152: step 155, loss 0.787046, acc 0.78\n",
      "current_step:  155\n",
      "2017-08-22T05:24:02.030019: step 156, loss 1.00251, acc 0.62\n",
      "current_step:  156\n",
      "2017-08-22T05:24:02.244886: step 157, loss 0.79586, acc 0.74\n",
      "current_step:  157\n",
      "2017-08-22T05:24:02.460623: step 158, loss 0.872055, acc 0.74\n",
      "current_step:  158\n",
      "2017-08-22T05:24:02.676588: step 159, loss 0.738459, acc 0.78\n",
      "current_step:  159\n",
      "2017-08-22T05:24:02.891797: step 160, loss 0.526765, acc 0.88\n",
      "current_step:  160\n",
      "2017-08-22T05:24:03.106069: step 161, loss 0.858426, acc 0.72\n",
      "current_step:  161\n",
      "2017-08-22T05:24:03.322166: step 162, loss 1.01533, acc 0.6\n",
      "current_step:  162\n",
      "2017-08-22T05:24:03.537235: step 163, loss 0.871848, acc 0.74\n",
      "current_step:  163\n",
      "2017-08-22T05:24:03.754055: step 164, loss 0.722767, acc 0.7\n",
      "current_step:  164\n",
      "2017-08-22T05:24:03.968304: step 165, loss 0.758834, acc 0.72\n",
      "current_step:  165\n",
      "2017-08-22T05:24:04.183753: step 166, loss 0.779075, acc 0.76\n",
      "current_step:  166\n",
      "2017-08-22T05:24:04.400714: step 167, loss 0.763067, acc 0.74\n",
      "current_step:  167\n",
      "2017-08-22T05:24:04.615435: step 168, loss 0.632891, acc 0.86\n",
      "current_step:  168\n",
      "2017-08-22T05:24:04.831541: step 169, loss 0.824716, acc 0.74\n",
      "current_step:  169\n",
      "2017-08-22T05:24:05.048282: step 170, loss 0.704802, acc 0.78\n",
      "current_step:  170\n",
      "2017-08-22T05:24:05.263534: step 171, loss 0.726393, acc 0.7\n",
      "current_step:  171\n",
      "2017-08-22T05:24:05.479697: step 172, loss 0.962762, acc 0.66\n",
      "current_step:  172\n",
      "2017-08-22T05:24:05.696828: step 173, loss 1.14434, acc 0.64\n",
      "current_step:  173\n",
      "2017-08-22T05:24:05.911988: step 174, loss 0.68748, acc 0.76\n",
      "current_step:  174\n",
      "2017-08-22T05:24:06.126999: step 175, loss 0.85715, acc 0.7\n",
      "current_step:  175\n",
      "2017-08-22T05:24:06.343646: step 176, loss 0.929247, acc 0.6\n",
      "current_step:  176\n",
      "2017-08-22T05:24:06.559874: step 177, loss 1.01629, acc 0.58\n",
      "current_step:  177\n",
      "2017-08-22T05:24:06.775618: step 178, loss 0.918681, acc 0.64\n",
      "current_step:  178\n",
      "2017-08-22T05:24:06.991548: step 179, loss 0.832671, acc 0.74\n",
      "current_step:  179\n",
      "2017-08-22T05:24:07.206823: step 180, loss 0.792001, acc 0.8\n",
      "current_step:  180\n",
      "2017-08-22T05:24:07.421547: step 181, loss 0.819192, acc 0.7\n",
      "current_step:  181\n",
      "2017-08-22T05:24:07.636253: step 182, loss 0.800098, acc 0.74\n",
      "current_step:  182\n",
      "2017-08-22T05:24:07.851581: step 183, loss 0.925578, acc 0.7\n",
      "current_step:  183\n",
      "2017-08-22T05:24:08.066624: step 184, loss 0.696315, acc 0.74\n",
      "current_step:  184\n",
      "2017-08-22T05:24:08.283406: step 185, loss 0.946958, acc 0.64\n",
      "current_step:  185\n",
      "2017-08-22T05:24:08.498424: step 186, loss 0.689466, acc 0.76\n",
      "current_step:  186\n",
      "2017-08-22T05:24:08.716319: step 187, loss 0.789478, acc 0.7\n",
      "current_step:  187\n",
      "2017-08-22T05:24:08.929729: step 188, loss 0.681029, acc 0.74\n",
      "current_step:  188\n",
      "2017-08-22T05:24:09.147736: step 189, loss 0.837699, acc 0.76\n",
      "current_step:  189\n",
      "2017-08-22T05:24:09.362894: step 190, loss 0.830139, acc 0.66\n",
      "current_step:  190\n",
      "2017-08-22T05:24:09.577335: step 191, loss 0.961273, acc 0.68\n",
      "current_step:  191\n",
      "2017-08-22T05:24:09.794702: step 192, loss 0.823946, acc 0.72\n",
      "current_step:  192\n",
      "2017-08-22T05:24:10.010450: step 193, loss 0.957754, acc 0.68\n",
      "current_step:  193\n",
      "2017-08-22T05:24:10.227447: step 194, loss 0.824348, acc 0.68\n",
      "current_step:  194\n",
      "2017-08-22T05:24:10.441855: step 195, loss 1.05047, acc 0.66\n",
      "current_step:  195\n",
      "2017-08-22T05:24:10.658384: step 196, loss 0.855855, acc 0.72\n",
      "current_step:  196\n",
      "2017-08-22T05:24:10.873709: step 197, loss 0.674801, acc 0.84\n",
      "current_step:  197\n",
      "2017-08-22T05:24:11.090358: step 198, loss 0.789053, acc 0.76\n",
      "current_step:  198\n",
      "2017-08-22T05:24:11.305864: step 199, loss 0.665216, acc 0.76\n",
      "current_step:  199\n",
      "2017-08-22T05:24:11.523195: step 200, loss 1.02325, acc 0.6\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:24:12.306442: step 200, loss 0.987782, acc 0.637011\n",
      "\n",
      "2017-08-22T05:24:12.520931: step 201, loss 0.730713, acc 0.76\n",
      "current_step:  201\n",
      "2017-08-22T05:24:12.738274: step 202, loss 0.722023, acc 0.72\n",
      "current_step:  202\n",
      "2017-08-22T05:24:12.953083: step 203, loss 0.845588, acc 0.76\n",
      "current_step:  203\n",
      "2017-08-22T05:24:12.995540: step 204, loss 1.47729, acc 0.666667\n",
      "current_step:  204\n",
      "2017-08-22T05:24:13.209918: step 205, loss 0.732377, acc 0.74\n",
      "current_step:  205\n",
      "2017-08-22T05:24:13.426096: step 206, loss 0.677455, acc 0.84\n",
      "current_step:  206\n",
      "2017-08-22T05:24:13.642187: step 207, loss 0.7014, acc 0.78\n",
      "current_step:  207\n",
      "2017-08-22T05:24:13.858338: step 208, loss 0.741715, acc 0.76\n",
      "current_step:  208\n",
      "2017-08-22T05:24:14.072651: step 209, loss 0.581988, acc 0.82\n",
      "current_step:  209\n",
      "2017-08-22T05:24:14.287358: step 210, loss 0.655487, acc 0.82\n",
      "current_step:  210\n",
      "2017-08-22T05:24:14.502919: step 211, loss 0.581066, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-22T05:24:14.718932: step 212, loss 0.787173, acc 0.7\n",
      "current_step:  212\n",
      "2017-08-22T05:24:14.933914: step 213, loss 0.70475, acc 0.72\n",
      "current_step:  213\n",
      "2017-08-22T05:24:15.149491: step 214, loss 0.594011, acc 0.8\n",
      "current_step:  214\n",
      "2017-08-22T05:24:15.363685: step 215, loss 0.458911, acc 0.88\n",
      "current_step:  215\n",
      "2017-08-22T05:24:15.578382: step 216, loss 0.535888, acc 0.9\n",
      "current_step:  216\n",
      "2017-08-22T05:24:15.794156: step 217, loss 0.659722, acc 0.76\n",
      "current_step:  217\n",
      "2017-08-22T05:24:16.009284: step 218, loss 0.610909, acc 0.86\n",
      "current_step:  218\n",
      "2017-08-22T05:24:16.224240: step 219, loss 0.656686, acc 0.8\n",
      "current_step:  219\n",
      "2017-08-22T05:24:16.439451: step 220, loss 0.815883, acc 0.66\n",
      "current_step:  220\n",
      "2017-08-22T05:24:16.655100: step 221, loss 0.774882, acc 0.78\n",
      "current_step:  221\n",
      "2017-08-22T05:24:16.872423: step 222, loss 0.513675, acc 0.86\n",
      "current_step:  222\n",
      "2017-08-22T05:24:17.085547: step 223, loss 0.510916, acc 0.86\n",
      "current_step:  223\n",
      "2017-08-22T05:24:17.301065: step 224, loss 0.5095, acc 0.82\n",
      "current_step:  224\n",
      "2017-08-22T05:24:17.515042: step 225, loss 0.508617, acc 0.86\n",
      "current_step:  225\n",
      "2017-08-22T05:24:17.731508: step 226, loss 0.703282, acc 0.8\n",
      "current_step:  226\n",
      "2017-08-22T05:24:17.946176: step 227, loss 0.879098, acc 0.68\n",
      "current_step:  227\n",
      "2017-08-22T05:24:18.161092: step 228, loss 0.521647, acc 0.78\n",
      "current_step:  228\n",
      "2017-08-22T05:24:18.376336: step 229, loss 0.680722, acc 0.74\n",
      "current_step:  229\n",
      "2017-08-22T05:24:18.593021: step 230, loss 0.507744, acc 0.8\n",
      "current_step:  230\n",
      "2017-08-22T05:24:18.807464: step 231, loss 0.841474, acc 0.74\n",
      "current_step:  231\n",
      "2017-08-22T05:24:19.021795: step 232, loss 0.662664, acc 0.74\n",
      "current_step:  232\n",
      "2017-08-22T05:24:19.238052: step 233, loss 0.573492, acc 0.84\n",
      "current_step:  233\n",
      "2017-08-22T05:24:19.452995: step 234, loss 0.504121, acc 0.84\n",
      "current_step:  234\n",
      "2017-08-22T05:24:19.670734: step 235, loss 0.485555, acc 0.86\n",
      "current_step:  235\n",
      "2017-08-22T05:24:19.886388: step 236, loss 0.646657, acc 0.78\n",
      "current_step:  236\n",
      "2017-08-22T05:24:20.101560: step 237, loss 0.602569, acc 0.78\n",
      "current_step:  237\n",
      "2017-08-22T05:24:20.317038: step 238, loss 0.802776, acc 0.74\n",
      "current_step:  238\n",
      "2017-08-22T05:24:20.533718: step 239, loss 0.61922, acc 0.8\n",
      "current_step:  239\n",
      "2017-08-22T05:24:20.747122: step 240, loss 0.810131, acc 0.72\n",
      "current_step:  240\n",
      "2017-08-22T05:24:20.962874: step 241, loss 0.72346, acc 0.72\n",
      "current_step:  241\n",
      "2017-08-22T05:24:21.176564: step 242, loss 0.561578, acc 0.82\n",
      "current_step:  242\n",
      "2017-08-22T05:24:21.393150: step 243, loss 0.563595, acc 0.88\n",
      "current_step:  243\n",
      "2017-08-22T05:24:21.609046: step 244, loss 0.565242, acc 0.86\n",
      "current_step:  244\n",
      "2017-08-22T05:24:21.826275: step 245, loss 0.590673, acc 0.8\n",
      "current_step:  245\n",
      "2017-08-22T05:24:22.041727: step 246, loss 0.646559, acc 0.84\n",
      "current_step:  246\n",
      "2017-08-22T05:24:22.257647: step 247, loss 0.680853, acc 0.82\n",
      "current_step:  247\n",
      "2017-08-22T05:24:22.473015: step 248, loss 0.66892, acc 0.78\n",
      "current_step:  248\n",
      "2017-08-22T05:24:22.688584: step 249, loss 0.999163, acc 0.66\n",
      "current_step:  249\n",
      "2017-08-22T05:24:22.903643: step 250, loss 0.634461, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-22T05:24:23.119421: step 251, loss 0.732032, acc 0.8\n",
      "current_step:  251\n",
      "2017-08-22T05:24:23.335284: step 252, loss 0.484569, acc 0.86\n",
      "current_step:  252\n",
      "2017-08-22T05:24:23.551982: step 253, loss 0.570829, acc 0.82\n",
      "current_step:  253\n",
      "2017-08-22T05:24:23.767978: step 254, loss 0.602822, acc 0.82\n",
      "current_step:  254\n",
      "2017-08-22T05:24:23.983350: step 255, loss 0.667188, acc 0.84\n",
      "current_step:  255\n",
      "2017-08-22T05:24:24.197203: step 256, loss 0.589504, acc 0.76\n",
      "current_step:  256\n",
      "2017-08-22T05:24:24.412411: step 257, loss 0.676268, acc 0.78\n",
      "current_step:  257\n",
      "2017-08-22T05:24:24.627805: step 258, loss 0.598405, acc 0.74\n",
      "current_step:  258\n",
      "2017-08-22T05:24:24.844492: step 259, loss 0.575311, acc 0.82\n",
      "current_step:  259\n",
      "2017-08-22T05:24:25.059263: step 260, loss 0.678983, acc 0.72\n",
      "current_step:  260\n",
      "2017-08-22T05:24:25.274939: step 261, loss 0.830761, acc 0.72\n",
      "current_step:  261\n",
      "2017-08-22T05:24:25.490272: step 262, loss 0.778754, acc 0.76\n",
      "current_step:  262\n",
      "2017-08-22T05:24:25.706095: step 263, loss 0.436119, acc 0.92\n",
      "current_step:  263\n",
      "2017-08-22T05:24:25.921063: step 264, loss 0.476901, acc 0.88\n",
      "current_step:  264\n",
      "2017-08-22T05:24:26.135933: step 265, loss 0.628803, acc 0.76\n",
      "current_step:  265\n",
      "2017-08-22T05:24:26.351744: step 266, loss 0.464781, acc 0.88\n",
      "current_step:  266\n",
      "2017-08-22T05:24:26.567569: step 267, loss 0.548756, acc 0.82\n",
      "current_step:  267\n",
      "2017-08-22T05:24:26.783821: step 268, loss 0.558624, acc 0.86\n",
      "current_step:  268\n",
      "2017-08-22T05:24:26.996748: step 269, loss 0.772443, acc 0.7\n",
      "current_step:  269\n",
      "2017-08-22T05:24:27.210722: step 270, loss 0.50953, acc 0.84\n",
      "current_step:  270\n",
      "2017-08-22T05:24:27.426353: step 271, loss 0.652515, acc 0.76\n",
      "current_step:  271\n",
      "2017-08-22T05:24:27.642072: step 272, loss 0.595481, acc 0.8\n",
      "current_step:  272\n",
      "2017-08-22T05:24:27.856864: step 273, loss 0.65114, acc 0.74\n",
      "current_step:  273\n",
      "2017-08-22T05:24:28.072535: step 274, loss 0.723819, acc 0.82\n",
      "current_step:  274\n",
      "2017-08-22T05:24:28.287746: step 275, loss 0.687873, acc 0.82\n",
      "current_step:  275\n",
      "2017-08-22T05:24:28.504450: step 276, loss 0.730716, acc 0.76\n",
      "current_step:  276\n",
      "2017-08-22T05:24:28.719799: step 277, loss 0.788016, acc 0.72\n",
      "current_step:  277\n",
      "2017-08-22T05:24:28.934231: step 278, loss 0.737249, acc 0.82\n",
      "current_step:  278\n",
      "2017-08-22T05:24:29.150589: step 279, loss 0.565683, acc 0.8\n",
      "current_step:  279\n",
      "2017-08-22T05:24:29.366052: step 280, loss 0.599855, acc 0.78\n",
      "current_step:  280\n",
      "2017-08-22T05:24:29.581467: step 281, loss 0.709372, acc 0.72\n",
      "current_step:  281\n",
      "2017-08-22T05:24:29.801204: step 282, loss 0.795033, acc 0.68\n",
      "current_step:  282\n",
      "2017-08-22T05:24:30.019033: step 283, loss 0.560562, acc 0.78\n",
      "current_step:  283\n",
      "2017-08-22T05:24:30.235082: step 284, loss 0.586707, acc 0.84\n",
      "current_step:  284\n",
      "2017-08-22T05:24:30.451338: step 285, loss 0.594063, acc 0.74\n",
      "current_step:  285\n",
      "2017-08-22T05:24:30.667960: step 286, loss 0.513379, acc 0.86\n",
      "current_step:  286\n",
      "2017-08-22T05:24:30.882632: step 287, loss 0.64615, acc 0.78\n",
      "current_step:  287\n",
      "2017-08-22T05:24:31.099232: step 288, loss 0.824515, acc 0.76\n",
      "current_step:  288\n",
      "2017-08-22T05:24:31.312865: step 289, loss 0.8066, acc 0.7\n",
      "current_step:  289\n",
      "2017-08-22T05:24:31.530091: step 290, loss 0.470122, acc 0.9\n",
      "current_step:  290\n",
      "2017-08-22T05:24:31.752529: step 291, loss 0.608207, acc 0.8\n",
      "current_step:  291\n",
      "2017-08-22T05:24:31.974059: step 292, loss 0.610003, acc 0.78\n",
      "current_step:  292\n",
      "2017-08-22T05:24:32.189865: step 293, loss 0.784725, acc 0.72\n",
      "current_step:  293\n",
      "2017-08-22T05:24:32.403507: step 294, loss 0.709835, acc 0.76\n",
      "current_step:  294\n",
      "2017-08-22T05:24:32.619376: step 295, loss 0.690687, acc 0.78\n",
      "current_step:  295\n",
      "2017-08-22T05:24:32.834688: step 296, loss 0.628191, acc 0.74\n",
      "current_step:  296\n",
      "2017-08-22T05:24:33.051190: step 297, loss 0.784596, acc 0.72\n",
      "current_step:  297\n",
      "2017-08-22T05:24:33.266725: step 298, loss 0.673783, acc 0.8\n",
      "current_step:  298\n",
      "2017-08-22T05:24:33.482045: step 299, loss 0.608188, acc 0.76\n",
      "current_step:  299\n",
      "2017-08-22T05:24:33.697308: step 300, loss 0.559421, acc 0.78\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:24:34.482860: step 300, loss 0.919054, acc 0.679715\n",
      "\n",
      "2017-08-22T05:24:34.699329: step 301, loss 0.683863, acc 0.78\n",
      "current_step:  301\n",
      "2017-08-22T05:24:34.915227: step 302, loss 0.836514, acc 0.7\n",
      "current_step:  302\n",
      "2017-08-22T05:24:35.130371: step 303, loss 0.592611, acc 0.84\n",
      "current_step:  303\n",
      "2017-08-22T05:24:35.346727: step 304, loss 0.709663, acc 0.78\n",
      "current_step:  304\n",
      "2017-08-22T05:24:35.562971: step 305, loss 0.526626, acc 0.9\n",
      "current_step:  305\n",
      "2017-08-22T05:24:35.605745: step 306, loss 0.363826, acc 0.888889\n",
      "current_step:  306\n",
      "2017-08-22T05:24:35.824307: step 307, loss 0.497301, acc 0.9\n",
      "current_step:  307\n",
      "2017-08-22T05:24:36.039698: step 308, loss 0.543505, acc 0.8\n",
      "current_step:  308\n",
      "2017-08-22T05:24:36.254420: step 309, loss 0.570472, acc 0.8\n",
      "current_step:  309\n",
      "2017-08-22T05:24:36.469180: step 310, loss 0.536995, acc 0.84\n",
      "current_step:  310\n",
      "2017-08-22T05:24:36.685907: step 311, loss 0.371478, acc 0.94\n",
      "current_step:  311\n",
      "2017-08-22T05:24:36.901283: step 312, loss 0.628136, acc 0.82\n",
      "current_step:  312\n",
      "2017-08-22T05:24:37.116699: step 313, loss 0.431987, acc 0.9\n",
      "current_step:  313\n",
      "2017-08-22T05:24:37.332533: step 314, loss 0.424335, acc 0.92\n",
      "current_step:  314\n",
      "2017-08-22T05:24:37.547909: step 315, loss 0.462574, acc 0.9\n",
      "current_step:  315\n",
      "2017-08-22T05:24:37.765046: step 316, loss 0.675879, acc 0.8\n",
      "current_step:  316\n",
      "2017-08-22T05:24:37.980621: step 317, loss 0.475644, acc 0.84\n",
      "current_step:  317\n",
      "2017-08-22T05:24:38.196172: step 318, loss 0.418136, acc 0.88\n",
      "current_step:  318\n",
      "2017-08-22T05:24:38.411194: step 319, loss 0.511355, acc 0.86\n",
      "current_step:  319\n",
      "2017-08-22T05:24:38.627574: step 320, loss 0.477877, acc 0.88\n",
      "current_step:  320\n",
      "2017-08-22T05:24:38.843239: step 321, loss 0.744057, acc 0.76\n",
      "current_step:  321\n",
      "2017-08-22T05:24:39.058468: step 322, loss 0.482731, acc 0.84\n",
      "current_step:  322\n",
      "2017-08-22T05:24:39.273747: step 323, loss 0.359351, acc 0.88\n",
      "current_step:  323\n",
      "2017-08-22T05:24:39.492171: step 324, loss 0.408486, acc 0.9\n",
      "current_step:  324\n",
      "2017-08-22T05:24:39.709674: step 325, loss 0.549273, acc 0.76\n",
      "current_step:  325\n",
      "2017-08-22T05:24:39.925511: step 326, loss 0.47909, acc 0.9\n",
      "current_step:  326\n",
      "2017-08-22T05:24:40.140842: step 327, loss 0.473235, acc 0.86\n",
      "current_step:  327\n",
      "2017-08-22T05:24:40.356050: step 328, loss 0.582148, acc 0.8\n",
      "current_step:  328\n",
      "2017-08-22T05:24:40.570542: step 329, loss 0.42406, acc 0.86\n",
      "current_step:  329\n",
      "2017-08-22T05:24:40.787980: step 330, loss 0.467205, acc 0.84\n",
      "current_step:  330\n",
      "2017-08-22T05:24:41.003412: step 331, loss 0.688879, acc 0.74\n",
      "current_step:  331\n",
      "2017-08-22T05:24:41.218043: step 332, loss 0.472562, acc 0.86\n",
      "current_step:  332\n",
      "2017-08-22T05:24:41.435103: step 333, loss 0.489592, acc 0.84\n",
      "current_step:  333\n",
      "2017-08-22T05:24:41.649919: step 334, loss 0.488548, acc 0.82\n",
      "current_step:  334\n",
      "2017-08-22T05:24:41.865248: step 335, loss 0.584342, acc 0.76\n",
      "current_step:  335\n",
      "2017-08-22T05:24:42.079141: step 336, loss 0.537145, acc 0.86\n",
      "current_step:  336\n",
      "2017-08-22T05:24:42.294214: step 337, loss 0.605625, acc 0.78\n",
      "current_step:  337\n",
      "2017-08-22T05:24:42.509150: step 338, loss 0.61331, acc 0.8\n",
      "current_step:  338\n",
      "2017-08-22T05:24:42.725813: step 339, loss 0.441826, acc 0.88\n",
      "current_step:  339\n",
      "2017-08-22T05:24:42.942014: step 340, loss 0.459629, acc 0.9\n",
      "current_step:  340\n",
      "2017-08-22T05:24:43.156961: step 341, loss 0.402861, acc 0.88\n",
      "current_step:  341\n",
      "2017-08-22T05:24:43.372181: step 342, loss 0.27588, acc 0.96\n",
      "current_step:  342\n",
      "2017-08-22T05:24:43.586740: step 343, loss 0.673265, acc 0.8\n",
      "current_step:  343\n",
      "2017-08-22T05:24:43.803311: step 344, loss 0.494068, acc 0.82\n",
      "current_step:  344\n",
      "2017-08-22T05:24:44.018914: step 345, loss 0.390169, acc 0.88\n",
      "current_step:  345\n",
      "2017-08-22T05:24:44.235481: step 346, loss 0.476492, acc 0.86\n",
      "current_step:  346\n",
      "2017-08-22T05:24:44.451561: step 347, loss 0.508261, acc 0.8\n",
      "current_step:  347\n",
      "2017-08-22T05:24:44.669192: step 348, loss 0.463223, acc 0.88\n",
      "current_step:  348\n",
      "2017-08-22T05:24:44.884398: step 349, loss 0.568184, acc 0.82\n",
      "current_step:  349\n",
      "2017-08-22T05:24:45.099864: step 350, loss 0.613687, acc 0.86\n",
      "current_step:  350\n",
      "2017-08-22T05:24:45.314998: step 351, loss 0.374704, acc 0.9\n",
      "current_step:  351\n",
      "2017-08-22T05:24:45.532250: step 352, loss 0.484922, acc 0.84\n",
      "current_step:  352\n",
      "2017-08-22T05:24:45.750946: step 353, loss 0.535575, acc 0.92\n",
      "current_step:  353\n",
      "2017-08-22T05:24:45.966394: step 354, loss 0.428154, acc 0.9\n",
      "current_step:  354\n",
      "2017-08-22T05:24:46.183181: step 355, loss 0.474409, acc 0.84\n",
      "current_step:  355\n",
      "2017-08-22T05:24:46.398866: step 356, loss 0.57584, acc 0.82\n",
      "current_step:  356\n",
      "2017-08-22T05:24:46.614530: step 357, loss 0.230581, acc 0.98\n",
      "current_step:  357\n",
      "2017-08-22T05:24:46.829888: step 358, loss 0.596995, acc 0.78\n",
      "current_step:  358\n",
      "2017-08-22T05:24:47.045170: step 359, loss 0.598282, acc 0.78\n",
      "current_step:  359\n",
      "2017-08-22T05:24:47.261049: step 360, loss 0.71017, acc 0.78\n",
      "current_step:  360\n",
      "2017-08-22T05:24:47.478029: step 361, loss 0.625666, acc 0.82\n",
      "current_step:  361\n",
      "2017-08-22T05:24:47.698806: step 362, loss 0.48978, acc 0.88\n",
      "current_step:  362\n",
      "2017-08-22T05:24:47.914804: step 363, loss 0.370655, acc 0.94\n",
      "current_step:  363\n",
      "2017-08-22T05:24:48.129676: step 364, loss 0.577422, acc 0.78\n",
      "current_step:  364\n",
      "2017-08-22T05:24:48.346546: step 365, loss 0.388238, acc 0.9\n",
      "current_step:  365\n",
      "2017-08-22T05:24:48.565146: step 366, loss 0.690586, acc 0.8\n",
      "current_step:  366\n",
      "2017-08-22T05:24:48.780645: step 367, loss 0.595349, acc 0.72\n",
      "current_step:  367\n",
      "2017-08-22T05:24:48.995903: step 368, loss 0.486354, acc 0.88\n",
      "current_step:  368\n",
      "2017-08-22T05:24:49.211086: step 369, loss 0.367234, acc 0.94\n",
      "current_step:  369\n",
      "2017-08-22T05:24:49.428041: step 370, loss 0.451057, acc 0.86\n",
      "current_step:  370\n",
      "2017-08-22T05:24:49.645244: step 371, loss 0.542697, acc 0.82\n",
      "current_step:  371\n",
      "2017-08-22T05:24:49.863340: step 372, loss 0.529537, acc 0.8\n",
      "current_step:  372\n",
      "2017-08-22T05:24:50.078891: step 373, loss 0.482962, acc 0.86\n",
      "current_step:  373\n",
      "2017-08-22T05:24:50.295041: step 374, loss 0.522912, acc 0.9\n",
      "current_step:  374\n",
      "2017-08-22T05:24:50.510399: step 375, loss 0.531715, acc 0.84\n",
      "current_step:  375\n",
      "2017-08-22T05:24:50.725452: step 376, loss 0.68464, acc 0.78\n",
      "current_step:  376\n",
      "2017-08-22T05:24:50.941006: step 377, loss 0.542758, acc 0.8\n",
      "current_step:  377\n",
      "2017-08-22T05:24:51.156086: step 378, loss 0.481761, acc 0.82\n",
      "current_step:  378\n",
      "2017-08-22T05:24:51.371678: step 379, loss 0.463199, acc 0.8\n",
      "current_step:  379\n",
      "2017-08-22T05:24:51.587383: step 380, loss 0.432961, acc 0.86\n",
      "current_step:  380\n",
      "2017-08-22T05:24:51.805135: step 381, loss 0.591374, acc 0.82\n",
      "current_step:  381\n",
      "2017-08-22T05:24:52.020938: step 382, loss 0.471058, acc 0.84\n",
      "current_step:  382\n",
      "2017-08-22T05:24:52.237544: step 383, loss 0.50148, acc 0.86\n",
      "current_step:  383\n",
      "2017-08-22T05:24:52.454363: step 384, loss 0.555079, acc 0.8\n",
      "current_step:  384\n",
      "2017-08-22T05:24:52.671196: step 385, loss 0.463215, acc 0.86\n",
      "current_step:  385\n",
      "2017-08-22T05:24:52.885433: step 386, loss 0.50819, acc 0.84\n",
      "current_step:  386\n",
      "2017-08-22T05:24:53.100701: step 387, loss 0.549762, acc 0.86\n",
      "current_step:  387\n",
      "2017-08-22T05:24:53.316716: step 388, loss 0.37738, acc 0.88\n",
      "current_step:  388\n",
      "2017-08-22T05:24:53.532745: step 389, loss 0.81887, acc 0.64\n",
      "current_step:  389\n",
      "2017-08-22T05:24:53.749768: step 390, loss 0.516114, acc 0.86\n",
      "current_step:  390\n",
      "2017-08-22T05:24:53.966370: step 391, loss 0.566328, acc 0.82\n",
      "current_step:  391\n",
      "2017-08-22T05:24:54.181860: step 392, loss 0.507178, acc 0.88\n",
      "current_step:  392\n",
      "2017-08-22T05:24:54.399878: step 393, loss 0.546116, acc 0.9\n",
      "current_step:  393\n",
      "2017-08-22T05:24:54.617446: step 394, loss 0.515119, acc 0.84\n",
      "current_step:  394\n",
      "2017-08-22T05:24:54.834769: step 395, loss 0.410629, acc 0.96\n",
      "current_step:  395\n",
      "2017-08-22T05:24:55.051074: step 396, loss 0.604352, acc 0.84\n",
      "current_step:  396\n",
      "2017-08-22T05:24:55.265931: step 397, loss 0.525886, acc 0.84\n",
      "current_step:  397\n",
      "2017-08-22T05:24:55.481999: step 398, loss 0.500532, acc 0.86\n",
      "current_step:  398\n",
      "2017-08-22T05:24:55.699459: step 399, loss 0.618152, acc 0.76\n",
      "current_step:  399\n",
      "2017-08-22T05:24:55.915652: step 400, loss 0.429789, acc 0.86\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:24:56.700900: step 400, loss 0.879093, acc 0.704626\n",
      "\n",
      "2017-08-22T05:24:56.915151: step 401, loss 0.580836, acc 0.84\n",
      "current_step:  401\n",
      "2017-08-22T05:24:57.129877: step 402, loss 0.558967, acc 0.84\n",
      "current_step:  402\n",
      "2017-08-22T05:24:57.345710: step 403, loss 0.644982, acc 0.76\n",
      "current_step:  403\n",
      "2017-08-22T05:24:57.563444: step 404, loss 0.478359, acc 0.9\n",
      "current_step:  404\n",
      "2017-08-22T05:24:57.779663: step 405, loss 0.579601, acc 0.86\n",
      "current_step:  405\n",
      "2017-08-22T05:24:57.994834: step 406, loss 0.533321, acc 0.86\n",
      "current_step:  406\n",
      "2017-08-22T05:24:58.211177: step 407, loss 0.663736, acc 0.84\n",
      "current_step:  407\n",
      "2017-08-22T05:24:58.254332: step 408, loss 0.333889, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-22T05:24:58.470369: step 409, loss 0.307503, acc 0.98\n",
      "current_step:  409\n",
      "2017-08-22T05:24:58.687780: step 410, loss 0.454404, acc 0.88\n",
      "current_step:  410\n",
      "2017-08-22T05:24:58.903889: step 411, loss 0.495879, acc 0.88\n",
      "current_step:  411\n",
      "2017-08-22T05:24:59.120597: step 412, loss 0.46997, acc 0.86\n",
      "current_step:  412\n",
      "2017-08-22T05:24:59.335767: step 413, loss 0.359036, acc 0.9\n",
      "current_step:  413\n",
      "2017-08-22T05:24:59.550586: step 414, loss 0.388789, acc 0.88\n",
      "current_step:  414\n",
      "2017-08-22T05:24:59.766197: step 415, loss 0.47804, acc 0.86\n",
      "current_step:  415\n",
      "2017-08-22T05:24:59.981211: step 416, loss 0.432849, acc 0.9\n",
      "current_step:  416\n",
      "2017-08-22T05:25:00.196977: step 417, loss 0.335479, acc 0.92\n",
      "current_step:  417\n",
      "2017-08-22T05:25:00.412938: step 418, loss 0.395896, acc 0.9\n",
      "current_step:  418\n",
      "2017-08-22T05:25:00.629576: step 419, loss 0.469195, acc 0.86\n",
      "current_step:  419\n",
      "2017-08-22T05:25:00.847043: step 420, loss 0.360829, acc 0.96\n",
      "current_step:  420\n",
      "2017-08-22T05:25:01.061200: step 421, loss 0.30905, acc 0.96\n",
      "current_step:  421\n",
      "2017-08-22T05:25:01.277340: step 422, loss 0.378305, acc 0.92\n",
      "current_step:  422\n",
      "2017-08-22T05:25:01.494474: step 423, loss 0.516042, acc 0.86\n",
      "current_step:  423\n",
      "2017-08-22T05:25:01.711402: step 424, loss 0.370701, acc 0.96\n",
      "current_step:  424\n",
      "2017-08-22T05:25:01.928208: step 425, loss 0.25737, acc 0.96\n",
      "current_step:  425\n",
      "2017-08-22T05:25:02.145000: step 426, loss 0.437726, acc 0.86\n",
      "current_step:  426\n",
      "2017-08-22T05:25:02.360812: step 427, loss 0.322543, acc 0.9\n",
      "current_step:  427\n",
      "2017-08-22T05:25:02.575674: step 428, loss 0.332979, acc 0.92\n",
      "current_step:  428\n",
      "2017-08-22T05:25:02.791658: step 429, loss 0.501098, acc 0.8\n",
      "current_step:  429\n",
      "2017-08-22T05:25:03.005483: step 430, loss 0.509577, acc 0.84\n",
      "current_step:  430\n",
      "2017-08-22T05:25:03.221285: step 431, loss 0.511606, acc 0.82\n",
      "current_step:  431\n",
      "2017-08-22T05:25:03.436520: step 432, loss 0.389962, acc 0.88\n",
      "current_step:  432\n",
      "2017-08-22T05:25:03.651180: step 433, loss 0.414465, acc 0.92\n",
      "current_step:  433\n",
      "2017-08-22T05:25:03.869089: step 434, loss 0.455121, acc 0.86\n",
      "current_step:  434\n",
      "2017-08-22T05:25:04.083733: step 435, loss 0.350602, acc 0.96\n",
      "current_step:  435\n",
      "2017-08-22T05:25:04.299391: step 436, loss 0.487473, acc 0.86\n",
      "current_step:  436\n",
      "2017-08-22T05:25:04.515146: step 437, loss 0.383698, acc 0.92\n",
      "current_step:  437\n",
      "2017-08-22T05:25:04.732018: step 438, loss 0.325188, acc 0.98\n",
      "current_step:  438\n",
      "2017-08-22T05:25:04.948117: step 439, loss 0.314924, acc 0.92\n",
      "current_step:  439\n",
      "2017-08-22T05:25:05.164964: step 440, loss 0.364033, acc 0.9\n",
      "current_step:  440\n",
      "2017-08-22T05:25:05.380093: step 441, loss 0.249703, acc 0.98\n",
      "current_step:  441\n",
      "2017-08-22T05:25:05.594499: step 442, loss 0.45084, acc 0.9\n",
      "current_step:  442\n",
      "2017-08-22T05:25:05.813096: step 443, loss 0.365584, acc 0.9\n",
      "current_step:  443\n",
      "2017-08-22T05:25:06.029701: step 444, loss 0.326919, acc 0.92\n",
      "current_step:  444\n",
      "2017-08-22T05:25:06.244277: step 445, loss 0.46112, acc 0.84\n",
      "current_step:  445\n",
      "2017-08-22T05:25:06.459833: step 446, loss 0.346411, acc 0.88\n",
      "current_step:  446\n",
      "2017-08-22T05:25:06.675913: step 447, loss 0.346272, acc 0.94\n",
      "current_step:  447\n",
      "2017-08-22T05:25:06.893754: step 448, loss 0.529417, acc 0.82\n",
      "current_step:  448\n",
      "2017-08-22T05:25:07.109949: step 449, loss 0.350118, acc 0.94\n",
      "current_step:  449\n",
      "2017-08-22T05:25:07.326609: step 450, loss 0.480764, acc 0.8\n",
      "current_step:  450\n",
      "2017-08-22T05:25:07.543810: step 451, loss 0.432563, acc 0.88\n",
      "current_step:  451\n",
      "2017-08-22T05:25:07.764013: step 452, loss 0.439375, acc 0.92\n",
      "current_step:  452\n",
      "2017-08-22T05:25:07.979987: step 453, loss 0.578551, acc 0.84\n",
      "current_step:  453\n",
      "2017-08-22T05:25:08.196248: step 454, loss 0.326753, acc 0.98\n",
      "current_step:  454\n",
      "2017-08-22T05:25:08.411411: step 455, loss 0.366674, acc 0.94\n",
      "current_step:  455\n",
      "2017-08-22T05:25:08.629563: step 456, loss 0.540758, acc 0.92\n",
      "current_step:  456\n",
      "2017-08-22T05:25:08.847031: step 457, loss 0.346888, acc 0.88\n",
      "current_step:  457\n",
      "2017-08-22T05:25:09.062543: step 458, loss 0.407105, acc 0.88\n",
      "current_step:  458\n",
      "2017-08-22T05:25:09.278164: step 459, loss 0.37148, acc 0.88\n",
      "current_step:  459\n",
      "2017-08-22T05:25:09.492987: step 460, loss 0.3997, acc 0.9\n",
      "current_step:  460\n",
      "2017-08-22T05:25:09.710403: step 461, loss 0.374933, acc 0.86\n",
      "current_step:  461\n",
      "2017-08-22T05:25:09.925162: step 462, loss 0.446039, acc 0.92\n",
      "current_step:  462\n",
      "2017-08-22T05:25:10.141465: step 463, loss 0.292602, acc 0.92\n",
      "current_step:  463\n",
      "2017-08-22T05:25:10.356930: step 464, loss 0.39654, acc 0.96\n",
      "current_step:  464\n",
      "2017-08-22T05:25:10.572951: step 465, loss 0.469264, acc 0.86\n",
      "current_step:  465\n",
      "2017-08-22T05:25:10.789380: step 466, loss 0.328509, acc 0.94\n",
      "current_step:  466\n",
      "2017-08-22T05:25:11.004033: step 467, loss 0.538499, acc 0.8\n",
      "current_step:  467\n",
      "2017-08-22T05:25:11.219933: step 468, loss 0.387734, acc 0.9\n",
      "current_step:  468\n",
      "2017-08-22T05:25:11.436966: step 469, loss 0.453793, acc 0.9\n",
      "current_step:  469\n",
      "2017-08-22T05:25:11.652406: step 470, loss 0.378659, acc 0.94\n",
      "current_step:  470\n",
      "2017-08-22T05:25:11.868513: step 471, loss 0.432825, acc 0.9\n",
      "current_step:  471\n",
      "2017-08-22T05:25:12.082884: step 472, loss 0.411458, acc 0.92\n",
      "current_step:  472\n",
      "2017-08-22T05:25:12.299446: step 473, loss 0.31322, acc 0.94\n",
      "current_step:  473\n",
      "2017-08-22T05:25:12.514393: step 474, loss 0.432167, acc 0.92\n",
      "current_step:  474\n",
      "2017-08-22T05:25:12.729901: step 475, loss 0.477178, acc 0.84\n",
      "current_step:  475\n",
      "2017-08-22T05:25:12.945588: step 476, loss 0.511352, acc 0.86\n",
      "current_step:  476\n",
      "2017-08-22T05:25:13.161382: step 477, loss 0.440295, acc 0.86\n",
      "current_step:  477\n",
      "2017-08-22T05:25:13.376727: step 478, loss 0.379976, acc 0.92\n",
      "current_step:  478\n",
      "2017-08-22T05:25:13.592563: step 479, loss 0.54853, acc 0.88\n",
      "current_step:  479\n",
      "2017-08-22T05:25:13.808039: step 480, loss 0.452257, acc 0.82\n",
      "current_step:  480\n",
      "2017-08-22T05:25:14.024034: step 481, loss 0.404603, acc 0.9\n",
      "current_step:  481\n",
      "2017-08-22T05:25:14.240147: step 482, loss 0.452286, acc 0.84\n",
      "current_step:  482\n",
      "2017-08-22T05:25:14.455126: step 483, loss 0.573653, acc 0.82\n",
      "current_step:  483\n",
      "2017-08-22T05:25:14.671973: step 484, loss 0.392008, acc 0.92\n",
      "current_step:  484\n",
      "2017-08-22T05:25:14.887924: step 485, loss 0.432128, acc 0.9\n",
      "current_step:  485\n",
      "2017-08-22T05:25:15.103172: step 486, loss 0.470353, acc 0.9\n",
      "current_step:  486\n",
      "2017-08-22T05:25:15.317451: step 487, loss 0.40628, acc 0.92\n",
      "current_step:  487\n",
      "2017-08-22T05:25:15.533561: step 488, loss 0.389128, acc 0.9\n",
      "current_step:  488\n",
      "2017-08-22T05:25:15.749705: step 489, loss 0.438997, acc 0.92\n",
      "current_step:  489\n",
      "2017-08-22T05:25:15.966126: step 490, loss 0.468549, acc 0.84\n",
      "current_step:  490\n",
      "2017-08-22T05:25:16.180192: step 491, loss 0.253094, acc 0.96\n",
      "current_step:  491\n",
      "2017-08-22T05:25:16.394848: step 492, loss 0.385709, acc 0.9\n",
      "current_step:  492\n",
      "2017-08-22T05:25:16.612754: step 493, loss 0.499989, acc 0.9\n",
      "current_step:  493\n",
      "2017-08-22T05:25:16.829071: step 494, loss 0.595069, acc 0.84\n",
      "current_step:  494\n",
      "2017-08-22T05:25:17.046313: step 495, loss 0.349062, acc 0.92\n",
      "current_step:  495\n",
      "2017-08-22T05:25:17.263054: step 496, loss 0.430608, acc 0.86\n",
      "current_step:  496\n",
      "2017-08-22T05:25:17.478598: step 497, loss 0.453017, acc 0.86\n",
      "current_step:  497\n",
      "2017-08-22T05:25:17.696879: step 498, loss 0.454976, acc 0.78\n",
      "current_step:  498\n",
      "2017-08-22T05:25:17.912212: step 499, loss 0.496256, acc 0.8\n",
      "current_step:  499\n",
      "2017-08-22T05:25:18.129471: step 500, loss 0.370267, acc 0.9\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:25:18.910940: step 500, loss 0.867132, acc 0.706406\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0523_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:25:19.195077: step 501, loss 0.334158, acc 0.86\n",
      "current_step:  501\n",
      "2017-08-22T05:25:19.409894: step 502, loss 0.376498, acc 0.94\n",
      "current_step:  502\n",
      "2017-08-22T05:25:19.625585: step 503, loss 0.34732, acc 0.88\n",
      "current_step:  503\n",
      "2017-08-22T05:25:19.841941: step 504, loss 0.428544, acc 0.9\n",
      "current_step:  504\n",
      "2017-08-22T05:25:20.062996: step 505, loss 0.362891, acc 0.92\n",
      "current_step:  505\n",
      "2017-08-22T05:25:20.278779: step 506, loss 0.389705, acc 0.92\n",
      "current_step:  506\n",
      "2017-08-22T05:25:20.494126: step 507, loss 0.42056, acc 0.9\n",
      "current_step:  507\n",
      "2017-08-22T05:25:20.710425: step 508, loss 0.409918, acc 0.9\n",
      "current_step:  508\n",
      "2017-08-22T05:25:20.926048: step 509, loss 0.40636, acc 0.86\n",
      "current_step:  509\n",
      "2017-08-22T05:25:20.968720: step 510, loss 0.421292, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.627336\n",
      "Precision Score = 0.605718\n",
      "Recall Score = 0.663701\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.611237\n",
      "Precision Score = 0.613497\n",
      "Recall Score = 0.628114\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.657893\n",
      "Precision Score = 0.645174\n",
      "Recall Score = 0.672598\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.661276\n",
      "Precision Score = 0.644308\n",
      "Recall Score = 0.697509\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.673937\n",
      "Precision Score = 0.671596\n",
      "Recall Score = 0.699288\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Subword-level Data Processor v3 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 3, remove_non_alpha = True, to_subwords = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  320\n",
      "Vocabulary Size: 20222\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0525_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0525_UTC\n",
      "2017-08-22T05:25:39.936838: step 1, loss 3.53635, acc 0.12\n",
      "current_step:  1\n",
      "2017-08-22T05:25:40.091791: step 2, loss 2.57076, acc 0.24\n",
      "current_step:  2\n",
      "2017-08-22T05:25:40.248731: step 3, loss 1.8837, acc 0.24\n",
      "current_step:  3\n",
      "2017-08-22T05:25:40.402638: step 4, loss 1.43903, acc 0.44\n",
      "current_step:  4\n",
      "2017-08-22T05:25:40.558776: step 5, loss 1.18597, acc 0.5\n",
      "current_step:  5\n",
      "2017-08-22T05:25:40.712883: step 6, loss 1.2527, acc 0.54\n",
      "current_step:  6\n",
      "2017-08-22T05:25:40.865792: step 7, loss 1.47984, acc 0.56\n",
      "current_step:  7\n",
      "2017-08-22T05:25:41.020903: step 8, loss 2.05342, acc 0.44\n",
      "current_step:  8\n",
      "2017-08-22T05:25:41.176350: step 9, loss 1.91719, acc 0.44\n",
      "current_step:  9\n",
      "2017-08-22T05:25:41.329492: step 10, loss 1.83345, acc 0.44\n",
      "current_step:  10\n",
      "2017-08-22T05:25:41.483922: step 11, loss 1.39977, acc 0.54\n",
      "current_step:  11\n",
      "2017-08-22T05:25:41.640819: step 12, loss 1.4516, acc 0.48\n",
      "current_step:  12\n",
      "2017-08-22T05:25:41.886445: step 13, loss 1.41184, acc 0.44\n",
      "current_step:  13\n",
      "2017-08-22T05:25:42.149497: step 14, loss 1.59371, acc 0.38\n",
      "current_step:  14\n",
      "2017-08-22T05:25:42.411310: step 15, loss 1.40753, acc 0.38\n",
      "current_step:  15\n",
      "2017-08-22T05:25:42.675598: step 16, loss 1.57305, acc 0.4\n",
      "current_step:  16\n",
      "2017-08-22T05:25:42.936692: step 17, loss 1.34472, acc 0.34\n",
      "current_step:  17\n",
      "2017-08-22T05:25:43.198141: step 18, loss 1.47164, acc 0.34\n",
      "current_step:  18\n",
      "2017-08-22T05:25:43.459223: step 19, loss 1.55653, acc 0.44\n",
      "current_step:  19\n",
      "2017-08-22T05:25:43.722772: step 20, loss 1.3804, acc 0.5\n",
      "current_step:  20\n",
      "2017-08-22T05:25:43.984534: step 21, loss 1.13677, acc 0.62\n",
      "current_step:  21\n",
      "2017-08-22T05:25:44.246543: step 22, loss 1.45023, acc 0.48\n",
      "current_step:  22\n",
      "2017-08-22T05:25:44.508310: step 23, loss 1.36658, acc 0.5\n",
      "current_step:  23\n",
      "2017-08-22T05:25:44.772624: step 24, loss 1.72881, acc 0.36\n",
      "current_step:  24\n",
      "2017-08-22T05:25:45.033225: step 25, loss 1.41213, acc 0.5\n",
      "current_step:  25\n",
      "2017-08-22T05:25:45.295471: step 26, loss 1.33812, acc 0.48\n",
      "current_step:  26\n",
      "2017-08-22T05:25:45.557568: step 27, loss 1.44899, acc 0.46\n",
      "current_step:  27\n",
      "2017-08-22T05:25:45.820942: step 28, loss 1.11195, acc 0.6\n",
      "current_step:  28\n",
      "2017-08-22T05:25:46.081646: step 29, loss 1.38273, acc 0.52\n",
      "current_step:  29\n",
      "2017-08-22T05:25:46.342558: step 30, loss 1.07712, acc 0.58\n",
      "current_step:  30\n",
      "2017-08-22T05:25:46.605328: step 31, loss 1.36436, acc 0.5\n",
      "current_step:  31\n",
      "2017-08-22T05:25:46.867971: step 32, loss 1.07247, acc 0.52\n",
      "current_step:  32\n",
      "2017-08-22T05:25:47.129372: step 33, loss 1.18097, acc 0.5\n",
      "current_step:  33\n",
      "2017-08-22T05:25:47.393055: step 34, loss 1.34178, acc 0.54\n",
      "current_step:  34\n",
      "2017-08-22T05:25:47.656188: step 35, loss 1.6872, acc 0.44\n",
      "current_step:  35\n",
      "2017-08-22T05:25:47.919555: step 36, loss 1.30142, acc 0.48\n",
      "current_step:  36\n",
      "2017-08-22T05:25:48.182210: step 37, loss 1.40057, acc 0.4\n",
      "current_step:  37\n",
      "2017-08-22T05:25:48.441830: step 38, loss 1.24657, acc 0.56\n",
      "current_step:  38\n",
      "2017-08-22T05:25:48.703872: step 39, loss 1.54322, acc 0.44\n",
      "current_step:  39\n",
      "2017-08-22T05:25:48.965876: step 40, loss 1.42662, acc 0.44\n",
      "current_step:  40\n",
      "2017-08-22T05:25:49.229348: step 41, loss 1.11488, acc 0.62\n",
      "current_step:  41\n",
      "2017-08-22T05:25:49.490971: step 42, loss 1.24329, acc 0.52\n",
      "current_step:  42\n",
      "2017-08-22T05:25:49.755723: step 43, loss 1.36301, acc 0.44\n",
      "current_step:  43\n",
      "2017-08-22T05:25:50.015780: step 44, loss 1.424, acc 0.58\n",
      "current_step:  44\n",
      "2017-08-22T05:25:50.278953: step 45, loss 1.19313, acc 0.48\n",
      "current_step:  45\n",
      "2017-08-22T05:25:50.539457: step 46, loss 1.52857, acc 0.54\n",
      "current_step:  46\n",
      "2017-08-22T05:25:50.803167: step 47, loss 1.30196, acc 0.56\n",
      "current_step:  47\n",
      "2017-08-22T05:25:51.062996: step 48, loss 1.2509, acc 0.56\n",
      "current_step:  48\n",
      "2017-08-22T05:25:51.326307: step 49, loss 1.06174, acc 0.62\n",
      "current_step:  49\n",
      "2017-08-22T05:25:51.586312: step 50, loss 1.41448, acc 0.36\n",
      "current_step:  50\n",
      "2017-08-22T05:25:51.849422: step 51, loss 1.21319, acc 0.54\n",
      "current_step:  51\n",
      "2017-08-22T05:25:52.111701: step 52, loss 1.09838, acc 0.58\n",
      "current_step:  52\n",
      "2017-08-22T05:25:52.373185: step 53, loss 1.08005, acc 0.56\n",
      "current_step:  53\n",
      "2017-08-22T05:25:52.633438: step 54, loss 1.20756, acc 0.64\n",
      "current_step:  54\n",
      "2017-08-22T05:25:52.896440: step 55, loss 1.21989, acc 0.56\n",
      "current_step:  55\n",
      "2017-08-22T05:25:53.155868: step 56, loss 1.10316, acc 0.62\n",
      "current_step:  56\n",
      "2017-08-22T05:25:53.418166: step 57, loss 1.56071, acc 0.38\n",
      "current_step:  57\n",
      "2017-08-22T05:25:53.681534: step 58, loss 1.25448, acc 0.5\n",
      "current_step:  58\n",
      "2017-08-22T05:25:53.943779: step 59, loss 1.05909, acc 0.58\n",
      "current_step:  59\n",
      "2017-08-22T05:25:54.204983: step 60, loss 1.21232, acc 0.54\n",
      "current_step:  60\n",
      "2017-08-22T05:25:54.465451: step 61, loss 1.14567, acc 0.48\n",
      "current_step:  61\n",
      "2017-08-22T05:25:54.728296: step 62, loss 1.10425, acc 0.58\n",
      "current_step:  62\n",
      "2017-08-22T05:25:54.990411: step 63, loss 1.04634, acc 0.58\n",
      "current_step:  63\n",
      "2017-08-22T05:25:55.252065: step 64, loss 1.14924, acc 0.56\n",
      "current_step:  64\n",
      "2017-08-22T05:25:55.517516: step 65, loss 1.11013, acc 0.64\n",
      "current_step:  65\n",
      "2017-08-22T05:25:55.781231: step 66, loss 0.943093, acc 0.7\n",
      "current_step:  66\n",
      "2017-08-22T05:25:56.043854: step 67, loss 0.958138, acc 0.64\n",
      "current_step:  67\n",
      "2017-08-22T05:25:56.304892: step 68, loss 1.44228, acc 0.54\n",
      "current_step:  68\n",
      "2017-08-22T05:25:56.568146: step 69, loss 0.947506, acc 0.56\n",
      "current_step:  69\n",
      "2017-08-22T05:25:56.831318: step 70, loss 1.11004, acc 0.6\n",
      "current_step:  70\n",
      "2017-08-22T05:25:57.091066: step 71, loss 1.41585, acc 0.42\n",
      "current_step:  71\n",
      "2017-08-22T05:25:57.353517: step 72, loss 1.27747, acc 0.44\n",
      "current_step:  72\n",
      "2017-08-22T05:25:57.614147: step 73, loss 1.48468, acc 0.48\n",
      "current_step:  73\n",
      "2017-08-22T05:25:57.876386: step 74, loss 1.08471, acc 0.64\n",
      "current_step:  74\n",
      "2017-08-22T05:25:58.139274: step 75, loss 1.22695, acc 0.6\n",
      "current_step:  75\n",
      "2017-08-22T05:25:58.400981: step 76, loss 1.07496, acc 0.6\n",
      "current_step:  76\n",
      "2017-08-22T05:25:58.662989: step 77, loss 1.24943, acc 0.6\n",
      "current_step:  77\n",
      "2017-08-22T05:25:58.927418: step 78, loss 1.09122, acc 0.58\n",
      "current_step:  78\n",
      "2017-08-22T05:25:59.189756: step 79, loss 1.12396, acc 0.56\n",
      "current_step:  79\n",
      "2017-08-22T05:25:59.450882: step 80, loss 1.0034, acc 0.66\n",
      "current_step:  80\n",
      "2017-08-22T05:25:59.713845: step 81, loss 1.18351, acc 0.56\n",
      "current_step:  81\n",
      "2017-08-22T05:25:59.973461: step 82, loss 1.11305, acc 0.54\n",
      "current_step:  82\n",
      "2017-08-22T05:26:00.234690: step 83, loss 1.39079, acc 0.42\n",
      "current_step:  83\n",
      "2017-08-22T05:26:00.497683: step 84, loss 1.00001, acc 0.58\n",
      "current_step:  84\n",
      "2017-08-22T05:26:00.761668: step 85, loss 1.23251, acc 0.54\n",
      "current_step:  85\n",
      "2017-08-22T05:26:01.022721: step 86, loss 1.12224, acc 0.58\n",
      "current_step:  86\n",
      "2017-08-22T05:26:01.284696: step 87, loss 1.10726, acc 0.56\n",
      "current_step:  87\n",
      "2017-08-22T05:26:01.545255: step 88, loss 1.30369, acc 0.42\n",
      "current_step:  88\n",
      "2017-08-22T05:26:01.807332: step 89, loss 1.07162, acc 0.68\n",
      "current_step:  89\n",
      "2017-08-22T05:26:02.068869: step 90, loss 1.26269, acc 0.54\n",
      "current_step:  90\n",
      "2017-08-22T05:26:02.330592: step 91, loss 1.05641, acc 0.56\n",
      "current_step:  91\n",
      "2017-08-22T05:26:02.593291: step 92, loss 1.29367, acc 0.6\n",
      "current_step:  92\n",
      "2017-08-22T05:26:02.858383: step 93, loss 1.13754, acc 0.58\n",
      "current_step:  93\n",
      "2017-08-22T05:26:03.120989: step 94, loss 1.10167, acc 0.58\n",
      "current_step:  94\n",
      "2017-08-22T05:26:03.382201: step 95, loss 1.0101, acc 0.72\n",
      "current_step:  95\n",
      "2017-08-22T05:26:03.644917: step 96, loss 1.12833, acc 0.62\n",
      "current_step:  96\n",
      "2017-08-22T05:26:03.907497: step 97, loss 1.22586, acc 0.52\n",
      "current_step:  97\n",
      "2017-08-22T05:26:04.169472: step 98, loss 1.34743, acc 0.56\n",
      "current_step:  98\n",
      "2017-08-22T05:26:04.431241: step 99, loss 1.09085, acc 0.6\n",
      "current_step:  99\n",
      "2017-08-22T05:26:04.695721: step 100, loss 1.14041, acc 0.56\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:26:05.659505: step 100, loss 1.07597, acc 0.594306\n",
      "\n",
      "2017-08-22T05:26:05.920905: step 101, loss 1.02608, acc 0.66\n",
      "current_step:  101\n",
      "2017-08-22T05:26:05.974579: step 102, loss 0.699271, acc 0.777778\n",
      "current_step:  102\n",
      "2017-08-22T05:26:06.236695: step 103, loss 0.809094, acc 0.78\n",
      "current_step:  103\n",
      "2017-08-22T05:26:06.499233: step 104, loss 0.755603, acc 0.72\n",
      "current_step:  104\n",
      "2017-08-22T05:26:06.764848: step 105, loss 0.67321, acc 0.8\n",
      "current_step:  105\n",
      "2017-08-22T05:26:07.026963: step 106, loss 0.970752, acc 0.6\n",
      "current_step:  106\n",
      "2017-08-22T05:26:07.290561: step 107, loss 0.905821, acc 0.58\n",
      "current_step:  107\n",
      "2017-08-22T05:26:07.551146: step 108, loss 0.661996, acc 0.76\n",
      "current_step:  108\n",
      "2017-08-22T05:26:07.813799: step 109, loss 0.906597, acc 0.74\n",
      "current_step:  109\n",
      "2017-08-22T05:26:08.075815: step 110, loss 0.951844, acc 0.66\n",
      "current_step:  110\n",
      "2017-08-22T05:26:08.336906: step 111, loss 0.852532, acc 0.74\n",
      "current_step:  111\n",
      "2017-08-22T05:26:08.599926: step 112, loss 0.976794, acc 0.7\n",
      "current_step:  112\n",
      "2017-08-22T05:26:08.864482: step 113, loss 0.848583, acc 0.72\n",
      "current_step:  113\n",
      "2017-08-22T05:26:09.128347: step 114, loss 0.920459, acc 0.78\n",
      "current_step:  114\n",
      "2017-08-22T05:26:09.390895: step 115, loss 0.739804, acc 0.74\n",
      "current_step:  115\n",
      "2017-08-22T05:26:09.653007: step 116, loss 0.857984, acc 0.78\n",
      "current_step:  116\n",
      "2017-08-22T05:26:09.914519: step 117, loss 0.857265, acc 0.72\n",
      "current_step:  117\n",
      "2017-08-22T05:26:10.176992: step 118, loss 0.837218, acc 0.76\n",
      "current_step:  118\n",
      "2017-08-22T05:26:10.438147: step 119, loss 0.767948, acc 0.8\n",
      "current_step:  119\n",
      "2017-08-22T05:26:10.701994: step 120, loss 1.06618, acc 0.58\n",
      "current_step:  120\n",
      "2017-08-22T05:26:10.961956: step 121, loss 0.959416, acc 0.64\n",
      "current_step:  121\n",
      "2017-08-22T05:26:11.223484: step 122, loss 0.914847, acc 0.7\n",
      "current_step:  122\n",
      "2017-08-22T05:26:11.484815: step 123, loss 0.782373, acc 0.74\n",
      "current_step:  123\n",
      "2017-08-22T05:26:11.747654: step 124, loss 0.791729, acc 0.7\n",
      "current_step:  124\n",
      "2017-08-22T05:26:12.009480: step 125, loss 0.751698, acc 0.74\n",
      "current_step:  125\n",
      "2017-08-22T05:26:12.271767: step 126, loss 0.918015, acc 0.66\n",
      "current_step:  126\n",
      "2017-08-22T05:26:12.534269: step 127, loss 0.700399, acc 0.86\n",
      "current_step:  127\n",
      "2017-08-22T05:26:12.797249: step 128, loss 0.882877, acc 0.74\n",
      "current_step:  128\n",
      "2017-08-22T05:26:13.058591: step 129, loss 0.971993, acc 0.66\n",
      "current_step:  129\n",
      "2017-08-22T05:26:13.319470: step 130, loss 0.837649, acc 0.76\n",
      "current_step:  130\n",
      "2017-08-22T05:26:13.582672: step 131, loss 0.776444, acc 0.76\n",
      "current_step:  131\n",
      "2017-08-22T05:26:13.845225: step 132, loss 0.898809, acc 0.72\n",
      "current_step:  132\n",
      "2017-08-22T05:26:14.107710: step 133, loss 0.956923, acc 0.74\n",
      "current_step:  133\n",
      "2017-08-22T05:26:14.371011: step 134, loss 0.903765, acc 0.74\n",
      "current_step:  134\n",
      "2017-08-22T05:26:14.635528: step 135, loss 0.994658, acc 0.62\n",
      "current_step:  135\n",
      "2017-08-22T05:26:14.900859: step 136, loss 0.971849, acc 0.64\n",
      "current_step:  136\n",
      "2017-08-22T05:26:15.163830: step 137, loss 0.855612, acc 0.64\n",
      "current_step:  137\n",
      "2017-08-22T05:26:15.428196: step 138, loss 0.707906, acc 0.76\n",
      "current_step:  138\n",
      "2017-08-22T05:26:15.692605: step 139, loss 0.942559, acc 0.64\n",
      "current_step:  139\n",
      "2017-08-22T05:26:15.954890: step 140, loss 0.69819, acc 0.74\n",
      "current_step:  140\n",
      "2017-08-22T05:26:16.217108: step 141, loss 0.888726, acc 0.66\n",
      "current_step:  141\n",
      "2017-08-22T05:26:16.478422: step 142, loss 1.1468, acc 0.5\n",
      "current_step:  142\n",
      "2017-08-22T05:26:16.742962: step 143, loss 1.07827, acc 0.64\n",
      "current_step:  143\n",
      "2017-08-22T05:26:17.002999: step 144, loss 0.799272, acc 0.7\n",
      "current_step:  144\n",
      "2017-08-22T05:26:17.266284: step 145, loss 1.01224, acc 0.68\n",
      "current_step:  145\n",
      "2017-08-22T05:26:17.527505: step 146, loss 0.799639, acc 0.76\n",
      "current_step:  146\n",
      "2017-08-22T05:26:17.790071: step 147, loss 1.01726, acc 0.68\n",
      "current_step:  147\n",
      "2017-08-22T05:26:18.051805: step 148, loss 0.746291, acc 0.86\n",
      "current_step:  148\n",
      "2017-08-22T05:26:18.314505: step 149, loss 0.847212, acc 0.78\n",
      "current_step:  149\n",
      "2017-08-22T05:26:18.575457: step 150, loss 0.79563, acc 0.74\n",
      "current_step:  150\n",
      "2017-08-22T05:26:18.838839: step 151, loss 0.79982, acc 0.74\n",
      "current_step:  151\n",
      "2017-08-22T05:26:19.099708: step 152, loss 0.660411, acc 0.78\n",
      "current_step:  152\n",
      "2017-08-22T05:26:19.361218: step 153, loss 0.721042, acc 0.8\n",
      "current_step:  153\n",
      "2017-08-22T05:26:19.622413: step 154, loss 0.684368, acc 0.76\n",
      "current_step:  154\n",
      "2017-08-22T05:26:19.885495: step 155, loss 0.851109, acc 0.72\n",
      "current_step:  155\n",
      "2017-08-22T05:26:20.147161: step 156, loss 0.891764, acc 0.62\n",
      "current_step:  156\n",
      "2017-08-22T05:26:20.408713: step 157, loss 0.906048, acc 0.62\n",
      "current_step:  157\n",
      "2017-08-22T05:26:20.670368: step 158, loss 0.780719, acc 0.66\n",
      "current_step:  158\n",
      "2017-08-22T05:26:20.931585: step 159, loss 0.889239, acc 0.72\n",
      "current_step:  159\n",
      "2017-08-22T05:26:21.194301: step 160, loss 0.970401, acc 0.62\n",
      "current_step:  160\n",
      "2017-08-22T05:26:21.456624: step 161, loss 0.769667, acc 0.82\n",
      "current_step:  161\n",
      "2017-08-22T05:26:21.720394: step 162, loss 0.814653, acc 0.68\n",
      "current_step:  162\n",
      "2017-08-22T05:26:21.983482: step 163, loss 1.23348, acc 0.64\n",
      "current_step:  163\n",
      "2017-08-22T05:26:22.246301: step 164, loss 1.00707, acc 0.64\n",
      "current_step:  164\n",
      "2017-08-22T05:26:22.507284: step 165, loss 0.807635, acc 0.78\n",
      "current_step:  165\n",
      "2017-08-22T05:26:22.769985: step 166, loss 0.848917, acc 0.76\n",
      "current_step:  166\n",
      "2017-08-22T05:26:23.030114: step 167, loss 0.870667, acc 0.7\n",
      "current_step:  167\n",
      "2017-08-22T05:26:23.291349: step 168, loss 0.712053, acc 0.8\n",
      "current_step:  168\n",
      "2017-08-22T05:26:23.553652: step 169, loss 0.771348, acc 0.74\n",
      "current_step:  169\n",
      "2017-08-22T05:26:23.815855: step 170, loss 0.947392, acc 0.64\n",
      "current_step:  170\n",
      "2017-08-22T05:26:24.078083: step 171, loss 1.04661, acc 0.62\n",
      "current_step:  171\n",
      "2017-08-22T05:26:24.337891: step 172, loss 0.925106, acc 0.58\n",
      "current_step:  172\n",
      "2017-08-22T05:26:24.600245: step 173, loss 0.778958, acc 0.7\n",
      "current_step:  173\n",
      "2017-08-22T05:26:24.864841: step 174, loss 0.65253, acc 0.84\n",
      "current_step:  174\n",
      "2017-08-22T05:26:25.125853: step 175, loss 0.809797, acc 0.74\n",
      "current_step:  175\n",
      "2017-08-22T05:26:25.387231: step 176, loss 0.790385, acc 0.74\n",
      "current_step:  176\n",
      "2017-08-22T05:26:25.649520: step 177, loss 0.766514, acc 0.76\n",
      "current_step:  177\n",
      "2017-08-22T05:26:25.910707: step 178, loss 0.812983, acc 0.74\n",
      "current_step:  178\n",
      "2017-08-22T05:26:26.173799: step 179, loss 0.994883, acc 0.6\n",
      "current_step:  179\n",
      "2017-08-22T05:26:26.436390: step 180, loss 1.02284, acc 0.68\n",
      "current_step:  180\n",
      "2017-08-22T05:26:26.701963: step 181, loss 0.896681, acc 0.6\n",
      "current_step:  181\n",
      "2017-08-22T05:26:26.963366: step 182, loss 1.05374, acc 0.66\n",
      "current_step:  182\n",
      "2017-08-22T05:26:27.225029: step 183, loss 0.607918, acc 0.84\n",
      "current_step:  183\n",
      "2017-08-22T05:26:27.486412: step 184, loss 0.859883, acc 0.72\n",
      "current_step:  184\n",
      "2017-08-22T05:26:27.750350: step 185, loss 0.71029, acc 0.82\n",
      "current_step:  185\n",
      "2017-08-22T05:26:28.012122: step 186, loss 0.98626, acc 0.68\n",
      "current_step:  186\n",
      "2017-08-22T05:26:28.273960: step 187, loss 1.02353, acc 0.64\n",
      "current_step:  187\n",
      "2017-08-22T05:26:28.536123: step 188, loss 0.785505, acc 0.74\n",
      "current_step:  188\n",
      "2017-08-22T05:26:28.800305: step 189, loss 0.799241, acc 0.74\n",
      "current_step:  189\n",
      "2017-08-22T05:26:29.063524: step 190, loss 0.86699, acc 0.7\n",
      "current_step:  190\n",
      "2017-08-22T05:26:29.324873: step 191, loss 0.712908, acc 0.84\n",
      "current_step:  191\n",
      "2017-08-22T05:26:29.589111: step 192, loss 0.891938, acc 0.68\n",
      "current_step:  192\n",
      "2017-08-22T05:26:29.853633: step 193, loss 0.886188, acc 0.66\n",
      "current_step:  193\n",
      "2017-08-22T05:26:30.115636: step 194, loss 0.822916, acc 0.66\n",
      "current_step:  194\n",
      "2017-08-22T05:26:30.379259: step 195, loss 0.873783, acc 0.74\n",
      "current_step:  195\n",
      "2017-08-22T05:26:30.641960: step 196, loss 0.862279, acc 0.72\n",
      "current_step:  196\n",
      "2017-08-22T05:26:30.903773: step 197, loss 0.941919, acc 0.58\n",
      "current_step:  197\n",
      "2017-08-22T05:26:31.165677: step 198, loss 0.692722, acc 0.82\n",
      "current_step:  198\n",
      "2017-08-22T05:26:31.427769: step 199, loss 0.633289, acc 0.8\n",
      "current_step:  199\n",
      "2017-08-22T05:26:31.692957: step 200, loss 0.73368, acc 0.76\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:26:32.662349: step 200, loss 1.00161, acc 0.651246\n",
      "\n",
      "2017-08-22T05:26:32.926166: step 201, loss 0.931191, acc 0.72\n",
      "current_step:  201\n",
      "2017-08-22T05:26:33.188982: step 202, loss 0.897621, acc 0.7\n",
      "current_step:  202\n",
      "2017-08-22T05:26:33.451505: step 203, loss 1.04221, acc 0.6\n",
      "current_step:  203\n",
      "2017-08-22T05:26:33.506484: step 204, loss 1.06107, acc 0.666667\n",
      "current_step:  204\n",
      "2017-08-22T05:26:33.770997: step 205, loss 0.644726, acc 0.8\n",
      "current_step:  205\n",
      "2017-08-22T05:26:34.030487: step 206, loss 0.494873, acc 0.88\n",
      "current_step:  206\n",
      "2017-08-22T05:26:34.292657: step 207, loss 0.555143, acc 0.82\n",
      "current_step:  207\n",
      "2017-08-22T05:26:34.554160: step 208, loss 0.571577, acc 0.84\n",
      "current_step:  208\n",
      "2017-08-22T05:26:34.817566: step 209, loss 0.704098, acc 0.7\n",
      "current_step:  209\n",
      "2017-08-22T05:26:35.078752: step 210, loss 0.802154, acc 0.78\n",
      "current_step:  210\n",
      "2017-08-22T05:26:35.340767: step 211, loss 0.602128, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-22T05:26:35.602575: step 212, loss 0.639516, acc 0.76\n",
      "current_step:  212\n",
      "2017-08-22T05:26:35.864712: step 213, loss 0.604474, acc 0.8\n",
      "current_step:  213\n",
      "2017-08-22T05:26:36.126592: step 214, loss 0.689236, acc 0.82\n",
      "current_step:  214\n",
      "2017-08-22T05:26:36.389323: step 215, loss 0.502148, acc 0.88\n",
      "current_step:  215\n",
      "2017-08-22T05:26:36.650459: step 216, loss 0.643826, acc 0.86\n",
      "current_step:  216\n",
      "2017-08-22T05:26:36.912071: step 217, loss 0.58574, acc 0.86\n",
      "current_step:  217\n",
      "2017-08-22T05:26:37.175659: step 218, loss 0.504367, acc 0.9\n",
      "current_step:  218\n",
      "2017-08-22T05:26:37.437847: step 219, loss 0.644327, acc 0.88\n",
      "current_step:  219\n",
      "2017-08-22T05:26:37.700026: step 220, loss 0.77589, acc 0.76\n",
      "current_step:  220\n",
      "2017-08-22T05:26:37.963019: step 221, loss 0.558981, acc 0.88\n",
      "current_step:  221\n",
      "2017-08-22T05:26:38.223777: step 222, loss 0.531124, acc 0.88\n",
      "current_step:  222\n",
      "2017-08-22T05:26:38.483495: step 223, loss 0.631982, acc 0.8\n",
      "current_step:  223\n",
      "2017-08-22T05:26:38.748720: step 224, loss 0.857893, acc 0.6\n",
      "current_step:  224\n",
      "2017-08-22T05:26:39.011493: step 225, loss 0.621781, acc 0.82\n",
      "current_step:  225\n",
      "2017-08-22T05:26:39.274452: step 226, loss 0.715386, acc 0.7\n",
      "current_step:  226\n",
      "2017-08-22T05:26:39.534200: step 227, loss 0.615057, acc 0.86\n",
      "current_step:  227\n",
      "2017-08-22T05:26:39.797424: step 228, loss 0.556316, acc 0.86\n",
      "current_step:  228\n",
      "2017-08-22T05:26:40.060200: step 229, loss 0.734984, acc 0.76\n",
      "current_step:  229\n",
      "2017-08-22T05:26:40.321732: step 230, loss 0.706187, acc 0.8\n",
      "current_step:  230\n",
      "2017-08-22T05:26:40.583794: step 231, loss 0.643567, acc 0.82\n",
      "current_step:  231\n",
      "2017-08-22T05:26:40.847828: step 232, loss 0.658262, acc 0.84\n",
      "current_step:  232\n",
      "2017-08-22T05:26:41.109240: step 233, loss 0.553214, acc 0.9\n",
      "current_step:  233\n",
      "2017-08-22T05:26:41.372163: step 234, loss 0.701721, acc 0.76\n",
      "current_step:  234\n",
      "2017-08-22T05:26:41.634271: step 235, loss 0.5267, acc 0.88\n",
      "current_step:  235\n",
      "2017-08-22T05:26:41.896061: step 236, loss 0.543481, acc 0.88\n",
      "current_step:  236\n",
      "2017-08-22T05:26:42.160332: step 237, loss 0.654083, acc 0.8\n",
      "current_step:  237\n",
      "2017-08-22T05:26:42.422056: step 238, loss 0.703853, acc 0.82\n",
      "current_step:  238\n",
      "2017-08-22T05:26:42.686089: step 239, loss 0.601552, acc 0.82\n",
      "current_step:  239\n",
      "2017-08-22T05:26:42.949087: step 240, loss 0.599446, acc 0.78\n",
      "current_step:  240\n",
      "2017-08-22T05:26:43.209718: step 241, loss 0.660109, acc 0.82\n",
      "current_step:  241\n",
      "2017-08-22T05:26:43.471867: step 242, loss 0.524057, acc 0.9\n",
      "current_step:  242\n",
      "2017-08-22T05:26:43.735586: step 243, loss 0.579422, acc 0.82\n",
      "current_step:  243\n",
      "2017-08-22T05:26:43.999320: step 244, loss 0.544931, acc 0.84\n",
      "current_step:  244\n",
      "2017-08-22T05:26:44.261769: step 245, loss 0.584708, acc 0.78\n",
      "current_step:  245\n",
      "2017-08-22T05:26:44.523451: step 246, loss 0.702733, acc 0.8\n",
      "current_step:  246\n",
      "2017-08-22T05:26:44.786675: step 247, loss 0.562143, acc 0.84\n",
      "current_step:  247\n",
      "2017-08-22T05:26:45.046853: step 248, loss 0.595462, acc 0.84\n",
      "current_step:  248\n",
      "2017-08-22T05:26:45.309979: step 249, loss 0.490761, acc 0.88\n",
      "current_step:  249\n",
      "2017-08-22T05:26:45.571810: step 250, loss 0.605423, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-22T05:26:45.834873: step 251, loss 0.615593, acc 0.84\n",
      "current_step:  251\n",
      "2017-08-22T05:26:46.096205: step 252, loss 0.47089, acc 0.94\n",
      "current_step:  252\n",
      "2017-08-22T05:26:46.358065: step 253, loss 0.545961, acc 0.9\n",
      "current_step:  253\n",
      "2017-08-22T05:26:46.621370: step 254, loss 0.615632, acc 0.86\n",
      "current_step:  254\n",
      "2017-08-22T05:26:46.882186: step 255, loss 0.578151, acc 0.82\n",
      "current_step:  255\n",
      "2017-08-22T05:26:47.144472: step 256, loss 0.458448, acc 0.9\n",
      "current_step:  256\n",
      "2017-08-22T05:26:47.406061: step 257, loss 0.679534, acc 0.8\n",
      "current_step:  257\n",
      "2017-08-22T05:26:47.669211: step 258, loss 0.431438, acc 0.84\n",
      "current_step:  258\n",
      "2017-08-22T05:26:47.931757: step 259, loss 0.566708, acc 0.78\n",
      "current_step:  259\n",
      "2017-08-22T05:26:48.191912: step 260, loss 0.698316, acc 0.76\n",
      "current_step:  260\n",
      "2017-08-22T05:26:48.456686: step 261, loss 0.516977, acc 0.84\n",
      "current_step:  261\n",
      "2017-08-22T05:26:48.722721: step 262, loss 0.7192, acc 0.76\n",
      "current_step:  262\n",
      "2017-08-22T05:26:48.984298: step 263, loss 0.508578, acc 0.92\n",
      "current_step:  263\n",
      "2017-08-22T05:26:49.247265: step 264, loss 0.553362, acc 0.84\n",
      "current_step:  264\n",
      "2017-08-22T05:26:49.509174: step 265, loss 0.510418, acc 0.82\n",
      "current_step:  265\n",
      "2017-08-22T05:26:49.772256: step 266, loss 0.544089, acc 0.84\n",
      "current_step:  266\n",
      "2017-08-22T05:26:50.033645: step 267, loss 0.528323, acc 0.8\n",
      "current_step:  267\n",
      "2017-08-22T05:26:50.292809: step 268, loss 0.62957, acc 0.8\n",
      "current_step:  268\n",
      "2017-08-22T05:26:50.555399: step 269, loss 0.616722, acc 0.82\n",
      "current_step:  269\n",
      "2017-08-22T05:26:50.817891: step 270, loss 0.65897, acc 0.74\n",
      "current_step:  270\n",
      "2017-08-22T05:26:51.079990: step 271, loss 0.863862, acc 0.74\n",
      "current_step:  271\n",
      "2017-08-22T05:26:51.341447: step 272, loss 0.569685, acc 0.8\n",
      "current_step:  272\n",
      "2017-08-22T05:26:51.603676: step 273, loss 0.466866, acc 0.9\n",
      "current_step:  273\n",
      "2017-08-22T05:26:51.866309: step 274, loss 0.677101, acc 0.78\n",
      "current_step:  274\n",
      "2017-08-22T05:26:52.126144: step 275, loss 0.582047, acc 0.82\n",
      "current_step:  275\n",
      "2017-08-22T05:26:52.388868: step 276, loss 0.542113, acc 0.84\n",
      "current_step:  276\n",
      "2017-08-22T05:26:52.651337: step 277, loss 0.654171, acc 0.8\n",
      "current_step:  277\n",
      "2017-08-22T05:26:52.913548: step 278, loss 0.494582, acc 0.84\n",
      "current_step:  278\n",
      "2017-08-22T05:26:53.177411: step 279, loss 0.643634, acc 0.84\n",
      "current_step:  279\n",
      "2017-08-22T05:26:53.439121: step 280, loss 0.603925, acc 0.82\n",
      "current_step:  280\n",
      "2017-08-22T05:26:53.703059: step 281, loss 0.560282, acc 0.82\n",
      "current_step:  281\n",
      "2017-08-22T05:26:53.963173: step 282, loss 0.707166, acc 0.76\n",
      "current_step:  282\n",
      "2017-08-22T05:26:54.224253: step 283, loss 0.479961, acc 0.9\n",
      "current_step:  283\n",
      "2017-08-22T05:26:54.490609: step 284, loss 0.810313, acc 0.7\n",
      "current_step:  284\n",
      "2017-08-22T05:26:54.761103: step 285, loss 0.816964, acc 0.72\n",
      "current_step:  285\n",
      "2017-08-22T05:26:55.030518: step 286, loss 0.685289, acc 0.78\n",
      "current_step:  286\n",
      "2017-08-22T05:26:55.298568: step 287, loss 0.713414, acc 0.8\n",
      "current_step:  287\n",
      "2017-08-22T05:26:55.568468: step 288, loss 0.531769, acc 0.86\n",
      "current_step:  288\n",
      "2017-08-22T05:26:55.838738: step 289, loss 0.670259, acc 0.8\n",
      "current_step:  289\n",
      "2017-08-22T05:26:56.108986: step 290, loss 0.703848, acc 0.76\n",
      "current_step:  290\n",
      "2017-08-22T05:26:56.376916: step 291, loss 0.747664, acc 0.72\n",
      "current_step:  291\n",
      "2017-08-22T05:26:56.645948: step 292, loss 0.580984, acc 0.82\n",
      "current_step:  292\n",
      "2017-08-22T05:26:56.914467: step 293, loss 0.732026, acc 0.78\n",
      "current_step:  293\n",
      "2017-08-22T05:26:57.183865: step 294, loss 0.525782, acc 0.9\n",
      "current_step:  294\n",
      "2017-08-22T05:26:57.450959: step 295, loss 0.663903, acc 0.78\n",
      "current_step:  295\n",
      "2017-08-22T05:26:57.721142: step 296, loss 0.575226, acc 0.88\n",
      "current_step:  296\n",
      "2017-08-22T05:26:57.988893: step 297, loss 0.767258, acc 0.76\n",
      "current_step:  297\n",
      "2017-08-22T05:26:58.258049: step 298, loss 0.490216, acc 0.94\n",
      "current_step:  298\n",
      "2017-08-22T05:26:58.525566: step 299, loss 0.505835, acc 0.88\n",
      "current_step:  299\n",
      "2017-08-22T05:26:58.795968: step 300, loss 0.554215, acc 0.84\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:26:59.769080: step 300, loss 0.965487, acc 0.656584\n",
      "\n",
      "2017-08-22T05:27:00.037014: step 301, loss 0.790667, acc 0.84\n",
      "current_step:  301\n",
      "2017-08-22T05:27:00.303921: step 302, loss 0.671645, acc 0.8\n",
      "current_step:  302\n",
      "2017-08-22T05:27:00.573993: step 303, loss 0.656644, acc 0.76\n",
      "current_step:  303\n",
      "2017-08-22T05:27:00.842707: step 304, loss 0.55177, acc 0.86\n",
      "current_step:  304\n",
      "2017-08-22T05:27:01.109307: step 305, loss 0.499351, acc 0.84\n",
      "current_step:  305\n",
      "2017-08-22T05:27:01.164607: step 306, loss 1.0442, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-22T05:27:01.437255: step 307, loss 0.466377, acc 0.92\n",
      "current_step:  307\n",
      "2017-08-22T05:27:01.707276: step 308, loss 0.547611, acc 0.88\n",
      "current_step:  308\n",
      "2017-08-22T05:27:01.976430: step 309, loss 0.446971, acc 0.92\n",
      "current_step:  309\n",
      "2017-08-22T05:27:02.245160: step 310, loss 0.352398, acc 0.96\n",
      "current_step:  310\n",
      "2017-08-22T05:27:02.513403: step 311, loss 0.477201, acc 0.9\n",
      "current_step:  311\n",
      "2017-08-22T05:27:02.783344: step 312, loss 0.502109, acc 0.88\n",
      "current_step:  312\n",
      "2017-08-22T05:27:03.052716: step 313, loss 0.537047, acc 0.88\n",
      "current_step:  313\n",
      "2017-08-22T05:27:03.322384: step 314, loss 0.454814, acc 0.9\n",
      "current_step:  314\n",
      "2017-08-22T05:27:03.591654: step 315, loss 0.475188, acc 0.88\n",
      "current_step:  315\n",
      "2017-08-22T05:27:03.861543: step 316, loss 0.460356, acc 0.94\n",
      "current_step:  316\n",
      "2017-08-22T05:27:04.132300: step 317, loss 0.455176, acc 0.86\n",
      "current_step:  317\n",
      "2017-08-22T05:27:04.400708: step 318, loss 0.493428, acc 0.84\n",
      "current_step:  318\n",
      "2017-08-22T05:27:04.666015: step 319, loss 0.380758, acc 0.94\n",
      "current_step:  319\n",
      "2017-08-22T05:27:04.931122: step 320, loss 0.355495, acc 0.96\n",
      "current_step:  320\n",
      "2017-08-22T05:27:05.193574: step 321, loss 0.377467, acc 0.9\n",
      "current_step:  321\n",
      "2017-08-22T05:27:05.456672: step 322, loss 0.48169, acc 0.84\n",
      "current_step:  322\n",
      "2017-08-22T05:27:05.720808: step 323, loss 0.326847, acc 0.92\n",
      "current_step:  323\n",
      "2017-08-22T05:27:05.984454: step 324, loss 0.486297, acc 0.84\n",
      "current_step:  324\n",
      "2017-08-22T05:27:06.247256: step 325, loss 0.452507, acc 0.9\n",
      "current_step:  325\n",
      "2017-08-22T05:27:06.508232: step 326, loss 0.522937, acc 0.84\n",
      "current_step:  326\n",
      "2017-08-22T05:27:06.770904: step 327, loss 0.32143, acc 0.96\n",
      "current_step:  327\n",
      "2017-08-22T05:27:07.034312: step 328, loss 0.427676, acc 0.92\n",
      "current_step:  328\n",
      "2017-08-22T05:27:07.296094: step 329, loss 0.474621, acc 0.9\n",
      "current_step:  329\n",
      "2017-08-22T05:27:07.559888: step 330, loss 0.322042, acc 0.9\n",
      "current_step:  330\n",
      "2017-08-22T05:27:07.823198: step 331, loss 0.649395, acc 0.78\n",
      "current_step:  331\n",
      "2017-08-22T05:27:08.083398: step 332, loss 0.437315, acc 0.92\n",
      "current_step:  332\n",
      "2017-08-22T05:27:08.345461: step 333, loss 0.454106, acc 0.94\n",
      "current_step:  333\n",
      "2017-08-22T05:27:08.608765: step 334, loss 0.402574, acc 0.96\n",
      "current_step:  334\n",
      "2017-08-22T05:27:08.873244: step 335, loss 0.448435, acc 0.88\n",
      "current_step:  335\n",
      "2017-08-22T05:27:09.133742: step 336, loss 0.410586, acc 0.96\n",
      "current_step:  336\n",
      "2017-08-22T05:27:09.395449: step 337, loss 0.422447, acc 0.94\n",
      "current_step:  337\n",
      "2017-08-22T05:27:09.660040: step 338, loss 0.452357, acc 0.9\n",
      "current_step:  338\n",
      "2017-08-22T05:27:09.922680: step 339, loss 0.354041, acc 0.94\n",
      "current_step:  339\n",
      "2017-08-22T05:27:10.185603: step 340, loss 0.370797, acc 0.92\n",
      "current_step:  340\n",
      "2017-08-22T05:27:10.447977: step 341, loss 0.555388, acc 0.8\n",
      "current_step:  341\n",
      "2017-08-22T05:27:10.714106: step 342, loss 0.34079, acc 0.92\n",
      "current_step:  342\n",
      "2017-08-22T05:27:10.974919: step 343, loss 0.49959, acc 0.86\n",
      "current_step:  343\n",
      "2017-08-22T05:27:11.237599: step 344, loss 0.466883, acc 0.88\n",
      "current_step:  344\n",
      "2017-08-22T05:27:11.499771: step 345, loss 0.346841, acc 0.98\n",
      "current_step:  345\n",
      "2017-08-22T05:27:11.764314: step 346, loss 0.53685, acc 0.84\n",
      "current_step:  346\n",
      "2017-08-22T05:27:12.027430: step 347, loss 0.455821, acc 0.84\n",
      "current_step:  347\n",
      "2017-08-22T05:27:12.292635: step 348, loss 0.368429, acc 0.9\n",
      "current_step:  348\n",
      "2017-08-22T05:27:12.553794: step 349, loss 0.418498, acc 0.94\n",
      "current_step:  349\n",
      "2017-08-22T05:27:12.816903: step 350, loss 0.53605, acc 0.86\n",
      "current_step:  350\n",
      "2017-08-22T05:27:13.079802: step 351, loss 0.397484, acc 0.96\n",
      "current_step:  351\n",
      "2017-08-22T05:27:13.340143: step 352, loss 0.367318, acc 0.92\n",
      "current_step:  352\n",
      "2017-08-22T05:27:13.605017: step 353, loss 0.487358, acc 0.9\n",
      "current_step:  353\n",
      "2017-08-22T05:27:13.864112: step 354, loss 0.354712, acc 0.96\n",
      "current_step:  354\n",
      "2017-08-22T05:27:14.126272: step 355, loss 0.579686, acc 0.78\n",
      "current_step:  355\n",
      "2017-08-22T05:27:14.387714: step 356, loss 0.470054, acc 0.94\n",
      "current_step:  356\n",
      "2017-08-22T05:27:14.648970: step 357, loss 0.392942, acc 0.92\n",
      "current_step:  357\n",
      "2017-08-22T05:27:14.911197: step 358, loss 0.371044, acc 0.92\n",
      "current_step:  358\n",
      "2017-08-22T05:27:15.173232: step 359, loss 0.455313, acc 0.9\n",
      "current_step:  359\n",
      "2017-08-22T05:27:15.433853: step 360, loss 0.506614, acc 0.84\n",
      "current_step:  360\n",
      "2017-08-22T05:27:15.696079: step 361, loss 0.378976, acc 0.9\n",
      "current_step:  361\n",
      "2017-08-22T05:27:15.958560: step 362, loss 0.346867, acc 0.92\n",
      "current_step:  362\n",
      "2017-08-22T05:27:16.219814: step 363, loss 0.598866, acc 0.78\n",
      "current_step:  363\n",
      "2017-08-22T05:27:16.482657: step 364, loss 0.408353, acc 0.88\n",
      "current_step:  364\n",
      "2017-08-22T05:27:16.746618: step 365, loss 0.402974, acc 0.9\n",
      "current_step:  365\n",
      "2017-08-22T05:27:17.008676: step 366, loss 0.56646, acc 0.82\n",
      "current_step:  366\n",
      "2017-08-22T05:27:17.270962: step 367, loss 0.504788, acc 0.84\n",
      "current_step:  367\n",
      "2017-08-22T05:27:17.534232: step 368, loss 0.410081, acc 0.88\n",
      "current_step:  368\n",
      "2017-08-22T05:27:17.796639: step 369, loss 0.375607, acc 0.88\n",
      "current_step:  369\n",
      "2017-08-22T05:27:18.058918: step 370, loss 0.458147, acc 0.92\n",
      "current_step:  370\n",
      "2017-08-22T05:27:18.319481: step 371, loss 0.44494, acc 0.88\n",
      "current_step:  371\n",
      "2017-08-22T05:27:18.582146: step 372, loss 0.55599, acc 0.86\n",
      "current_step:  372\n",
      "2017-08-22T05:27:18.845965: step 373, loss 0.517695, acc 0.8\n",
      "current_step:  373\n",
      "2017-08-22T05:27:19.108285: step 374, loss 0.379248, acc 0.92\n",
      "current_step:  374\n",
      "2017-08-22T05:27:19.370471: step 375, loss 0.442232, acc 0.9\n",
      "current_step:  375\n",
      "2017-08-22T05:27:19.631312: step 376, loss 0.333335, acc 0.94\n",
      "current_step:  376\n",
      "2017-08-22T05:27:19.895135: step 377, loss 0.438139, acc 0.88\n",
      "current_step:  377\n",
      "2017-08-22T05:27:20.158619: step 378, loss 0.299831, acc 0.96\n",
      "current_step:  378\n",
      "2017-08-22T05:27:20.420192: step 379, loss 0.394658, acc 0.92\n",
      "current_step:  379\n",
      "2017-08-22T05:27:20.682008: step 380, loss 0.402604, acc 0.92\n",
      "current_step:  380\n",
      "2017-08-22T05:27:20.945247: step 381, loss 0.50515, acc 0.84\n",
      "current_step:  381\n",
      "2017-08-22T05:27:21.209754: step 382, loss 0.390611, acc 0.86\n",
      "current_step:  382\n",
      "2017-08-22T05:27:21.472590: step 383, loss 0.525904, acc 0.84\n",
      "current_step:  383\n",
      "2017-08-22T05:27:21.736492: step 384, loss 0.503326, acc 0.84\n",
      "current_step:  384\n",
      "2017-08-22T05:27:21.997938: step 385, loss 0.545977, acc 0.82\n",
      "current_step:  385\n",
      "2017-08-22T05:27:22.262183: step 386, loss 0.444559, acc 0.92\n",
      "current_step:  386\n",
      "2017-08-22T05:27:22.523584: step 387, loss 0.355938, acc 0.92\n",
      "current_step:  387\n",
      "2017-08-22T05:27:22.788267: step 388, loss 0.353424, acc 0.98\n",
      "current_step:  388\n",
      "2017-08-22T05:27:23.050103: step 389, loss 0.458462, acc 0.9\n",
      "current_step:  389\n",
      "2017-08-22T05:27:23.313070: step 390, loss 0.33634, acc 0.94\n",
      "current_step:  390\n",
      "2017-08-22T05:27:23.575980: step 391, loss 0.580862, acc 0.8\n",
      "current_step:  391\n",
      "2017-08-22T05:27:23.838386: step 392, loss 0.450464, acc 0.84\n",
      "current_step:  392\n",
      "2017-08-22T05:27:24.100040: step 393, loss 0.513617, acc 0.86\n",
      "current_step:  393\n",
      "2017-08-22T05:27:24.362181: step 394, loss 0.476425, acc 0.8\n",
      "current_step:  394\n",
      "2017-08-22T05:27:24.625605: step 395, loss 0.505078, acc 0.84\n",
      "current_step:  395\n",
      "2017-08-22T05:27:24.888475: step 396, loss 0.42381, acc 0.9\n",
      "current_step:  396\n",
      "2017-08-22T05:27:25.152096: step 397, loss 0.46534, acc 0.9\n",
      "current_step:  397\n",
      "2017-08-22T05:27:25.416081: step 398, loss 0.28725, acc 0.96\n",
      "current_step:  398\n",
      "2017-08-22T05:27:25.679448: step 399, loss 0.483371, acc 0.86\n",
      "current_step:  399\n",
      "2017-08-22T05:27:25.940518: step 400, loss 0.417518, acc 0.92\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:27:26.894436: step 400, loss 0.925267, acc 0.647687\n",
      "\n",
      "2017-08-22T05:27:27.157736: step 401, loss 0.371339, acc 0.9\n",
      "current_step:  401\n",
      "2017-08-22T05:27:27.419953: step 402, loss 0.292862, acc 0.96\n",
      "current_step:  402\n",
      "2017-08-22T05:27:27.683608: step 403, loss 0.455588, acc 0.84\n",
      "current_step:  403\n",
      "2017-08-22T05:27:27.946823: step 404, loss 0.455426, acc 0.86\n",
      "current_step:  404\n",
      "2017-08-22T05:27:28.210030: step 405, loss 0.368437, acc 0.9\n",
      "current_step:  405\n",
      "2017-08-22T05:27:28.471859: step 406, loss 0.456473, acc 0.9\n",
      "current_step:  406\n",
      "2017-08-22T05:27:28.736458: step 407, loss 0.444745, acc 0.84\n",
      "current_step:  407\n",
      "2017-08-22T05:27:28.790300: step 408, loss 0.440233, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-22T05:27:29.054019: step 409, loss 0.392999, acc 0.92\n",
      "current_step:  409\n",
      "2017-08-22T05:27:29.316224: step 410, loss 0.224073, acc 0.98\n",
      "current_step:  410\n",
      "2017-08-22T05:27:29.579712: step 411, loss 0.321809, acc 0.96\n",
      "current_step:  411\n",
      "2017-08-22T05:27:29.843456: step 412, loss 0.370201, acc 0.94\n",
      "current_step:  412\n",
      "2017-08-22T05:27:30.109017: step 413, loss 0.276802, acc 0.94\n",
      "current_step:  413\n",
      "2017-08-22T05:27:30.370822: step 414, loss 0.345198, acc 0.94\n",
      "current_step:  414\n",
      "2017-08-22T05:27:30.633370: step 415, loss 0.380513, acc 0.92\n",
      "current_step:  415\n",
      "2017-08-22T05:27:30.894057: step 416, loss 0.268814, acc 0.96\n",
      "current_step:  416\n",
      "2017-08-22T05:27:31.153798: step 417, loss 0.275187, acc 0.96\n",
      "current_step:  417\n",
      "2017-08-22T05:27:31.415945: step 418, loss 0.321485, acc 0.92\n",
      "current_step:  418\n",
      "2017-08-22T05:27:31.676992: step 419, loss 0.272218, acc 0.98\n",
      "current_step:  419\n",
      "2017-08-22T05:27:31.939679: step 420, loss 0.260653, acc 0.98\n",
      "current_step:  420\n",
      "2017-08-22T05:27:32.203567: step 421, loss 0.238912, acc 0.96\n",
      "current_step:  421\n",
      "2017-08-22T05:27:32.467941: step 422, loss 0.343036, acc 0.94\n",
      "current_step:  422\n",
      "2017-08-22T05:27:32.733137: step 423, loss 0.228486, acc 0.98\n",
      "current_step:  423\n",
      "2017-08-22T05:27:32.994634: step 424, loss 0.298438, acc 0.98\n",
      "current_step:  424\n",
      "2017-08-22T05:27:33.258375: step 425, loss 0.278761, acc 0.96\n",
      "current_step:  425\n",
      "2017-08-22T05:27:33.520994: step 426, loss 0.20047, acc 0.98\n",
      "current_step:  426\n",
      "2017-08-22T05:27:33.781721: step 427, loss 0.312876, acc 0.94\n",
      "current_step:  427\n",
      "2017-08-22T05:27:34.041138: step 428, loss 0.340006, acc 0.94\n",
      "current_step:  428\n",
      "2017-08-22T05:27:34.303517: step 429, loss 0.303508, acc 0.92\n",
      "current_step:  429\n",
      "2017-08-22T05:27:34.566093: step 430, loss 0.413421, acc 0.86\n",
      "current_step:  430\n",
      "2017-08-22T05:27:34.828604: step 431, loss 0.282692, acc 0.94\n",
      "current_step:  431\n",
      "2017-08-22T05:27:35.089268: step 432, loss 0.347167, acc 0.92\n",
      "current_step:  432\n",
      "2017-08-22T05:27:35.349782: step 433, loss 0.2767, acc 0.98\n",
      "current_step:  433\n",
      "2017-08-22T05:27:35.610008: step 434, loss 0.356873, acc 0.9\n",
      "current_step:  434\n",
      "2017-08-22T05:27:35.872485: step 435, loss 0.257155, acc 0.98\n",
      "current_step:  435\n",
      "2017-08-22T05:27:36.133397: step 436, loss 0.257735, acc 0.94\n",
      "current_step:  436\n",
      "2017-08-22T05:27:36.394011: step 437, loss 0.36723, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-22T05:27:36.656559: step 438, loss 0.227928, acc 0.96\n",
      "current_step:  438\n",
      "2017-08-22T05:27:36.919732: step 439, loss 0.263613, acc 0.94\n",
      "current_step:  439\n",
      "2017-08-22T05:27:37.182335: step 440, loss 0.452835, acc 0.88\n",
      "current_step:  440\n",
      "2017-08-22T05:27:37.444417: step 441, loss 0.312804, acc 0.92\n",
      "current_step:  441\n",
      "2017-08-22T05:27:37.706785: step 442, loss 0.276008, acc 0.96\n",
      "current_step:  442\n",
      "2017-08-22T05:27:37.969051: step 443, loss 0.282876, acc 0.9\n",
      "current_step:  443\n",
      "2017-08-22T05:27:38.230308: step 444, loss 0.305863, acc 0.96\n",
      "current_step:  444\n",
      "2017-08-22T05:27:38.490055: step 445, loss 0.292849, acc 0.94\n",
      "current_step:  445\n",
      "2017-08-22T05:27:38.751926: step 446, loss 0.356649, acc 0.86\n",
      "current_step:  446\n",
      "2017-08-22T05:27:39.013792: step 447, loss 0.295255, acc 0.96\n",
      "current_step:  447\n",
      "2017-08-22T05:27:39.274841: step 448, loss 0.383119, acc 0.9\n",
      "current_step:  448\n",
      "2017-08-22T05:27:39.536020: step 449, loss 0.259294, acc 0.94\n",
      "current_step:  449\n",
      "2017-08-22T05:27:39.797967: step 450, loss 0.310655, acc 0.98\n",
      "current_step:  450\n",
      "2017-08-22T05:27:40.061521: step 451, loss 0.290286, acc 0.96\n",
      "current_step:  451\n",
      "2017-08-22T05:27:40.326830: step 452, loss 0.290716, acc 0.98\n",
      "current_step:  452\n",
      "2017-08-22T05:27:40.589235: step 453, loss 0.21082, acc 0.98\n",
      "current_step:  453\n",
      "2017-08-22T05:27:40.853571: step 454, loss 0.325014, acc 0.92\n",
      "current_step:  454\n",
      "2017-08-22T05:27:41.117583: step 455, loss 0.400093, acc 0.92\n",
      "current_step:  455\n",
      "2017-08-22T05:27:41.378063: step 456, loss 0.261834, acc 0.94\n",
      "current_step:  456\n",
      "2017-08-22T05:27:41.639525: step 457, loss 0.329709, acc 0.9\n",
      "current_step:  457\n",
      "2017-08-22T05:27:41.900948: step 458, loss 0.387978, acc 0.92\n",
      "current_step:  458\n",
      "2017-08-22T05:27:42.162745: step 459, loss 0.288214, acc 0.94\n",
      "current_step:  459\n",
      "2017-08-22T05:27:42.422927: step 460, loss 0.277105, acc 0.96\n",
      "current_step:  460\n",
      "2017-08-22T05:27:42.685772: step 461, loss 0.26384, acc 0.98\n",
      "current_step:  461\n",
      "2017-08-22T05:27:42.949078: step 462, loss 0.265812, acc 1\n",
      "current_step:  462\n",
      "2017-08-22T05:27:43.211368: step 463, loss 0.219022, acc 1\n",
      "current_step:  463\n",
      "2017-08-22T05:27:43.472380: step 464, loss 0.433501, acc 0.9\n",
      "current_step:  464\n",
      "2017-08-22T05:27:43.736547: step 465, loss 0.28665, acc 0.96\n",
      "current_step:  465\n",
      "2017-08-22T05:27:43.998871: step 466, loss 0.310626, acc 0.92\n",
      "current_step:  466\n",
      "2017-08-22T05:27:44.261508: step 467, loss 0.312292, acc 0.94\n",
      "current_step:  467\n",
      "2017-08-22T05:27:44.523404: step 468, loss 0.386353, acc 0.9\n",
      "current_step:  468\n",
      "2017-08-22T05:27:44.789325: step 469, loss 0.318422, acc 0.94\n",
      "current_step:  469\n",
      "2017-08-22T05:27:45.051033: step 470, loss 0.390582, acc 0.94\n",
      "current_step:  470\n",
      "2017-08-22T05:27:45.313957: step 471, loss 0.336879, acc 0.88\n",
      "current_step:  471\n",
      "2017-08-22T05:27:45.576532: step 472, loss 0.290697, acc 0.9\n",
      "current_step:  472\n",
      "2017-08-22T05:27:45.841258: step 473, loss 0.314646, acc 0.9\n",
      "current_step:  473\n",
      "2017-08-22T05:27:46.105300: step 474, loss 0.242547, acc 0.94\n",
      "current_step:  474\n",
      "2017-08-22T05:27:46.366375: step 475, loss 0.320699, acc 0.88\n",
      "current_step:  475\n",
      "2017-08-22T05:27:46.631283: step 476, loss 0.280668, acc 0.92\n",
      "current_step:  476\n",
      "2017-08-22T05:27:46.894112: step 477, loss 0.404926, acc 0.88\n",
      "current_step:  477\n",
      "2017-08-22T05:27:47.156318: step 478, loss 0.330121, acc 0.94\n",
      "current_step:  478\n",
      "2017-08-22T05:27:47.419258: step 479, loss 0.403238, acc 0.9\n",
      "current_step:  479\n",
      "2017-08-22T05:27:47.684984: step 480, loss 0.416107, acc 0.88\n",
      "current_step:  480\n",
      "2017-08-22T05:27:47.947524: step 481, loss 0.421345, acc 0.88\n",
      "current_step:  481\n",
      "2017-08-22T05:27:48.210043: step 482, loss 0.393336, acc 0.96\n",
      "current_step:  482\n",
      "2017-08-22T05:27:48.471786: step 483, loss 0.418859, acc 0.88\n",
      "current_step:  483\n",
      "2017-08-22T05:27:48.736094: step 484, loss 0.460694, acc 0.86\n",
      "current_step:  484\n",
      "2017-08-22T05:27:48.998913: step 485, loss 0.380031, acc 0.9\n",
      "current_step:  485\n",
      "2017-08-22T05:27:49.260958: step 486, loss 0.320407, acc 0.9\n",
      "current_step:  486\n",
      "2017-08-22T05:27:49.521851: step 487, loss 0.283641, acc 0.92\n",
      "current_step:  487\n",
      "2017-08-22T05:27:49.785769: step 488, loss 0.364796, acc 0.9\n",
      "current_step:  488\n",
      "2017-08-22T05:27:50.048075: step 489, loss 0.290056, acc 0.92\n",
      "current_step:  489\n",
      "2017-08-22T05:27:50.310166: step 490, loss 0.33757, acc 0.96\n",
      "current_step:  490\n",
      "2017-08-22T05:27:50.572461: step 491, loss 0.341648, acc 0.94\n",
      "current_step:  491\n",
      "2017-08-22T05:27:50.836093: step 492, loss 0.33446, acc 0.92\n",
      "current_step:  492\n",
      "2017-08-22T05:27:51.099413: step 493, loss 0.25127, acc 0.96\n",
      "current_step:  493\n",
      "2017-08-22T05:27:51.360133: step 494, loss 0.354227, acc 0.94\n",
      "current_step:  494\n",
      "2017-08-22T05:27:51.621514: step 495, loss 0.227986, acc 0.98\n",
      "current_step:  495\n",
      "2017-08-22T05:27:51.881452: step 496, loss 0.330822, acc 0.92\n",
      "current_step:  496\n",
      "2017-08-22T05:27:52.141913: step 497, loss 0.390274, acc 0.9\n",
      "current_step:  497\n",
      "2017-08-22T05:27:52.403516: step 498, loss 0.313684, acc 0.96\n",
      "current_step:  498\n",
      "2017-08-22T05:27:52.666107: step 499, loss 0.418379, acc 0.9\n",
      "current_step:  499\n",
      "2017-08-22T05:27:52.926115: step 500, loss 0.255154, acc 0.94\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:27:53.878221: step 500, loss 0.899359, acc 0.677936\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0525_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:27:54.214899: step 501, loss 0.267996, acc 0.94\n",
      "current_step:  501\n",
      "2017-08-22T05:27:54.478227: step 502, loss 0.274051, acc 0.94\n",
      "current_step:  502\n",
      "2017-08-22T05:27:54.740395: step 503, loss 0.343374, acc 0.94\n",
      "current_step:  503\n",
      "2017-08-22T05:27:55.002415: step 504, loss 0.355265, acc 0.88\n",
      "current_step:  504\n",
      "2017-08-22T05:27:55.263558: step 505, loss 0.375898, acc 0.88\n",
      "current_step:  505\n",
      "2017-08-22T05:27:55.526246: step 506, loss 0.244446, acc 0.98\n",
      "current_step:  506\n",
      "2017-08-22T05:27:55.789232: step 507, loss 0.237326, acc 0.96\n",
      "current_step:  507\n",
      "2017-08-22T05:27:56.051140: step 508, loss 0.304063, acc 0.92\n",
      "current_step:  508\n",
      "2017-08-22T05:27:56.312518: step 509, loss 0.36999, acc 0.9\n",
      "current_step:  509\n",
      "2017-08-22T05:27:56.366073: step 510, loss 0.246196, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.524830\n",
      "Precision Score = 0.508287\n",
      "Recall Score = 0.587189\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.598904\n",
      "Precision Score = 0.575033\n",
      "Recall Score = 0.647687\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.603600\n",
      "Precision Score = 0.573151\n",
      "Recall Score = 0.653025\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.600551\n",
      "Precision Score = 0.577176\n",
      "Recall Score = 0.644128\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.632943\n",
      "Precision Score = 0.614354\n",
      "Recall Score = 0.674377\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v1 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 1, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  205\n",
      "Vocabulary Size: 13764\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0528_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0528_UTC\n",
      "2017-08-22T05:28:14.099204: step 1, loss 1.82973, acc 0.58\n",
      "current_step:  1\n",
      "2017-08-22T05:28:14.201874: step 2, loss 1.70845, acc 0.52\n",
      "current_step:  2\n",
      "2017-08-22T05:28:14.303171: step 3, loss 1.26824, acc 0.46\n",
      "current_step:  3\n",
      "2017-08-22T05:28:14.405191: step 4, loss 1.38592, acc 0.48\n",
      "current_step:  4\n",
      "2017-08-22T05:28:14.504166: step 5, loss 1.33992, acc 0.44\n",
      "current_step:  5\n",
      "2017-08-22T05:28:14.605351: step 6, loss 1.35852, acc 0.48\n",
      "current_step:  6\n",
      "2017-08-22T05:28:14.705420: step 7, loss 1.21025, acc 0.38\n",
      "current_step:  7\n",
      "2017-08-22T05:28:14.805403: step 8, loss 1.21597, acc 0.54\n",
      "current_step:  8\n",
      "2017-08-22T05:28:14.904699: step 9, loss 1.18647, acc 0.58\n",
      "current_step:  9\n",
      "2017-08-22T05:28:15.003416: step 10, loss 1.20133, acc 0.5\n",
      "current_step:  10\n",
      "2017-08-22T05:28:15.102102: step 11, loss 1.42529, acc 0.52\n",
      "current_step:  11\n",
      "2017-08-22T05:28:15.203097: step 12, loss 1.32457, acc 0.48\n",
      "current_step:  12\n",
      "2017-08-22T05:28:15.303963: step 13, loss 1.33743, acc 0.52\n",
      "current_step:  13\n",
      "2017-08-22T05:28:15.405592: step 14, loss 1.49721, acc 0.54\n",
      "current_step:  14\n",
      "2017-08-22T05:28:15.504633: step 15, loss 1.55612, acc 0.46\n",
      "current_step:  15\n",
      "2017-08-22T05:28:15.609830: step 16, loss 1.29842, acc 0.5\n",
      "current_step:  16\n",
      "2017-08-22T05:28:15.738809: step 17, loss 1.1935, acc 0.5\n",
      "current_step:  17\n",
      "2017-08-22T05:28:15.909094: step 18, loss 1.12299, acc 0.54\n",
      "current_step:  18\n",
      "2017-08-22T05:28:16.079225: step 19, loss 1.41783, acc 0.44\n",
      "current_step:  19\n",
      "2017-08-22T05:28:16.247371: step 20, loss 1.19024, acc 0.44\n",
      "current_step:  20\n",
      "2017-08-22T05:28:16.416774: step 21, loss 1.13602, acc 0.62\n",
      "current_step:  21\n",
      "2017-08-22T05:28:16.585382: step 22, loss 1.37328, acc 0.44\n",
      "current_step:  22\n",
      "2017-08-22T05:28:16.754536: step 23, loss 1.21157, acc 0.58\n",
      "current_step:  23\n",
      "2017-08-22T05:28:16.922626: step 24, loss 1.48051, acc 0.4\n",
      "current_step:  24\n",
      "2017-08-22T05:28:17.091952: step 25, loss 1.23655, acc 0.52\n",
      "current_step:  25\n",
      "2017-08-22T05:28:17.260588: step 26, loss 1.28449, acc 0.5\n",
      "current_step:  26\n",
      "2017-08-22T05:28:17.429713: step 27, loss 1.48648, acc 0.52\n",
      "current_step:  27\n",
      "2017-08-22T05:28:17.599000: step 28, loss 0.97749, acc 0.66\n",
      "current_step:  28\n",
      "2017-08-22T05:28:17.770848: step 29, loss 1.19109, acc 0.54\n",
      "current_step:  29\n",
      "2017-08-22T05:28:17.939682: step 30, loss 1.15473, acc 0.52\n",
      "current_step:  30\n",
      "2017-08-22T05:28:18.110610: step 31, loss 1.57789, acc 0.4\n",
      "current_step:  31\n",
      "2017-08-22T05:28:18.279872: step 32, loss 1.09406, acc 0.58\n",
      "current_step:  32\n",
      "2017-08-22T05:28:18.451347: step 33, loss 1.42, acc 0.46\n",
      "current_step:  33\n",
      "2017-08-22T05:28:18.620295: step 34, loss 1.22647, acc 0.52\n",
      "current_step:  34\n",
      "2017-08-22T05:28:18.792630: step 35, loss 1.03707, acc 0.64\n",
      "current_step:  35\n",
      "2017-08-22T05:28:18.961481: step 36, loss 1.12622, acc 0.6\n",
      "current_step:  36\n",
      "2017-08-22T05:28:19.132513: step 37, loss 1.43108, acc 0.42\n",
      "current_step:  37\n",
      "2017-08-22T05:28:19.302072: step 38, loss 1.51845, acc 0.44\n",
      "current_step:  38\n",
      "2017-08-22T05:28:19.471501: step 39, loss 1.48205, acc 0.5\n",
      "current_step:  39\n",
      "2017-08-22T05:28:19.641285: step 40, loss 1.19115, acc 0.52\n",
      "current_step:  40\n",
      "2017-08-22T05:28:19.812830: step 41, loss 1.02795, acc 0.66\n",
      "current_step:  41\n",
      "2017-08-22T05:28:19.982312: step 42, loss 1.19942, acc 0.5\n",
      "current_step:  42\n",
      "2017-08-22T05:28:20.153037: step 43, loss 1.07259, acc 0.6\n",
      "current_step:  43\n",
      "2017-08-22T05:28:20.321468: step 44, loss 0.913738, acc 0.64\n",
      "current_step:  44\n",
      "2017-08-22T05:28:20.490960: step 45, loss 1.18016, acc 0.44\n",
      "current_step:  45\n",
      "2017-08-22T05:28:20.661417: step 46, loss 1.21279, acc 0.54\n",
      "current_step:  46\n",
      "2017-08-22T05:28:20.832266: step 47, loss 0.960898, acc 0.6\n",
      "current_step:  47\n",
      "2017-08-22T05:28:21.002535: step 48, loss 1.20033, acc 0.56\n",
      "current_step:  48\n",
      "2017-08-22T05:28:21.171815: step 49, loss 1.31298, acc 0.64\n",
      "current_step:  49\n",
      "2017-08-22T05:28:21.340937: step 50, loss 0.945662, acc 0.64\n",
      "current_step:  50\n",
      "2017-08-22T05:28:21.510675: step 51, loss 0.937616, acc 0.58\n",
      "current_step:  51\n",
      "2017-08-22T05:28:21.681994: step 52, loss 1.22094, acc 0.54\n",
      "current_step:  52\n",
      "2017-08-22T05:28:21.852869: step 53, loss 1.39374, acc 0.52\n",
      "current_step:  53\n",
      "2017-08-22T05:28:22.021499: step 54, loss 1.24667, acc 0.5\n",
      "current_step:  54\n",
      "2017-08-22T05:28:22.194064: step 55, loss 1.15346, acc 0.52\n",
      "current_step:  55\n",
      "2017-08-22T05:28:22.369018: step 56, loss 1.0153, acc 0.66\n",
      "current_step:  56\n",
      "2017-08-22T05:28:22.551618: step 57, loss 1.06485, acc 0.54\n",
      "current_step:  57\n",
      "2017-08-22T05:28:22.722416: step 58, loss 1.11501, acc 0.68\n",
      "current_step:  58\n",
      "2017-08-22T05:28:22.893303: step 59, loss 1.36249, acc 0.5\n",
      "current_step:  59\n",
      "2017-08-22T05:28:23.063419: step 60, loss 1.11461, acc 0.66\n",
      "current_step:  60\n",
      "2017-08-22T05:28:23.233724: step 61, loss 1.01909, acc 0.62\n",
      "current_step:  61\n",
      "2017-08-22T05:28:23.401538: step 62, loss 1.39652, acc 0.56\n",
      "current_step:  62\n",
      "2017-08-22T05:28:23.572558: step 63, loss 1.34882, acc 0.5\n",
      "current_step:  63\n",
      "2017-08-22T05:28:23.743902: step 64, loss 1.18551, acc 0.5\n",
      "current_step:  64\n",
      "2017-08-22T05:28:23.914201: step 65, loss 1.05098, acc 0.62\n",
      "current_step:  65\n",
      "2017-08-22T05:28:24.085430: step 66, loss 1.0594, acc 0.6\n",
      "current_step:  66\n",
      "2017-08-22T05:28:24.255605: step 67, loss 1.12397, acc 0.64\n",
      "current_step:  67\n",
      "2017-08-22T05:28:24.425217: step 68, loss 1.1404, acc 0.6\n",
      "current_step:  68\n",
      "2017-08-22T05:28:24.596955: step 69, loss 1.06407, acc 0.66\n",
      "current_step:  69\n",
      "2017-08-22T05:28:24.767378: step 70, loss 1.20055, acc 0.52\n",
      "current_step:  70\n",
      "2017-08-22T05:28:24.936053: step 71, loss 1.10924, acc 0.58\n",
      "current_step:  71\n",
      "2017-08-22T05:28:25.106859: step 72, loss 1.20252, acc 0.5\n",
      "current_step:  72\n",
      "2017-08-22T05:28:25.275366: step 73, loss 1.21911, acc 0.6\n",
      "current_step:  73\n",
      "2017-08-22T05:28:25.444773: step 74, loss 1.09225, acc 0.6\n",
      "current_step:  74\n",
      "2017-08-22T05:28:25.616513: step 75, loss 1.08891, acc 0.62\n",
      "current_step:  75\n",
      "2017-08-22T05:28:25.788827: step 76, loss 1.11231, acc 0.6\n",
      "current_step:  76\n",
      "2017-08-22T05:28:25.957485: step 77, loss 1.1819, acc 0.6\n",
      "current_step:  77\n",
      "2017-08-22T05:28:26.125557: step 78, loss 0.969813, acc 0.66\n",
      "current_step:  78\n",
      "2017-08-22T05:28:26.294120: step 79, loss 1.41163, acc 0.44\n",
      "current_step:  79\n",
      "2017-08-22T05:28:26.462764: step 80, loss 0.887787, acc 0.64\n",
      "current_step:  80\n",
      "2017-08-22T05:28:26.633080: step 81, loss 1.2613, acc 0.6\n",
      "current_step:  81\n",
      "2017-08-22T05:28:26.803884: step 82, loss 0.993267, acc 0.6\n",
      "current_step:  82\n",
      "2017-08-22T05:28:26.973028: step 83, loss 1.11024, acc 0.52\n",
      "current_step:  83\n",
      "2017-08-22T05:28:27.141739: step 84, loss 1.1134, acc 0.58\n",
      "current_step:  84\n",
      "2017-08-22T05:28:27.311171: step 85, loss 1.01602, acc 0.64\n",
      "current_step:  85\n",
      "2017-08-22T05:28:27.479784: step 86, loss 1.1309, acc 0.6\n",
      "current_step:  86\n",
      "2017-08-22T05:28:27.650336: step 87, loss 1.11062, acc 0.56\n",
      "current_step:  87\n",
      "2017-08-22T05:28:27.819766: step 88, loss 1.10857, acc 0.68\n",
      "current_step:  88\n",
      "2017-08-22T05:28:27.989517: step 89, loss 1.02681, acc 0.62\n",
      "current_step:  89\n",
      "2017-08-22T05:28:28.160466: step 90, loss 1.18717, acc 0.62\n",
      "current_step:  90\n",
      "2017-08-22T05:28:28.330448: step 91, loss 1.18576, acc 0.58\n",
      "current_step:  91\n",
      "2017-08-22T05:28:28.501505: step 92, loss 0.934667, acc 0.72\n",
      "current_step:  92\n",
      "2017-08-22T05:28:28.671727: step 93, loss 1.01188, acc 0.62\n",
      "current_step:  93\n",
      "2017-08-22T05:28:28.842236: step 94, loss 1.30089, acc 0.54\n",
      "current_step:  94\n",
      "2017-08-22T05:28:29.012587: step 95, loss 1.23097, acc 0.52\n",
      "current_step:  95\n",
      "2017-08-22T05:28:29.181363: step 96, loss 0.942588, acc 0.66\n",
      "current_step:  96\n",
      "2017-08-22T05:28:29.352028: step 97, loss 0.921289, acc 0.5\n",
      "current_step:  97\n",
      "2017-08-22T05:28:29.521912: step 98, loss 1.32824, acc 0.48\n",
      "current_step:  98\n",
      "2017-08-22T05:28:29.693082: step 99, loss 1.02019, acc 0.54\n",
      "current_step:  99\n",
      "2017-08-22T05:28:29.863657: step 100, loss 1.1428, acc 0.62\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:28:30.481549: step 100, loss 1.01901, acc 0.622776\n",
      "\n",
      "2017-08-22T05:28:30.651477: step 101, loss 0.926646, acc 0.68\n",
      "current_step:  101\n",
      "2017-08-22T05:28:30.687450: step 102, loss 1.28273, acc 0.444444\n",
      "current_step:  102\n",
      "2017-08-22T05:28:30.857464: step 103, loss 0.707979, acc 0.78\n",
      "current_step:  103\n",
      "2017-08-22T05:28:31.027261: step 104, loss 0.690024, acc 0.82\n",
      "current_step:  104\n",
      "2017-08-22T05:28:31.195754: step 105, loss 0.799119, acc 0.76\n",
      "current_step:  105\n",
      "2017-08-22T05:28:31.364644: step 106, loss 0.918635, acc 0.72\n",
      "current_step:  106\n",
      "2017-08-22T05:28:31.535179: step 107, loss 0.965877, acc 0.66\n",
      "current_step:  107\n",
      "2017-08-22T05:28:31.707060: step 108, loss 0.828133, acc 0.72\n",
      "current_step:  108\n",
      "2017-08-22T05:28:31.884891: step 109, loss 0.739173, acc 0.74\n",
      "current_step:  109\n",
      "2017-08-22T05:28:32.064299: step 110, loss 0.659363, acc 0.74\n",
      "current_step:  110\n",
      "2017-08-22T05:28:32.233374: step 111, loss 0.736291, acc 0.72\n",
      "current_step:  111\n",
      "2017-08-22T05:28:32.402724: step 112, loss 0.827832, acc 0.76\n",
      "current_step:  112\n",
      "2017-08-22T05:28:32.572062: step 113, loss 0.710141, acc 0.78\n",
      "current_step:  113\n",
      "2017-08-22T05:28:32.781414: step 114, loss 0.70521, acc 0.8\n",
      "current_step:  114\n",
      "2017-08-22T05:28:32.956184: step 115, loss 0.954367, acc 0.74\n",
      "current_step:  115\n",
      "2017-08-22T05:28:33.125847: step 116, loss 1.0266, acc 0.64\n",
      "current_step:  116\n",
      "2017-08-22T05:28:33.294668: step 117, loss 0.842823, acc 0.72\n",
      "current_step:  117\n",
      "2017-08-22T05:28:33.463347: step 118, loss 0.722334, acc 0.84\n",
      "current_step:  118\n",
      "2017-08-22T05:28:33.634643: step 119, loss 0.852407, acc 0.64\n",
      "current_step:  119\n",
      "2017-08-22T05:28:33.806811: step 120, loss 0.96968, acc 0.68\n",
      "current_step:  120\n",
      "2017-08-22T05:28:33.975414: step 121, loss 0.914594, acc 0.68\n",
      "current_step:  121\n",
      "2017-08-22T05:28:34.146110: step 122, loss 0.977233, acc 0.62\n",
      "current_step:  122\n",
      "2017-08-22T05:28:34.316670: step 123, loss 0.97224, acc 0.7\n",
      "current_step:  123\n",
      "2017-08-22T05:28:34.487262: step 124, loss 0.740886, acc 0.72\n",
      "current_step:  124\n",
      "2017-08-22T05:28:34.656736: step 125, loss 0.862644, acc 0.76\n",
      "current_step:  125\n",
      "2017-08-22T05:28:34.826530: step 126, loss 0.702893, acc 0.78\n",
      "current_step:  126\n",
      "2017-08-22T05:28:34.995392: step 127, loss 0.898385, acc 0.72\n",
      "current_step:  127\n",
      "2017-08-22T05:28:35.165494: step 128, loss 0.896082, acc 0.64\n",
      "current_step:  128\n",
      "2017-08-22T05:28:35.337267: step 129, loss 0.935714, acc 0.64\n",
      "current_step:  129\n",
      "2017-08-22T05:28:35.507422: step 130, loss 0.930526, acc 0.74\n",
      "current_step:  130\n",
      "2017-08-22T05:28:35.679998: step 131, loss 0.696428, acc 0.8\n",
      "current_step:  131\n",
      "2017-08-22T05:28:35.850809: step 132, loss 0.784844, acc 0.68\n",
      "current_step:  132\n",
      "2017-08-22T05:28:36.020442: step 133, loss 0.757018, acc 0.72\n",
      "current_step:  133\n",
      "2017-08-22T05:28:36.190648: step 134, loss 1.00141, acc 0.62\n",
      "current_step:  134\n",
      "2017-08-22T05:28:36.358706: step 135, loss 0.714591, acc 0.8\n",
      "current_step:  135\n",
      "2017-08-22T05:28:36.527328: step 136, loss 0.870675, acc 0.64\n",
      "current_step:  136\n",
      "2017-08-22T05:28:36.698255: step 137, loss 0.931867, acc 0.66\n",
      "current_step:  137\n",
      "2017-08-22T05:28:36.868894: step 138, loss 0.96233, acc 0.6\n",
      "current_step:  138\n",
      "2017-08-22T05:28:37.039417: step 139, loss 1.05072, acc 0.6\n",
      "current_step:  139\n",
      "2017-08-22T05:28:37.208801: step 140, loss 0.869103, acc 0.64\n",
      "current_step:  140\n",
      "2017-08-22T05:28:37.378216: step 141, loss 0.989576, acc 0.64\n",
      "current_step:  141\n",
      "2017-08-22T05:28:37.548144: step 142, loss 1.01405, acc 0.68\n",
      "current_step:  142\n",
      "2017-08-22T05:28:37.723182: step 143, loss 0.722934, acc 0.78\n",
      "current_step:  143\n",
      "2017-08-22T05:28:37.892299: step 144, loss 0.91135, acc 0.72\n",
      "current_step:  144\n",
      "2017-08-22T05:28:38.062403: step 145, loss 0.951356, acc 0.6\n",
      "current_step:  145\n",
      "2017-08-22T05:28:38.232589: step 146, loss 0.883425, acc 0.78\n",
      "current_step:  146\n",
      "2017-08-22T05:28:38.401597: step 147, loss 0.817142, acc 0.64\n",
      "current_step:  147\n",
      "2017-08-22T05:28:38.571231: step 148, loss 0.927115, acc 0.68\n",
      "current_step:  148\n",
      "2017-08-22T05:28:38.740174: step 149, loss 0.822111, acc 0.76\n",
      "current_step:  149\n",
      "2017-08-22T05:28:38.910050: step 150, loss 0.891711, acc 0.64\n",
      "current_step:  150\n",
      "2017-08-22T05:28:39.078756: step 151, loss 0.752239, acc 0.74\n",
      "current_step:  151\n",
      "2017-08-22T05:28:39.248697: step 152, loss 0.851056, acc 0.7\n",
      "current_step:  152\n",
      "2017-08-22T05:28:39.418188: step 153, loss 0.925039, acc 0.7\n",
      "current_step:  153\n",
      "2017-08-22T05:28:39.587228: step 154, loss 0.909777, acc 0.74\n",
      "current_step:  154\n",
      "2017-08-22T05:28:39.756313: step 155, loss 0.817006, acc 0.72\n",
      "current_step:  155\n",
      "2017-08-22T05:28:39.926300: step 156, loss 0.794332, acc 0.78\n",
      "current_step:  156\n",
      "2017-08-22T05:28:40.095991: step 157, loss 0.697209, acc 0.78\n",
      "current_step:  157\n",
      "2017-08-22T05:28:40.266865: step 158, loss 0.759731, acc 0.8\n",
      "current_step:  158\n",
      "2017-08-22T05:28:40.436507: step 159, loss 0.802247, acc 0.72\n",
      "current_step:  159\n",
      "2017-08-22T05:28:40.606017: step 160, loss 0.897697, acc 0.7\n",
      "current_step:  160\n",
      "2017-08-22T05:28:40.775177: step 161, loss 0.663361, acc 0.78\n",
      "current_step:  161\n",
      "2017-08-22T05:28:40.945876: step 162, loss 0.729222, acc 0.76\n",
      "current_step:  162\n",
      "2017-08-22T05:28:41.115608: step 163, loss 0.843982, acc 0.7\n",
      "current_step:  163\n",
      "2017-08-22T05:28:41.285516: step 164, loss 0.654269, acc 0.78\n",
      "current_step:  164\n",
      "2017-08-22T05:28:41.454169: step 165, loss 0.740713, acc 0.78\n",
      "current_step:  165\n",
      "2017-08-22T05:28:41.625439: step 166, loss 0.816729, acc 0.72\n",
      "current_step:  166\n",
      "2017-08-22T05:28:41.794664: step 167, loss 0.709221, acc 0.72\n",
      "current_step:  167\n",
      "2017-08-22T05:28:41.963856: step 168, loss 1.03982, acc 0.62\n",
      "current_step:  168\n",
      "2017-08-22T05:28:42.132798: step 169, loss 0.59813, acc 0.9\n",
      "current_step:  169\n",
      "2017-08-22T05:28:42.301748: step 170, loss 0.542646, acc 0.82\n",
      "current_step:  170\n",
      "2017-08-22T05:28:42.470927: step 171, loss 0.931243, acc 0.64\n",
      "current_step:  171\n",
      "2017-08-22T05:28:42.640000: step 172, loss 0.800527, acc 0.72\n",
      "current_step:  172\n",
      "2017-08-22T05:28:42.811088: step 173, loss 0.767858, acc 0.72\n",
      "current_step:  173\n",
      "2017-08-22T05:28:42.980985: step 174, loss 0.862735, acc 0.66\n",
      "current_step:  174\n",
      "2017-08-22T05:28:43.151863: step 175, loss 0.917394, acc 0.66\n",
      "current_step:  175\n",
      "2017-08-22T05:28:43.322484: step 176, loss 0.868066, acc 0.76\n",
      "current_step:  176\n",
      "2017-08-22T05:28:43.491084: step 177, loss 1.08694, acc 0.7\n",
      "current_step:  177\n",
      "2017-08-22T05:28:43.663039: step 178, loss 0.703126, acc 0.72\n",
      "current_step:  178\n",
      "2017-08-22T05:28:43.833513: step 179, loss 0.793086, acc 0.74\n",
      "current_step:  179\n",
      "2017-08-22T05:28:44.002882: step 180, loss 0.787192, acc 0.72\n",
      "current_step:  180\n",
      "2017-08-22T05:28:44.173480: step 181, loss 0.719385, acc 0.8\n",
      "current_step:  181\n",
      "2017-08-22T05:28:44.342866: step 182, loss 0.785815, acc 0.82\n",
      "current_step:  182\n",
      "2017-08-22T05:28:44.511289: step 183, loss 0.824443, acc 0.74\n",
      "current_step:  183\n",
      "2017-08-22T05:28:44.681465: step 184, loss 0.900137, acc 0.74\n",
      "current_step:  184\n",
      "2017-08-22T05:28:44.850949: step 185, loss 0.742541, acc 0.7\n",
      "current_step:  185\n",
      "2017-08-22T05:28:45.019655: step 186, loss 0.618635, acc 0.84\n",
      "current_step:  186\n",
      "2017-08-22T05:28:45.189152: step 187, loss 0.806961, acc 0.66\n",
      "current_step:  187\n",
      "2017-08-22T05:28:45.360186: step 188, loss 0.860559, acc 0.56\n",
      "current_step:  188\n",
      "2017-08-22T05:28:45.530067: step 189, loss 0.708629, acc 0.7\n",
      "current_step:  189\n",
      "2017-08-22T05:28:45.701302: step 190, loss 0.783431, acc 0.72\n",
      "current_step:  190\n",
      "2017-08-22T05:28:45.870103: step 191, loss 0.676612, acc 0.74\n",
      "current_step:  191\n",
      "2017-08-22T05:28:46.039473: step 192, loss 0.789953, acc 0.74\n",
      "current_step:  192\n",
      "2017-08-22T05:28:46.208530: step 193, loss 0.867793, acc 0.7\n",
      "current_step:  193\n",
      "2017-08-22T05:28:46.378205: step 194, loss 0.748276, acc 0.74\n",
      "current_step:  194\n",
      "2017-08-22T05:28:46.545889: step 195, loss 0.833061, acc 0.68\n",
      "current_step:  195\n",
      "2017-08-22T05:28:46.716792: step 196, loss 0.753256, acc 0.66\n",
      "current_step:  196\n",
      "2017-08-22T05:28:46.886295: step 197, loss 0.74288, acc 0.76\n",
      "current_step:  197\n",
      "2017-08-22T05:28:47.055685: step 198, loss 0.850693, acc 0.74\n",
      "current_step:  198\n",
      "2017-08-22T05:28:47.224670: step 199, loss 0.759135, acc 0.72\n",
      "current_step:  199\n",
      "2017-08-22T05:28:47.394574: step 200, loss 0.834972, acc 0.66\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:28:48.007016: step 200, loss 0.972348, acc 0.651246\n",
      "\n",
      "2017-08-22T05:28:48.176179: step 201, loss 0.683113, acc 0.76\n",
      "current_step:  201\n",
      "2017-08-22T05:28:48.347166: step 202, loss 0.903964, acc 0.6\n",
      "current_step:  202\n",
      "2017-08-22T05:28:48.515819: step 203, loss 0.98224, acc 0.62\n",
      "current_step:  203\n",
      "2017-08-22T05:28:48.552972: step 204, loss 0.50493, acc 0.888889\n",
      "current_step:  204\n",
      "2017-08-22T05:28:48.723422: step 205, loss 0.454837, acc 0.88\n",
      "current_step:  205\n",
      "2017-08-22T05:28:48.893258: step 206, loss 0.665161, acc 0.8\n",
      "current_step:  206\n",
      "2017-08-22T05:28:49.062550: step 207, loss 0.467273, acc 0.84\n",
      "current_step:  207\n",
      "2017-08-22T05:28:49.232717: step 208, loss 0.798209, acc 0.7\n",
      "current_step:  208\n",
      "2017-08-22T05:28:49.403435: step 209, loss 0.4956, acc 0.84\n",
      "current_step:  209\n",
      "2017-08-22T05:28:49.574645: step 210, loss 0.580817, acc 0.84\n",
      "current_step:  210\n",
      "2017-08-22T05:28:49.743724: step 211, loss 0.536417, acc 0.84\n",
      "current_step:  211\n",
      "2017-08-22T05:28:49.915071: step 212, loss 0.599213, acc 0.86\n",
      "current_step:  212\n",
      "2017-08-22T05:28:50.084553: step 213, loss 0.514512, acc 0.9\n",
      "current_step:  213\n",
      "2017-08-22T05:28:50.255475: step 214, loss 0.545408, acc 0.82\n",
      "current_step:  214\n",
      "2017-08-22T05:28:50.422918: step 215, loss 0.533297, acc 0.86\n",
      "current_step:  215\n",
      "2017-08-22T05:28:50.592876: step 216, loss 0.613906, acc 0.8\n",
      "current_step:  216\n",
      "2017-08-22T05:28:50.763269: step 217, loss 0.729774, acc 0.78\n",
      "current_step:  217\n",
      "2017-08-22T05:28:50.932896: step 218, loss 0.718994, acc 0.74\n",
      "current_step:  218\n",
      "2017-08-22T05:28:51.102617: step 219, loss 0.668412, acc 0.82\n",
      "current_step:  219\n",
      "2017-08-22T05:28:51.271905: step 220, loss 0.579937, acc 0.82\n",
      "current_step:  220\n",
      "2017-08-22T05:28:51.441177: step 221, loss 0.683958, acc 0.74\n",
      "current_step:  221\n",
      "2017-08-22T05:28:51.611135: step 222, loss 0.471315, acc 0.82\n",
      "current_step:  222\n",
      "2017-08-22T05:28:51.781173: step 223, loss 0.587439, acc 0.8\n",
      "current_step:  223\n",
      "2017-08-22T05:28:51.949965: step 224, loss 0.49822, acc 0.88\n",
      "current_step:  224\n",
      "2017-08-22T05:28:52.120802: step 225, loss 0.720807, acc 0.74\n",
      "current_step:  225\n",
      "2017-08-22T05:28:52.289271: step 226, loss 0.722341, acc 0.76\n",
      "current_step:  226\n",
      "2017-08-22T05:28:52.456296: step 227, loss 0.678399, acc 0.74\n",
      "current_step:  227\n",
      "2017-08-22T05:28:52.625872: step 228, loss 0.642107, acc 0.74\n",
      "current_step:  228\n",
      "2017-08-22T05:28:52.796960: step 229, loss 0.522942, acc 0.88\n",
      "current_step:  229\n",
      "2017-08-22T05:28:52.965388: step 230, loss 0.541814, acc 0.84\n",
      "current_step:  230\n",
      "2017-08-22T05:28:53.135581: step 231, loss 0.724971, acc 0.82\n",
      "current_step:  231\n",
      "2017-08-22T05:28:53.304279: step 232, loss 0.606377, acc 0.84\n",
      "current_step:  232\n",
      "2017-08-22T05:28:53.473179: step 233, loss 0.630533, acc 0.8\n",
      "current_step:  233\n",
      "2017-08-22T05:28:53.644028: step 234, loss 0.712274, acc 0.8\n",
      "current_step:  234\n",
      "2017-08-22T05:28:53.814429: step 235, loss 0.561692, acc 0.88\n",
      "current_step:  235\n",
      "2017-08-22T05:28:53.983951: step 236, loss 0.596351, acc 0.88\n",
      "current_step:  236\n",
      "2017-08-22T05:28:54.151524: step 237, loss 0.610025, acc 0.82\n",
      "current_step:  237\n",
      "2017-08-22T05:28:54.320246: step 238, loss 0.660032, acc 0.8\n",
      "current_step:  238\n",
      "2017-08-22T05:28:54.489417: step 239, loss 0.418907, acc 0.92\n",
      "current_step:  239\n",
      "2017-08-22T05:28:54.659381: step 240, loss 0.452195, acc 0.92\n",
      "current_step:  240\n",
      "2017-08-22T05:28:54.829272: step 241, loss 0.583387, acc 0.78\n",
      "current_step:  241\n",
      "2017-08-22T05:28:54.999528: step 242, loss 0.481801, acc 0.88\n",
      "current_step:  242\n",
      "2017-08-22T05:28:55.169286: step 243, loss 0.613596, acc 0.8\n",
      "current_step:  243\n",
      "2017-08-22T05:28:55.338740: step 244, loss 0.502331, acc 0.86\n",
      "current_step:  244\n",
      "2017-08-22T05:28:55.507756: step 245, loss 0.596231, acc 0.8\n",
      "current_step:  245\n",
      "2017-08-22T05:28:55.680057: step 246, loss 0.643979, acc 0.72\n",
      "current_step:  246\n",
      "2017-08-22T05:28:55.850522: step 247, loss 0.530866, acc 0.8\n",
      "current_step:  247\n",
      "2017-08-22T05:28:56.022147: step 248, loss 0.457784, acc 0.88\n",
      "current_step:  248\n",
      "2017-08-22T05:28:56.191821: step 249, loss 0.503527, acc 0.86\n",
      "current_step:  249\n",
      "2017-08-22T05:28:56.361239: step 250, loss 0.525724, acc 0.82\n",
      "current_step:  250\n",
      "2017-08-22T05:28:56.530959: step 251, loss 0.600657, acc 0.78\n",
      "current_step:  251\n",
      "2017-08-22T05:28:56.700689: step 252, loss 0.635792, acc 0.82\n",
      "current_step:  252\n",
      "2017-08-22T05:28:56.871014: step 253, loss 0.528347, acc 0.9\n",
      "current_step:  253\n",
      "2017-08-22T05:28:57.040402: step 254, loss 0.572045, acc 0.82\n",
      "current_step:  254\n",
      "2017-08-22T05:28:57.210222: step 255, loss 0.685684, acc 0.76\n",
      "current_step:  255\n",
      "2017-08-22T05:28:57.380530: step 256, loss 0.568798, acc 0.84\n",
      "current_step:  256\n",
      "2017-08-22T05:28:57.549954: step 257, loss 0.609419, acc 0.82\n",
      "current_step:  257\n",
      "2017-08-22T05:28:57.721018: step 258, loss 0.659573, acc 0.78\n",
      "current_step:  258\n",
      "2017-08-22T05:28:57.890756: step 259, loss 0.658392, acc 0.74\n",
      "current_step:  259\n",
      "2017-08-22T05:28:58.059859: step 260, loss 0.513292, acc 0.86\n",
      "current_step:  260\n",
      "2017-08-22T05:28:58.229714: step 261, loss 0.707818, acc 0.84\n",
      "current_step:  261\n",
      "2017-08-22T05:28:58.398734: step 262, loss 0.723453, acc 0.78\n",
      "current_step:  262\n",
      "2017-08-22T05:28:58.568626: step 263, loss 0.546579, acc 0.8\n",
      "current_step:  263\n",
      "2017-08-22T05:28:58.740684: step 264, loss 0.720773, acc 0.78\n",
      "current_step:  264\n",
      "2017-08-22T05:28:58.910770: step 265, loss 0.729518, acc 0.74\n",
      "current_step:  265\n",
      "2017-08-22T05:28:59.079980: step 266, loss 0.62203, acc 0.82\n",
      "current_step:  266\n",
      "2017-08-22T05:28:59.247313: step 267, loss 0.628447, acc 0.84\n",
      "current_step:  267\n",
      "2017-08-22T05:28:59.417162: step 268, loss 0.535048, acc 0.84\n",
      "current_step:  268\n",
      "2017-08-22T05:28:59.585736: step 269, loss 0.457607, acc 0.82\n",
      "current_step:  269\n",
      "2017-08-22T05:28:59.755964: step 270, loss 0.601137, acc 0.76\n",
      "current_step:  270\n",
      "2017-08-22T05:28:59.923976: step 271, loss 0.509708, acc 0.84\n",
      "current_step:  271\n",
      "2017-08-22T05:29:00.093578: step 272, loss 0.511166, acc 0.84\n",
      "current_step:  272\n",
      "2017-08-22T05:29:00.263576: step 273, loss 0.546873, acc 0.8\n",
      "current_step:  273\n",
      "2017-08-22T05:29:00.432524: step 274, loss 0.669254, acc 0.78\n",
      "current_step:  274\n",
      "2017-08-22T05:29:00.602532: step 275, loss 0.656236, acc 0.74\n",
      "current_step:  275\n",
      "2017-08-22T05:29:00.772659: step 276, loss 0.826337, acc 0.74\n",
      "current_step:  276\n",
      "2017-08-22T05:29:00.941440: step 277, loss 0.547543, acc 0.84\n",
      "current_step:  277\n",
      "2017-08-22T05:29:01.110482: step 278, loss 0.91727, acc 0.7\n",
      "current_step:  278\n",
      "2017-08-22T05:29:01.280117: step 279, loss 0.619258, acc 0.84\n",
      "current_step:  279\n",
      "2017-08-22T05:29:01.449254: step 280, loss 0.466346, acc 0.86\n",
      "current_step:  280\n",
      "2017-08-22T05:29:01.618911: step 281, loss 0.64738, acc 0.82\n",
      "current_step:  281\n",
      "2017-08-22T05:29:01.790909: step 282, loss 0.695595, acc 0.78\n",
      "current_step:  282\n",
      "2017-08-22T05:29:01.959950: step 283, loss 0.693188, acc 0.8\n",
      "current_step:  283\n",
      "2017-08-22T05:29:02.129866: step 284, loss 0.545283, acc 0.82\n",
      "current_step:  284\n",
      "2017-08-22T05:29:02.298825: step 285, loss 0.545072, acc 0.82\n",
      "current_step:  285\n",
      "2017-08-22T05:29:02.470308: step 286, loss 0.555056, acc 0.8\n",
      "current_step:  286\n",
      "2017-08-22T05:29:02.642045: step 287, loss 0.553332, acc 0.78\n",
      "current_step:  287\n",
      "2017-08-22T05:29:02.812717: step 288, loss 0.808967, acc 0.72\n",
      "current_step:  288\n",
      "2017-08-22T05:29:02.981093: step 289, loss 0.56013, acc 0.82\n",
      "current_step:  289\n",
      "2017-08-22T05:29:03.151443: step 290, loss 0.623976, acc 0.84\n",
      "current_step:  290\n",
      "2017-08-22T05:29:03.321369: step 291, loss 0.481246, acc 0.76\n",
      "current_step:  291\n",
      "2017-08-22T05:29:03.490285: step 292, loss 0.550565, acc 0.86\n",
      "current_step:  292\n",
      "2017-08-22T05:29:03.661286: step 293, loss 0.683384, acc 0.78\n",
      "current_step:  293\n",
      "2017-08-22T05:29:03.831757: step 294, loss 0.692385, acc 0.78\n",
      "current_step:  294\n",
      "2017-08-22T05:29:04.001071: step 295, loss 0.612502, acc 0.84\n",
      "current_step:  295\n",
      "2017-08-22T05:29:04.170249: step 296, loss 0.713578, acc 0.72\n",
      "current_step:  296\n",
      "2017-08-22T05:29:04.339051: step 297, loss 0.731877, acc 0.74\n",
      "current_step:  297\n",
      "2017-08-22T05:29:04.508822: step 298, loss 0.633559, acc 0.8\n",
      "current_step:  298\n",
      "2017-08-22T05:29:04.678692: step 299, loss 0.738195, acc 0.8\n",
      "current_step:  299\n",
      "2017-08-22T05:29:04.848742: step 300, loss 0.656691, acc 0.86\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:29:05.457483: step 300, loss 0.948473, acc 0.645907\n",
      "\n",
      "2017-08-22T05:29:05.627915: step 301, loss 0.771218, acc 0.76\n",
      "current_step:  301\n",
      "2017-08-22T05:29:05.798284: step 302, loss 0.498474, acc 0.86\n",
      "current_step:  302\n",
      "2017-08-22T05:29:05.968578: step 303, loss 0.603419, acc 0.8\n",
      "current_step:  303\n",
      "2017-08-22T05:29:06.138206: step 304, loss 0.621091, acc 0.78\n",
      "current_step:  304\n",
      "2017-08-22T05:29:06.308144: step 305, loss 0.659044, acc 0.72\n",
      "current_step:  305\n",
      "2017-08-22T05:29:06.344267: step 306, loss 0.733139, acc 0.777778\n",
      "current_step:  306\n",
      "2017-08-22T05:29:06.514384: step 307, loss 0.380192, acc 0.94\n",
      "current_step:  307\n",
      "2017-08-22T05:29:06.684132: step 308, loss 0.395092, acc 0.88\n",
      "current_step:  308\n",
      "2017-08-22T05:29:06.854367: step 309, loss 0.591897, acc 0.82\n",
      "current_step:  309\n",
      "2017-08-22T05:29:07.023811: step 310, loss 0.349642, acc 0.96\n",
      "current_step:  310\n",
      "2017-08-22T05:29:07.193104: step 311, loss 0.585089, acc 0.78\n",
      "current_step:  311\n",
      "2017-08-22T05:29:07.362938: step 312, loss 0.328763, acc 0.94\n",
      "current_step:  312\n",
      "2017-08-22T05:29:07.532474: step 313, loss 0.423422, acc 0.92\n",
      "current_step:  313\n",
      "2017-08-22T05:29:07.704931: step 314, loss 0.285982, acc 0.94\n",
      "current_step:  314\n",
      "2017-08-22T05:29:07.874859: step 315, loss 0.491345, acc 0.88\n",
      "current_step:  315\n",
      "2017-08-22T05:29:08.043989: step 316, loss 0.470766, acc 0.84\n",
      "current_step:  316\n",
      "2017-08-22T05:29:08.214355: step 317, loss 0.394767, acc 0.92\n",
      "current_step:  317\n",
      "2017-08-22T05:29:08.383884: step 318, loss 0.485531, acc 0.9\n",
      "current_step:  318\n",
      "2017-08-22T05:29:08.554083: step 319, loss 0.383545, acc 0.9\n",
      "current_step:  319\n",
      "2017-08-22T05:29:08.725250: step 320, loss 0.470329, acc 0.9\n",
      "current_step:  320\n",
      "2017-08-22T05:29:08.895024: step 321, loss 0.500331, acc 0.86\n",
      "current_step:  321\n",
      "2017-08-22T05:29:09.062939: step 322, loss 0.354457, acc 0.92\n",
      "current_step:  322\n",
      "2017-08-22T05:29:09.232186: step 323, loss 0.424388, acc 0.88\n",
      "current_step:  323\n",
      "2017-08-22T05:29:09.401921: step 324, loss 0.569949, acc 0.84\n",
      "current_step:  324\n",
      "2017-08-22T05:29:09.570722: step 325, loss 0.483305, acc 0.82\n",
      "current_step:  325\n",
      "2017-08-22T05:29:09.740654: step 326, loss 0.402634, acc 0.9\n",
      "current_step:  326\n",
      "2017-08-22T05:29:09.910006: step 327, loss 0.387456, acc 0.9\n",
      "current_step:  327\n",
      "2017-08-22T05:29:10.078265: step 328, loss 0.42483, acc 0.88\n",
      "current_step:  328\n",
      "2017-08-22T05:29:10.246727: step 329, loss 0.53629, acc 0.88\n",
      "current_step:  329\n",
      "2017-08-22T05:29:10.416071: step 330, loss 0.336024, acc 0.92\n",
      "current_step:  330\n",
      "2017-08-22T05:29:10.586093: step 331, loss 0.423894, acc 0.88\n",
      "current_step:  331\n",
      "2017-08-22T05:29:10.755187: step 332, loss 0.46467, acc 0.86\n",
      "current_step:  332\n",
      "2017-08-22T05:29:10.923424: step 333, loss 0.40599, acc 0.86\n",
      "current_step:  333\n",
      "2017-08-22T05:29:11.092176: step 334, loss 0.406576, acc 0.9\n",
      "current_step:  334\n",
      "2017-08-22T05:29:11.261795: step 335, loss 0.540608, acc 0.82\n",
      "current_step:  335\n",
      "2017-08-22T05:29:11.429950: step 336, loss 0.428192, acc 0.86\n",
      "current_step:  336\n",
      "2017-08-22T05:29:11.599015: step 337, loss 0.652395, acc 0.8\n",
      "current_step:  337\n",
      "2017-08-22T05:29:11.769611: step 338, loss 0.392127, acc 0.86\n",
      "current_step:  338\n",
      "2017-08-22T05:29:11.938162: step 339, loss 0.489366, acc 0.84\n",
      "current_step:  339\n",
      "2017-08-22T05:29:12.107887: step 340, loss 0.571102, acc 0.84\n",
      "current_step:  340\n",
      "2017-08-22T05:29:12.277407: step 341, loss 0.590184, acc 0.8\n",
      "current_step:  341\n",
      "2017-08-22T05:29:12.445951: step 342, loss 0.370695, acc 0.92\n",
      "current_step:  342\n",
      "2017-08-22T05:29:12.614620: step 343, loss 0.45161, acc 0.9\n",
      "current_step:  343\n",
      "2017-08-22T05:29:12.783702: step 344, loss 0.458002, acc 0.88\n",
      "current_step:  344\n",
      "2017-08-22T05:29:12.952507: step 345, loss 0.437558, acc 0.88\n",
      "current_step:  345\n",
      "2017-08-22T05:29:13.121371: step 346, loss 0.380773, acc 0.88\n",
      "current_step:  346\n",
      "2017-08-22T05:29:13.289196: step 347, loss 0.44909, acc 0.86\n",
      "current_step:  347\n",
      "2017-08-22T05:29:13.458385: step 348, loss 0.544195, acc 0.8\n",
      "current_step:  348\n",
      "2017-08-22T05:29:13.627850: step 349, loss 0.487768, acc 0.8\n",
      "current_step:  349\n",
      "2017-08-22T05:29:13.798190: step 350, loss 0.444769, acc 0.88\n",
      "current_step:  350\n",
      "2017-08-22T05:29:13.966832: step 351, loss 0.496105, acc 0.82\n",
      "current_step:  351\n",
      "2017-08-22T05:29:14.135623: step 352, loss 0.455031, acc 0.84\n",
      "current_step:  352\n",
      "2017-08-22T05:29:14.305227: step 353, loss 0.480569, acc 0.82\n",
      "current_step:  353\n",
      "2017-08-22T05:29:14.477634: step 354, loss 0.423656, acc 0.92\n",
      "current_step:  354\n",
      "2017-08-22T05:29:14.647123: step 355, loss 0.491646, acc 0.88\n",
      "current_step:  355\n",
      "2017-08-22T05:29:14.816222: step 356, loss 0.460146, acc 0.82\n",
      "current_step:  356\n",
      "2017-08-22T05:29:14.986042: step 357, loss 0.368629, acc 0.9\n",
      "current_step:  357\n",
      "2017-08-22T05:29:15.154790: step 358, loss 0.531262, acc 0.82\n",
      "current_step:  358\n",
      "2017-08-22T05:29:15.324690: step 359, loss 0.44631, acc 0.9\n",
      "current_step:  359\n",
      "2017-08-22T05:29:15.492503: step 360, loss 0.541654, acc 0.88\n",
      "current_step:  360\n",
      "2017-08-22T05:29:15.664757: step 361, loss 0.5053, acc 0.88\n",
      "current_step:  361\n",
      "2017-08-22T05:29:15.836048: step 362, loss 0.423502, acc 0.88\n",
      "current_step:  362\n",
      "2017-08-22T05:29:16.004801: step 363, loss 0.422469, acc 0.92\n",
      "current_step:  363\n",
      "2017-08-22T05:29:16.174890: step 364, loss 0.589359, acc 0.82\n",
      "current_step:  364\n",
      "2017-08-22T05:29:16.343660: step 365, loss 0.397948, acc 0.88\n",
      "current_step:  365\n",
      "2017-08-22T05:29:16.512721: step 366, loss 0.571987, acc 0.76\n",
      "current_step:  366\n",
      "2017-08-22T05:29:16.683034: step 367, loss 0.522826, acc 0.86\n",
      "current_step:  367\n",
      "2017-08-22T05:29:16.853224: step 368, loss 0.39641, acc 0.9\n",
      "current_step:  368\n",
      "2017-08-22T05:29:17.022860: step 369, loss 0.553897, acc 0.8\n",
      "current_step:  369\n",
      "2017-08-22T05:29:17.193370: step 370, loss 0.324583, acc 0.9\n",
      "current_step:  370\n",
      "2017-08-22T05:29:17.361971: step 371, loss 0.470071, acc 0.8\n",
      "current_step:  371\n",
      "2017-08-22T05:29:17.530162: step 372, loss 0.698305, acc 0.8\n",
      "current_step:  372\n",
      "2017-08-22T05:29:17.699541: step 373, loss 0.38975, acc 0.94\n",
      "current_step:  373\n",
      "2017-08-22T05:29:17.869450: step 374, loss 0.411388, acc 0.9\n",
      "current_step:  374\n",
      "2017-08-22T05:29:18.040142: step 375, loss 0.58992, acc 0.8\n",
      "current_step:  375\n",
      "2017-08-22T05:29:18.209310: step 376, loss 0.419224, acc 0.94\n",
      "current_step:  376\n",
      "2017-08-22T05:29:18.378424: step 377, loss 0.496749, acc 0.92\n",
      "current_step:  377\n",
      "2017-08-22T05:29:18.548052: step 378, loss 0.497289, acc 0.86\n",
      "current_step:  378\n",
      "2017-08-22T05:29:18.717279: step 379, loss 0.498533, acc 0.88\n",
      "current_step:  379\n",
      "2017-08-22T05:29:18.885740: step 380, loss 0.595542, acc 0.74\n",
      "current_step:  380\n",
      "2017-08-22T05:29:19.052850: step 381, loss 0.480427, acc 0.88\n",
      "current_step:  381\n",
      "2017-08-22T05:29:19.221235: step 382, loss 0.724205, acc 0.78\n",
      "current_step:  382\n",
      "2017-08-22T05:29:19.389740: step 383, loss 0.411223, acc 0.86\n",
      "current_step:  383\n",
      "2017-08-22T05:29:19.560533: step 384, loss 0.306362, acc 0.98\n",
      "current_step:  384\n",
      "2017-08-22T05:29:19.730893: step 385, loss 0.476505, acc 0.88\n",
      "current_step:  385\n",
      "2017-08-22T05:29:19.899273: step 386, loss 0.406263, acc 0.86\n",
      "current_step:  386\n",
      "2017-08-22T05:29:20.068008: step 387, loss 0.524178, acc 0.84\n",
      "current_step:  387\n",
      "2017-08-22T05:29:20.238005: step 388, loss 0.482377, acc 0.86\n",
      "current_step:  388\n",
      "2017-08-22T05:29:20.406937: step 389, loss 0.571565, acc 0.8\n",
      "current_step:  389\n",
      "2017-08-22T05:29:20.576483: step 390, loss 0.49212, acc 0.84\n",
      "current_step:  390\n",
      "2017-08-22T05:29:20.748051: step 391, loss 0.566483, acc 0.8\n",
      "current_step:  391\n",
      "2017-08-22T05:29:20.917088: step 392, loss 0.516819, acc 0.86\n",
      "current_step:  392\n",
      "2017-08-22T05:29:21.085545: step 393, loss 0.481051, acc 0.9\n",
      "current_step:  393\n",
      "2017-08-22T05:29:21.253606: step 394, loss 0.421115, acc 0.92\n",
      "current_step:  394\n",
      "2017-08-22T05:29:21.421819: step 395, loss 0.369959, acc 0.92\n",
      "current_step:  395\n",
      "2017-08-22T05:29:21.592466: step 396, loss 0.520726, acc 0.84\n",
      "current_step:  396\n",
      "2017-08-22T05:29:21.762154: step 397, loss 0.296703, acc 0.9\n",
      "current_step:  397\n",
      "2017-08-22T05:29:21.931643: step 398, loss 0.389145, acc 0.88\n",
      "current_step:  398\n",
      "2017-08-22T05:29:22.100800: step 399, loss 0.334457, acc 0.92\n",
      "current_step:  399\n",
      "2017-08-22T05:29:22.271677: step 400, loss 0.37276, acc 0.88\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:29:22.883116: step 400, loss 0.902743, acc 0.692171\n",
      "\n",
      "2017-08-22T05:29:23.051970: step 401, loss 0.42576, acc 0.9\n",
      "current_step:  401\n",
      "2017-08-22T05:29:23.222543: step 402, loss 0.399506, acc 0.9\n",
      "current_step:  402\n",
      "2017-08-22T05:29:23.390167: step 403, loss 0.463583, acc 0.86\n",
      "current_step:  403\n",
      "2017-08-22T05:29:23.560016: step 404, loss 0.567073, acc 0.82\n",
      "current_step:  404\n",
      "2017-08-22T05:29:23.732877: step 405, loss 0.415911, acc 0.88\n",
      "current_step:  405\n",
      "2017-08-22T05:29:23.902061: step 406, loss 0.507671, acc 0.88\n",
      "current_step:  406\n",
      "2017-08-22T05:29:24.071091: step 407, loss 0.444885, acc 0.82\n",
      "current_step:  407\n",
      "2017-08-22T05:29:24.107156: step 408, loss 0.483419, acc 0.888889\n",
      "current_step:  408\n",
      "2017-08-22T05:29:24.275575: step 409, loss 0.326215, acc 0.96\n",
      "current_step:  409\n",
      "2017-08-22T05:29:24.443461: step 410, loss 0.283091, acc 0.96\n",
      "current_step:  410\n",
      "2017-08-22T05:29:24.612159: step 411, loss 0.320445, acc 0.92\n",
      "current_step:  411\n",
      "2017-08-22T05:29:24.782043: step 412, loss 0.443719, acc 0.88\n",
      "current_step:  412\n",
      "2017-08-22T05:29:24.951902: step 413, loss 0.361952, acc 0.96\n",
      "current_step:  413\n",
      "2017-08-22T05:29:25.122024: step 414, loss 0.244555, acc 1\n",
      "current_step:  414\n",
      "2017-08-22T05:29:25.293159: step 415, loss 0.37507, acc 0.92\n",
      "current_step:  415\n",
      "2017-08-22T05:29:25.461095: step 416, loss 0.387948, acc 0.92\n",
      "current_step:  416\n",
      "2017-08-22T05:29:25.630457: step 417, loss 0.283085, acc 0.98\n",
      "current_step:  417\n",
      "2017-08-22T05:29:25.800994: step 418, loss 0.41441, acc 0.88\n",
      "current_step:  418\n",
      "2017-08-22T05:29:25.972141: step 419, loss 0.361443, acc 0.92\n",
      "current_step:  419\n",
      "2017-08-22T05:29:26.142398: step 420, loss 0.336549, acc 0.9\n",
      "current_step:  420\n",
      "2017-08-22T05:29:26.310400: step 421, loss 0.290868, acc 0.94\n",
      "current_step:  421\n",
      "2017-08-22T05:29:26.479141: step 422, loss 0.274461, acc 0.92\n",
      "current_step:  422\n",
      "2017-08-22T05:29:26.647785: step 423, loss 0.2881, acc 0.92\n",
      "current_step:  423\n",
      "2017-08-22T05:29:26.817310: step 424, loss 0.29689, acc 0.94\n",
      "current_step:  424\n",
      "2017-08-22T05:29:26.987305: step 425, loss 0.266671, acc 0.92\n",
      "current_step:  425\n",
      "2017-08-22T05:29:27.156222: step 426, loss 0.380449, acc 0.92\n",
      "current_step:  426\n",
      "2017-08-22T05:29:27.324762: step 427, loss 0.459693, acc 0.86\n",
      "current_step:  427\n",
      "2017-08-22T05:29:27.494357: step 428, loss 0.306705, acc 0.96\n",
      "current_step:  428\n",
      "2017-08-22T05:29:27.664555: step 429, loss 0.325491, acc 0.94\n",
      "current_step:  429\n",
      "2017-08-22T05:29:27.834753: step 430, loss 0.265608, acc 0.92\n",
      "current_step:  430\n",
      "2017-08-22T05:29:28.004252: step 431, loss 0.353577, acc 0.9\n",
      "current_step:  431\n",
      "2017-08-22T05:29:28.174365: step 432, loss 0.421644, acc 0.92\n",
      "current_step:  432\n",
      "2017-08-22T05:29:28.344465: step 433, loss 0.327213, acc 0.92\n",
      "current_step:  433\n",
      "2017-08-22T05:29:28.514800: step 434, loss 0.308079, acc 0.88\n",
      "current_step:  434\n",
      "2017-08-22T05:29:28.685008: step 435, loss 0.327481, acc 0.92\n",
      "current_step:  435\n",
      "2017-08-22T05:29:28.856020: step 436, loss 0.287067, acc 0.94\n",
      "current_step:  436\n",
      "2017-08-22T05:29:29.027328: step 437, loss 0.331946, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-22T05:29:29.196644: step 438, loss 0.358818, acc 0.9\n",
      "current_step:  438\n",
      "2017-08-22T05:29:29.366267: step 439, loss 0.331493, acc 0.92\n",
      "current_step:  439\n",
      "2017-08-22T05:29:29.536007: step 440, loss 0.309327, acc 0.92\n",
      "current_step:  440\n",
      "2017-08-22T05:29:29.705972: step 441, loss 0.286828, acc 0.96\n",
      "current_step:  441\n",
      "2017-08-22T05:29:29.875458: step 442, loss 0.307608, acc 0.94\n",
      "current_step:  442\n",
      "2017-08-22T05:29:30.046188: step 443, loss 0.298773, acc 0.96\n",
      "current_step:  443\n",
      "2017-08-22T05:29:30.215452: step 444, loss 0.329136, acc 0.92\n",
      "current_step:  444\n",
      "2017-08-22T05:29:30.384176: step 445, loss 0.388884, acc 0.9\n",
      "current_step:  445\n",
      "2017-08-22T05:29:30.554813: step 446, loss 0.326645, acc 0.96\n",
      "current_step:  446\n",
      "2017-08-22T05:29:30.723544: step 447, loss 0.387061, acc 0.92\n",
      "current_step:  447\n",
      "2017-08-22T05:29:30.891809: step 448, loss 0.488716, acc 0.82\n",
      "current_step:  448\n",
      "2017-08-22T05:29:31.061488: step 449, loss 0.240586, acc 0.92\n",
      "current_step:  449\n",
      "2017-08-22T05:29:31.230276: step 450, loss 0.354798, acc 0.94\n",
      "current_step:  450\n",
      "2017-08-22T05:29:31.398157: step 451, loss 0.524516, acc 0.82\n",
      "current_step:  451\n",
      "2017-08-22T05:29:31.568983: step 452, loss 0.403404, acc 0.92\n",
      "current_step:  452\n",
      "2017-08-22T05:29:31.738815: step 453, loss 0.348467, acc 0.9\n",
      "current_step:  453\n",
      "2017-08-22T05:29:31.908337: step 454, loss 0.367241, acc 0.94\n",
      "current_step:  454\n",
      "2017-08-22T05:29:32.076971: step 455, loss 0.328459, acc 0.96\n",
      "current_step:  455\n",
      "2017-08-22T05:29:32.246425: step 456, loss 0.427693, acc 0.88\n",
      "current_step:  456\n",
      "2017-08-22T05:29:32.415390: step 457, loss 0.386648, acc 0.9\n",
      "current_step:  457\n",
      "2017-08-22T05:29:32.585414: step 458, loss 0.376629, acc 0.86\n",
      "current_step:  458\n",
      "2017-08-22T05:29:32.754920: step 459, loss 0.431752, acc 0.84\n",
      "current_step:  459\n",
      "2017-08-22T05:29:32.924687: step 460, loss 0.414774, acc 0.88\n",
      "current_step:  460\n",
      "2017-08-22T05:29:33.095686: step 461, loss 0.317149, acc 0.96\n",
      "current_step:  461\n",
      "2017-08-22T05:29:33.265445: step 462, loss 0.233622, acc 0.96\n",
      "current_step:  462\n",
      "2017-08-22T05:29:33.435114: step 463, loss 0.432334, acc 0.88\n",
      "current_step:  463\n",
      "2017-08-22T05:29:33.606746: step 464, loss 0.487377, acc 0.86\n",
      "current_step:  464\n",
      "2017-08-22T05:29:33.776550: step 465, loss 0.337818, acc 0.9\n",
      "current_step:  465\n",
      "2017-08-22T05:29:33.944948: step 466, loss 0.371387, acc 0.88\n",
      "current_step:  466\n",
      "2017-08-22T05:29:34.115688: step 467, loss 0.311973, acc 0.94\n",
      "current_step:  467\n",
      "2017-08-22T05:29:34.282546: step 468, loss 0.374479, acc 0.96\n",
      "current_step:  468\n",
      "2017-08-22T05:29:34.451946: step 469, loss 0.420359, acc 0.88\n",
      "current_step:  469\n",
      "2017-08-22T05:29:34.622013: step 470, loss 0.278686, acc 0.96\n",
      "current_step:  470\n",
      "2017-08-22T05:29:34.791905: step 471, loss 0.356614, acc 0.94\n",
      "current_step:  471\n",
      "2017-08-22T05:29:34.962703: step 472, loss 0.361197, acc 0.92\n",
      "current_step:  472\n",
      "2017-08-22T05:29:35.132234: step 473, loss 0.32541, acc 0.92\n",
      "current_step:  473\n",
      "2017-08-22T05:29:35.300732: step 474, loss 0.351087, acc 0.88\n",
      "current_step:  474\n",
      "2017-08-22T05:29:35.470561: step 475, loss 0.360001, acc 0.9\n",
      "current_step:  475\n",
      "2017-08-22T05:29:35.642038: step 476, loss 0.275128, acc 0.92\n",
      "current_step:  476\n",
      "2017-08-22T05:29:35.813016: step 477, loss 0.334814, acc 0.92\n",
      "current_step:  477\n",
      "2017-08-22T05:29:35.982972: step 478, loss 0.354723, acc 0.9\n",
      "current_step:  478\n",
      "2017-08-22T05:29:36.152403: step 479, loss 0.291517, acc 0.94\n",
      "current_step:  479\n",
      "2017-08-22T05:29:36.321536: step 480, loss 0.340434, acc 0.92\n",
      "current_step:  480\n",
      "2017-08-22T05:29:36.491374: step 481, loss 0.199907, acc 0.94\n",
      "current_step:  481\n",
      "2017-08-22T05:29:36.662259: step 482, loss 0.690955, acc 0.8\n",
      "current_step:  482\n",
      "2017-08-22T05:29:36.832298: step 483, loss 0.434507, acc 0.82\n",
      "current_step:  483\n",
      "2017-08-22T05:29:37.002599: step 484, loss 0.299824, acc 0.9\n",
      "current_step:  484\n",
      "2017-08-22T05:29:37.171822: step 485, loss 0.366217, acc 0.9\n",
      "current_step:  485\n",
      "2017-08-22T05:29:37.341771: step 486, loss 0.256582, acc 0.96\n",
      "current_step:  486\n",
      "2017-08-22T05:29:37.511669: step 487, loss 0.261124, acc 0.98\n",
      "current_step:  487\n",
      "2017-08-22T05:29:37.680540: step 488, loss 0.304197, acc 0.94\n",
      "current_step:  488\n",
      "2017-08-22T05:29:37.850526: step 489, loss 0.302246, acc 0.98\n",
      "current_step:  489\n",
      "2017-08-22T05:29:38.020435: step 490, loss 0.358391, acc 0.92\n",
      "current_step:  490\n",
      "2017-08-22T05:29:38.190421: step 491, loss 0.227258, acc 0.96\n",
      "current_step:  491\n",
      "2017-08-22T05:29:38.360300: step 492, loss 0.506808, acc 0.88\n",
      "current_step:  492\n",
      "2017-08-22T05:29:38.528248: step 493, loss 0.376081, acc 0.94\n",
      "current_step:  493\n",
      "2017-08-22T05:29:38.699438: step 494, loss 0.254007, acc 1\n",
      "current_step:  494\n",
      "2017-08-22T05:29:38.868403: step 495, loss 0.262378, acc 0.94\n",
      "current_step:  495\n",
      "2017-08-22T05:29:39.037510: step 496, loss 0.270364, acc 0.96\n",
      "current_step:  496\n",
      "2017-08-22T05:29:39.206854: step 497, loss 0.27248, acc 0.98\n",
      "current_step:  497\n",
      "2017-08-22T05:29:39.376310: step 498, loss 0.300518, acc 0.92\n",
      "current_step:  498\n",
      "2017-08-22T05:29:39.546910: step 499, loss 0.266163, acc 0.94\n",
      "current_step:  499\n",
      "2017-08-22T05:29:39.717895: step 500, loss 0.426426, acc 0.86\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:29:40.324009: step 500, loss 0.879771, acc 0.69395\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0528_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:29:40.567473: step 501, loss 0.330314, acc 0.96\n",
      "current_step:  501\n",
      "2017-08-22T05:29:40.737522: step 502, loss 0.264577, acc 0.96\n",
      "current_step:  502\n",
      "2017-08-22T05:29:40.907558: step 503, loss 0.36737, acc 0.92\n",
      "current_step:  503\n",
      "2017-08-22T05:29:41.075621: step 504, loss 0.374125, acc 0.88\n",
      "current_step:  504\n",
      "2017-08-22T05:29:41.245099: step 505, loss 0.40399, acc 0.94\n",
      "current_step:  505\n",
      "2017-08-22T05:29:41.415451: step 506, loss 0.29019, acc 0.96\n",
      "current_step:  506\n",
      "2017-08-22T05:29:41.585760: step 507, loss 0.505591, acc 0.88\n",
      "current_step:  507\n",
      "2017-08-22T05:29:41.755536: step 508, loss 0.300084, acc 0.94\n",
      "current_step:  508\n",
      "2017-08-22T05:29:41.925575: step 509, loss 0.297801, acc 0.96\n",
      "current_step:  509\n",
      "2017-08-22T05:29:41.961949: step 510, loss 0.473593, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.556769\n",
      "Precision Score = 0.512757\n",
      "Recall Score = 0.613879\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.597212\n",
      "Precision Score = 0.570479\n",
      "Recall Score = 0.644128\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.602048\n",
      "Precision Score = 0.622874\n",
      "Recall Score = 0.637011\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.646255\n",
      "Precision Score = 0.651567\n",
      "Recall Score = 0.683274\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.654053\n",
      "Precision Score = 0.648825\n",
      "Recall Score = 0.686833\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v2 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 2, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "max_chunk_length =  149\n",
      "Vocabulary Size: 13762\n",
      "Train/Dev split on data (x): 5059/562\n",
      "Train/Dev split on labels (y): 5059/562\n",
      "Writing to /home/vslchu/w266/project/code/runs/20170822_0529_UTC\n",
      "\n",
      "grads_and_vars.shape =  (9, 2)\n",
      "cnn.out_dir =  /home/vslchu/w266/project/code/runs/20170822_0529_UTC\n",
      "2017-08-22T05:29:59.526612: step 1, loss 3.20563, acc 0.02\n",
      "current_step:  1\n",
      "2017-08-22T05:29:59.603924: step 2, loss 1.96764, acc 0.34\n",
      "current_step:  2\n",
      "2017-08-22T05:29:59.678169: step 3, loss 1.86751, acc 0.4\n",
      "current_step:  3\n",
      "2017-08-22T05:29:59.754751: step 4, loss 1.60973, acc 0.4\n",
      "current_step:  4\n",
      "2017-08-22T05:29:59.828186: step 5, loss 1.26315, acc 0.56\n",
      "current_step:  5\n",
      "2017-08-22T05:29:59.901422: step 6, loss 1.55993, acc 0.4\n",
      "current_step:  6\n",
      "2017-08-22T05:29:59.977269: step 7, loss 1.43233, acc 0.46\n",
      "current_step:  7\n",
      "2017-08-22T05:30:00.052249: step 8, loss 1.80465, acc 0.42\n",
      "current_step:  8\n",
      "2017-08-22T05:30:00.128540: step 9, loss 1.65463, acc 0.52\n",
      "current_step:  9\n",
      "2017-08-22T05:30:00.205337: step 10, loss 1.46476, acc 0.36\n",
      "current_step:  10\n",
      "2017-08-22T05:30:00.280523: step 11, loss 1.62661, acc 0.4\n",
      "current_step:  11\n",
      "2017-08-22T05:30:00.356180: step 12, loss 1.52289, acc 0.4\n",
      "current_step:  12\n",
      "2017-08-22T05:30:00.431282: step 13, loss 1.34386, acc 0.42\n",
      "current_step:  13\n",
      "2017-08-22T05:30:00.507266: step 14, loss 1.22999, acc 0.38\n",
      "current_step:  14\n",
      "2017-08-22T05:30:00.582525: step 15, loss 1.12389, acc 0.54\n",
      "current_step:  15\n",
      "2017-08-22T05:30:00.656930: step 16, loss 1.32441, acc 0.54\n",
      "current_step:  16\n",
      "2017-08-22T05:30:00.732922: step 17, loss 1.54955, acc 0.46\n",
      "current_step:  17\n",
      "2017-08-22T05:30:00.807166: step 18, loss 1.17815, acc 0.66\n",
      "current_step:  18\n",
      "2017-08-22T05:30:00.882121: step 19, loss 1.6841, acc 0.44\n",
      "current_step:  19\n",
      "2017-08-22T05:30:00.957354: step 20, loss 1.82824, acc 0.44\n",
      "current_step:  20\n",
      "2017-08-22T05:30:01.033034: step 21, loss 1.6033, acc 0.44\n",
      "current_step:  21\n",
      "2017-08-22T05:30:01.108569: step 22, loss 1.31034, acc 0.58\n",
      "current_step:  22\n",
      "2017-08-22T05:30:01.183143: step 23, loss 1.07549, acc 0.58\n",
      "current_step:  23\n",
      "2017-08-22T05:30:01.257455: step 24, loss 0.947563, acc 0.6\n",
      "current_step:  24\n",
      "2017-08-22T05:30:01.332801: step 25, loss 1.35222, acc 0.6\n",
      "current_step:  25\n",
      "2017-08-22T05:30:01.406971: step 26, loss 1.34954, acc 0.58\n",
      "current_step:  26\n",
      "2017-08-22T05:30:01.480985: step 27, loss 1.15836, acc 0.58\n",
      "current_step:  27\n",
      "2017-08-22T05:30:01.557158: step 28, loss 1.39638, acc 0.52\n",
      "current_step:  28\n",
      "2017-08-22T05:30:01.632606: step 29, loss 1.17232, acc 0.54\n",
      "current_step:  29\n",
      "2017-08-22T05:30:01.730863: step 30, loss 1.10261, acc 0.6\n",
      "current_step:  30\n",
      "2017-08-22T05:30:01.856838: step 31, loss 1.17911, acc 0.54\n",
      "current_step:  31\n",
      "2017-08-22T05:30:01.983249: step 32, loss 1.20522, acc 0.58\n",
      "current_step:  32\n",
      "2017-08-22T05:30:02.110131: step 33, loss 1.2514, acc 0.5\n",
      "current_step:  33\n",
      "2017-08-22T05:30:02.235229: step 34, loss 1.20752, acc 0.54\n",
      "current_step:  34\n",
      "2017-08-22T05:30:02.362484: step 35, loss 1.02913, acc 0.68\n",
      "current_step:  35\n",
      "2017-08-22T05:30:02.488606: step 36, loss 1.30192, acc 0.46\n",
      "current_step:  36\n",
      "2017-08-22T05:30:02.614852: step 37, loss 1.39476, acc 0.44\n",
      "current_step:  37\n",
      "2017-08-22T05:30:02.742444: step 38, loss 1.00994, acc 0.7\n",
      "current_step:  38\n",
      "2017-08-22T05:30:02.868310: step 39, loss 1.46862, acc 0.44\n",
      "current_step:  39\n",
      "2017-08-22T05:30:02.994669: step 40, loss 0.987563, acc 0.7\n",
      "current_step:  40\n",
      "2017-08-22T05:30:03.121451: step 41, loss 1.33811, acc 0.52\n",
      "current_step:  41\n",
      "2017-08-22T05:30:03.247699: step 42, loss 1.42101, acc 0.56\n",
      "current_step:  42\n",
      "2017-08-22T05:30:03.373959: step 43, loss 1.34273, acc 0.54\n",
      "current_step:  43\n",
      "2017-08-22T05:30:03.500003: step 44, loss 1.27617, acc 0.54\n",
      "current_step:  44\n",
      "2017-08-22T05:30:03.627256: step 45, loss 1.25553, acc 0.5\n",
      "current_step:  45\n",
      "2017-08-22T05:30:03.754493: step 46, loss 1.20595, acc 0.56\n",
      "current_step:  46\n",
      "2017-08-22T05:30:03.879324: step 47, loss 1.30392, acc 0.5\n",
      "current_step:  47\n",
      "2017-08-22T05:30:04.005041: step 48, loss 1.21142, acc 0.62\n",
      "current_step:  48\n",
      "2017-08-22T05:30:04.129969: step 49, loss 1.27004, acc 0.56\n",
      "current_step:  49\n",
      "2017-08-22T05:30:04.254198: step 50, loss 1.29093, acc 0.46\n",
      "current_step:  50\n",
      "2017-08-22T05:30:04.380655: step 51, loss 0.991519, acc 0.58\n",
      "current_step:  51\n",
      "2017-08-22T05:30:04.506446: step 52, loss 1.09581, acc 0.62\n",
      "current_step:  52\n",
      "2017-08-22T05:30:04.633395: step 53, loss 1.37877, acc 0.5\n",
      "current_step:  53\n",
      "2017-08-22T05:30:04.759994: step 54, loss 1.22511, acc 0.56\n",
      "current_step:  54\n",
      "2017-08-22T05:30:04.885645: step 55, loss 1.13493, acc 0.56\n",
      "current_step:  55\n",
      "2017-08-22T05:30:05.010302: step 56, loss 1.04859, acc 0.64\n",
      "current_step:  56\n",
      "2017-08-22T05:30:05.135674: step 57, loss 1.05137, acc 0.6\n",
      "current_step:  57\n",
      "2017-08-22T05:30:05.261020: step 58, loss 1.41809, acc 0.48\n",
      "current_step:  58\n",
      "2017-08-22T05:30:05.386950: step 59, loss 1.12611, acc 0.54\n",
      "current_step:  59\n",
      "2017-08-22T05:30:05.512377: step 60, loss 1.25062, acc 0.52\n",
      "current_step:  60\n",
      "2017-08-22T05:30:05.640641: step 61, loss 0.895153, acc 0.68\n",
      "current_step:  61\n",
      "2017-08-22T05:30:05.768146: step 62, loss 1.15185, acc 0.6\n",
      "current_step:  62\n",
      "2017-08-22T05:30:05.895204: step 63, loss 1.25777, acc 0.5\n",
      "current_step:  63\n",
      "2017-08-22T05:30:06.020082: step 64, loss 1.12721, acc 0.66\n",
      "current_step:  64\n",
      "2017-08-22T05:30:06.145259: step 65, loss 0.817392, acc 0.72\n",
      "current_step:  65\n",
      "2017-08-22T05:30:06.270977: step 66, loss 1.12531, acc 0.56\n",
      "current_step:  66\n",
      "2017-08-22T05:30:06.397071: step 67, loss 1.00208, acc 0.54\n",
      "current_step:  67\n",
      "2017-08-22T05:30:06.523619: step 68, loss 1.02048, acc 0.58\n",
      "current_step:  68\n",
      "2017-08-22T05:30:06.651094: step 69, loss 1.17356, acc 0.62\n",
      "current_step:  69\n",
      "2017-08-22T05:30:06.778034: step 70, loss 1.01906, acc 0.56\n",
      "current_step:  70\n",
      "2017-08-22T05:30:06.906228: step 71, loss 1.00675, acc 0.64\n",
      "current_step:  71\n",
      "2017-08-22T05:30:07.031433: step 72, loss 1.34888, acc 0.5\n",
      "current_step:  72\n",
      "2017-08-22T05:30:07.156944: step 73, loss 0.903563, acc 0.7\n",
      "current_step:  73\n",
      "2017-08-22T05:30:07.282429: step 74, loss 1.08908, acc 0.64\n",
      "current_step:  74\n",
      "2017-08-22T05:30:07.409470: step 75, loss 1.21986, acc 0.54\n",
      "current_step:  75\n",
      "2017-08-22T05:30:07.535243: step 76, loss 1.13641, acc 0.54\n",
      "current_step:  76\n",
      "2017-08-22T05:30:07.661109: step 77, loss 0.95743, acc 0.62\n",
      "current_step:  77\n",
      "2017-08-22T05:30:07.786277: step 78, loss 1.30389, acc 0.54\n",
      "current_step:  78\n",
      "2017-08-22T05:30:07.912152: step 79, loss 1.0947, acc 0.62\n",
      "current_step:  79\n",
      "2017-08-22T05:30:08.036537: step 80, loss 0.871015, acc 0.74\n",
      "current_step:  80\n",
      "2017-08-22T05:30:08.163813: step 81, loss 1.28346, acc 0.62\n",
      "current_step:  81\n",
      "2017-08-22T05:30:08.290321: step 82, loss 0.955968, acc 0.7\n",
      "current_step:  82\n",
      "2017-08-22T05:30:08.416120: step 83, loss 1.16163, acc 0.58\n",
      "current_step:  83\n",
      "2017-08-22T05:30:08.541903: step 84, loss 1.01214, acc 0.6\n",
      "current_step:  84\n",
      "2017-08-22T05:30:08.668841: step 85, loss 1.00956, acc 0.62\n",
      "current_step:  85\n",
      "2017-08-22T05:30:08.794308: step 86, loss 1.05248, acc 0.58\n",
      "current_step:  86\n",
      "2017-08-22T05:30:08.921883: step 87, loss 1.06598, acc 0.62\n",
      "current_step:  87\n",
      "2017-08-22T05:30:09.046509: step 88, loss 1.34032, acc 0.54\n",
      "current_step:  88\n",
      "2017-08-22T05:30:09.172624: step 89, loss 1.06957, acc 0.6\n",
      "current_step:  89\n",
      "2017-08-22T05:30:09.299052: step 90, loss 1.14547, acc 0.54\n",
      "current_step:  90\n",
      "2017-08-22T05:30:09.425227: step 91, loss 0.980699, acc 0.68\n",
      "current_step:  91\n",
      "2017-08-22T05:30:09.551442: step 92, loss 1.19516, acc 0.54\n",
      "current_step:  92\n",
      "2017-08-22T05:30:09.676795: step 93, loss 1.35052, acc 0.48\n",
      "current_step:  93\n",
      "2017-08-22T05:30:09.801576: step 94, loss 1.07681, acc 0.62\n",
      "current_step:  94\n",
      "2017-08-22T05:30:09.927429: step 95, loss 1.01553, acc 0.68\n",
      "current_step:  95\n",
      "2017-08-22T05:30:10.052178: step 96, loss 1.53057, acc 0.42\n",
      "current_step:  96\n",
      "2017-08-22T05:30:10.178571: step 97, loss 1.04487, acc 0.72\n",
      "current_step:  97\n",
      "2017-08-22T05:30:10.303836: step 98, loss 1.14209, acc 0.58\n",
      "current_step:  98\n",
      "2017-08-22T05:30:10.429480: step 99, loss 1.19794, acc 0.62\n",
      "current_step:  99\n",
      "2017-08-22T05:30:10.554601: step 100, loss 0.982378, acc 0.64\n",
      "current_step:  100\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:30:11.002010: step 100, loss 1.03836, acc 0.613879\n",
      "\n",
      "2017-08-22T05:30:11.127869: step 101, loss 0.940967, acc 0.66\n",
      "current_step:  101\n",
      "2017-08-22T05:30:11.155930: step 102, loss 1.44251, acc 0.444444\n",
      "current_step:  102\n",
      "2017-08-22T05:30:11.281841: step 103, loss 1.03386, acc 0.6\n",
      "current_step:  103\n",
      "2017-08-22T05:30:11.406841: step 104, loss 1.07658, acc 0.64\n",
      "current_step:  104\n",
      "2017-08-22T05:30:11.532076: step 105, loss 0.974928, acc 0.62\n",
      "current_step:  105\n",
      "2017-08-22T05:30:11.657945: step 106, loss 0.775805, acc 0.78\n",
      "current_step:  106\n",
      "2017-08-22T05:30:11.784146: step 107, loss 0.905148, acc 0.68\n",
      "current_step:  107\n",
      "2017-08-22T05:30:11.910031: step 108, loss 0.856876, acc 0.74\n",
      "current_step:  108\n",
      "2017-08-22T05:30:12.038251: step 109, loss 0.998826, acc 0.58\n",
      "current_step:  109\n",
      "2017-08-22T05:30:12.163389: step 110, loss 0.91772, acc 0.76\n",
      "current_step:  110\n",
      "2017-08-22T05:30:12.288299: step 111, loss 1.05638, acc 0.6\n",
      "current_step:  111\n",
      "2017-08-22T05:30:12.413724: step 112, loss 0.679501, acc 0.78\n",
      "current_step:  112\n",
      "2017-08-22T05:30:12.539789: step 113, loss 0.77671, acc 0.74\n",
      "current_step:  113\n",
      "2017-08-22T05:30:12.666640: step 114, loss 1.09792, acc 0.58\n",
      "current_step:  114\n",
      "2017-08-22T05:30:12.792881: step 115, loss 1.05401, acc 0.58\n",
      "current_step:  115\n",
      "2017-08-22T05:30:12.918763: step 116, loss 1.08197, acc 0.56\n",
      "current_step:  116\n",
      "2017-08-22T05:30:13.043321: step 117, loss 0.923904, acc 0.64\n",
      "current_step:  117\n",
      "2017-08-22T05:30:13.169398: step 118, loss 0.853858, acc 0.62\n",
      "current_step:  118\n",
      "2017-08-22T05:30:13.296089: step 119, loss 0.931849, acc 0.74\n",
      "current_step:  119\n",
      "2017-08-22T05:30:13.422019: step 120, loss 0.960961, acc 0.66\n",
      "current_step:  120\n",
      "2017-08-22T05:30:13.549219: step 121, loss 0.926426, acc 0.74\n",
      "current_step:  121\n",
      "2017-08-22T05:30:13.675716: step 122, loss 0.834936, acc 0.72\n",
      "current_step:  122\n",
      "2017-08-22T05:30:13.802952: step 123, loss 0.994068, acc 0.68\n",
      "current_step:  123\n",
      "2017-08-22T05:30:13.928332: step 124, loss 0.864768, acc 0.72\n",
      "current_step:  124\n",
      "2017-08-22T05:30:14.056283: step 125, loss 0.75243, acc 0.8\n",
      "current_step:  125\n",
      "2017-08-22T05:30:14.180970: step 126, loss 0.905418, acc 0.68\n",
      "current_step:  126\n",
      "2017-08-22T05:30:14.306763: step 127, loss 0.763488, acc 0.7\n",
      "current_step:  127\n",
      "2017-08-22T05:30:14.432793: step 128, loss 0.821619, acc 0.76\n",
      "current_step:  128\n",
      "2017-08-22T05:30:14.558476: step 129, loss 0.887884, acc 0.64\n",
      "current_step:  129\n",
      "2017-08-22T05:30:14.684635: step 130, loss 0.843114, acc 0.66\n",
      "current_step:  130\n",
      "2017-08-22T05:30:14.811593: step 131, loss 0.815379, acc 0.72\n",
      "current_step:  131\n",
      "2017-08-22T05:30:14.936491: step 132, loss 1.01291, acc 0.66\n",
      "current_step:  132\n",
      "2017-08-22T05:30:15.062793: step 133, loss 0.931313, acc 0.7\n",
      "current_step:  133\n",
      "2017-08-22T05:30:15.187968: step 134, loss 0.902257, acc 0.68\n",
      "current_step:  134\n",
      "2017-08-22T05:30:15.315274: step 135, loss 0.917259, acc 0.7\n",
      "current_step:  135\n",
      "2017-08-22T05:30:15.441674: step 136, loss 0.870034, acc 0.64\n",
      "current_step:  136\n",
      "2017-08-22T05:30:15.568459: step 137, loss 0.815366, acc 0.76\n",
      "current_step:  137\n",
      "2017-08-22T05:30:15.694281: step 138, loss 0.751823, acc 0.76\n",
      "current_step:  138\n",
      "2017-08-22T05:30:15.823994: step 139, loss 0.837874, acc 0.72\n",
      "current_step:  139\n",
      "2017-08-22T05:30:15.949056: step 140, loss 0.85338, acc 0.68\n",
      "current_step:  140\n",
      "2017-08-22T05:30:16.075237: step 141, loss 0.964879, acc 0.68\n",
      "current_step:  141\n",
      "2017-08-22T05:30:16.200988: step 142, loss 0.927364, acc 0.68\n",
      "current_step:  142\n",
      "2017-08-22T05:30:16.326506: step 143, loss 0.852697, acc 0.76\n",
      "current_step:  143\n",
      "2017-08-22T05:30:16.451781: step 144, loss 1.05548, acc 0.64\n",
      "current_step:  144\n",
      "2017-08-22T05:30:16.578103: step 145, loss 0.968337, acc 0.72\n",
      "current_step:  145\n",
      "2017-08-22T05:30:16.703784: step 146, loss 0.663522, acc 0.76\n",
      "current_step:  146\n",
      "2017-08-22T05:30:16.830463: step 147, loss 0.906472, acc 0.7\n",
      "current_step:  147\n",
      "2017-08-22T05:30:16.957447: step 148, loss 0.807416, acc 0.76\n",
      "current_step:  148\n",
      "2017-08-22T05:30:17.082577: step 149, loss 0.747291, acc 0.72\n",
      "current_step:  149\n",
      "2017-08-22T05:30:17.207946: step 150, loss 0.792503, acc 0.72\n",
      "current_step:  150\n",
      "2017-08-22T05:30:17.335855: step 151, loss 1.08386, acc 0.58\n",
      "current_step:  151\n",
      "2017-08-22T05:30:17.462364: step 152, loss 0.83886, acc 0.66\n",
      "current_step:  152\n",
      "2017-08-22T05:30:17.588476: step 153, loss 0.78349, acc 0.68\n",
      "current_step:  153\n",
      "2017-08-22T05:30:17.715079: step 154, loss 1.09057, acc 0.66\n",
      "current_step:  154\n",
      "2017-08-22T05:30:17.841444: step 155, loss 0.819809, acc 0.78\n",
      "current_step:  155\n",
      "2017-08-22T05:30:17.965866: step 156, loss 0.906401, acc 0.72\n",
      "current_step:  156\n",
      "2017-08-22T05:30:18.091174: step 157, loss 0.782089, acc 0.74\n",
      "current_step:  157\n",
      "2017-08-22T05:30:18.216987: step 158, loss 0.921805, acc 0.62\n",
      "current_step:  158\n",
      "2017-08-22T05:30:18.342687: step 159, loss 0.931802, acc 0.64\n",
      "current_step:  159\n",
      "2017-08-22T05:30:18.467934: step 160, loss 0.634663, acc 0.8\n",
      "current_step:  160\n",
      "2017-08-22T05:30:18.593522: step 161, loss 0.820982, acc 0.68\n",
      "current_step:  161\n",
      "2017-08-22T05:30:18.719344: step 162, loss 0.73164, acc 0.7\n",
      "current_step:  162\n",
      "2017-08-22T05:30:18.845543: step 163, loss 0.777089, acc 0.72\n",
      "current_step:  163\n",
      "2017-08-22T05:30:18.970201: step 164, loss 0.818356, acc 0.72\n",
      "current_step:  164\n",
      "2017-08-22T05:30:19.094946: step 165, loss 0.700545, acc 0.8\n",
      "current_step:  165\n",
      "2017-08-22T05:30:19.220274: step 166, loss 0.89428, acc 0.64\n",
      "current_step:  166\n",
      "2017-08-22T05:30:19.347676: step 167, loss 1.08037, acc 0.68\n",
      "current_step:  167\n",
      "2017-08-22T05:30:19.473562: step 168, loss 0.938912, acc 0.7\n",
      "current_step:  168\n",
      "2017-08-22T05:30:19.599933: step 169, loss 0.908458, acc 0.7\n",
      "current_step:  169\n",
      "2017-08-22T05:30:19.726505: step 170, loss 0.761633, acc 0.7\n",
      "current_step:  170\n",
      "2017-08-22T05:30:19.852585: step 171, loss 0.924051, acc 0.74\n",
      "current_step:  171\n",
      "2017-08-22T05:30:19.979087: step 172, loss 0.840149, acc 0.66\n",
      "current_step:  172\n",
      "2017-08-22T05:30:20.107259: step 173, loss 0.808228, acc 0.68\n",
      "current_step:  173\n",
      "2017-08-22T05:30:20.234967: step 174, loss 0.653635, acc 0.86\n",
      "current_step:  174\n",
      "2017-08-22T05:30:20.362542: step 175, loss 0.867141, acc 0.66\n",
      "current_step:  175\n",
      "2017-08-22T05:30:20.489012: step 176, loss 0.785205, acc 0.68\n",
      "current_step:  176\n",
      "2017-08-22T05:30:20.615862: step 177, loss 0.885064, acc 0.66\n",
      "current_step:  177\n",
      "2017-08-22T05:30:20.741401: step 178, loss 0.784226, acc 0.76\n",
      "current_step:  178\n",
      "2017-08-22T05:30:20.867088: step 179, loss 0.861887, acc 0.62\n",
      "current_step:  179\n",
      "2017-08-22T05:30:20.992463: step 180, loss 0.739083, acc 0.72\n",
      "current_step:  180\n",
      "2017-08-22T05:30:21.119217: step 181, loss 0.887165, acc 0.74\n",
      "current_step:  181\n",
      "2017-08-22T05:30:21.245818: step 182, loss 0.802041, acc 0.72\n",
      "current_step:  182\n",
      "2017-08-22T05:30:21.371652: step 183, loss 1.08062, acc 0.54\n",
      "current_step:  183\n",
      "2017-08-22T05:30:21.497940: step 184, loss 0.786331, acc 0.74\n",
      "current_step:  184\n",
      "2017-08-22T05:30:21.624868: step 185, loss 0.851891, acc 0.72\n",
      "current_step:  185\n",
      "2017-08-22T05:30:21.751788: step 186, loss 0.791389, acc 0.74\n",
      "current_step:  186\n",
      "2017-08-22T05:30:21.877263: step 187, loss 0.794883, acc 0.72\n",
      "current_step:  187\n",
      "2017-08-22T05:30:22.002442: step 188, loss 1.03413, acc 0.62\n",
      "current_step:  188\n",
      "2017-08-22T05:30:22.129828: step 189, loss 0.778048, acc 0.74\n",
      "current_step:  189\n",
      "2017-08-22T05:30:22.256160: step 190, loss 0.799444, acc 0.66\n",
      "current_step:  190\n",
      "2017-08-22T05:30:22.382791: step 191, loss 0.760216, acc 0.74\n",
      "current_step:  191\n",
      "2017-08-22T05:30:22.508391: step 192, loss 0.828197, acc 0.74\n",
      "current_step:  192\n",
      "2017-08-22T05:30:22.636017: step 193, loss 1.0287, acc 0.62\n",
      "current_step:  193\n",
      "2017-08-22T05:30:22.762881: step 194, loss 0.777871, acc 0.7\n",
      "current_step:  194\n",
      "2017-08-22T05:30:22.889631: step 195, loss 0.953315, acc 0.74\n",
      "current_step:  195\n",
      "2017-08-22T05:30:23.015820: step 196, loss 0.755865, acc 0.76\n",
      "current_step:  196\n",
      "2017-08-22T05:30:23.140644: step 197, loss 0.725248, acc 0.74\n",
      "current_step:  197\n",
      "2017-08-22T05:30:23.266903: step 198, loss 0.873757, acc 0.72\n",
      "current_step:  198\n",
      "2017-08-22T05:30:23.393286: step 199, loss 0.83541, acc 0.68\n",
      "current_step:  199\n",
      "2017-08-22T05:30:23.518865: step 200, loss 0.713033, acc 0.7\n",
      "current_step:  200\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:30:23.960901: step 200, loss 0.975813, acc 0.645907\n",
      "\n",
      "2017-08-22T05:30:24.085908: step 201, loss 0.665717, acc 0.7\n",
      "current_step:  201\n",
      "2017-08-22T05:30:24.212231: step 202, loss 0.858979, acc 0.72\n",
      "current_step:  202\n",
      "2017-08-22T05:30:24.337303: step 203, loss 0.940387, acc 0.62\n",
      "current_step:  203\n",
      "2017-08-22T05:30:24.365873: step 204, loss 0.817767, acc 0.555556\n",
      "current_step:  204\n",
      "2017-08-22T05:30:24.493685: step 205, loss 0.477101, acc 0.82\n",
      "current_step:  205\n",
      "2017-08-22T05:30:24.621253: step 206, loss 0.649941, acc 0.82\n",
      "current_step:  206\n",
      "2017-08-22T05:30:24.746895: step 207, loss 0.748481, acc 0.74\n",
      "current_step:  207\n",
      "2017-08-22T05:30:24.871854: step 208, loss 0.655129, acc 0.8\n",
      "current_step:  208\n",
      "2017-08-22T05:30:24.997191: step 209, loss 0.666471, acc 0.82\n",
      "current_step:  209\n",
      "2017-08-22T05:30:25.122564: step 210, loss 0.690225, acc 0.76\n",
      "current_step:  210\n",
      "2017-08-22T05:30:25.248011: step 211, loss 0.51666, acc 0.82\n",
      "current_step:  211\n",
      "2017-08-22T05:30:25.374154: step 212, loss 0.647306, acc 0.78\n",
      "current_step:  212\n",
      "2017-08-22T05:30:25.498876: step 213, loss 0.779056, acc 0.7\n",
      "current_step:  213\n",
      "2017-08-22T05:30:25.623878: step 214, loss 0.741841, acc 0.76\n",
      "current_step:  214\n",
      "2017-08-22T05:30:25.751944: step 215, loss 0.603204, acc 0.82\n",
      "current_step:  215\n",
      "2017-08-22T05:30:25.877797: step 216, loss 0.489373, acc 0.84\n",
      "current_step:  216\n",
      "2017-08-22T05:30:26.002786: step 217, loss 0.877764, acc 0.74\n",
      "current_step:  217\n",
      "2017-08-22T05:30:26.128312: step 218, loss 0.751917, acc 0.72\n",
      "current_step:  218\n",
      "2017-08-22T05:30:26.255163: step 219, loss 0.857981, acc 0.66\n",
      "current_step:  219\n",
      "2017-08-22T05:30:26.380545: step 220, loss 0.678232, acc 0.74\n",
      "current_step:  220\n",
      "2017-08-22T05:30:26.507809: step 221, loss 0.528986, acc 0.82\n",
      "current_step:  221\n",
      "2017-08-22T05:30:26.633429: step 222, loss 0.691865, acc 0.8\n",
      "current_step:  222\n",
      "2017-08-22T05:30:26.759550: step 223, loss 0.688663, acc 0.76\n",
      "current_step:  223\n",
      "2017-08-22T05:30:26.884849: step 224, loss 0.677641, acc 0.82\n",
      "current_step:  224\n",
      "2017-08-22T05:30:27.010187: step 225, loss 0.837069, acc 0.7\n",
      "current_step:  225\n",
      "2017-08-22T05:30:27.136563: step 226, loss 0.689997, acc 0.76\n",
      "current_step:  226\n",
      "2017-08-22T05:30:27.262767: step 227, loss 0.476986, acc 0.9\n",
      "current_step:  227\n",
      "2017-08-22T05:30:27.387691: step 228, loss 0.713514, acc 0.7\n",
      "current_step:  228\n",
      "2017-08-22T05:30:27.512491: step 229, loss 0.687284, acc 0.7\n",
      "current_step:  229\n",
      "2017-08-22T05:30:27.639787: step 230, loss 0.705181, acc 0.72\n",
      "current_step:  230\n",
      "2017-08-22T05:30:27.766169: step 231, loss 0.512142, acc 0.88\n",
      "current_step:  231\n",
      "2017-08-22T05:30:27.891462: step 232, loss 0.741041, acc 0.72\n",
      "current_step:  232\n",
      "2017-08-22T05:30:28.019123: step 233, loss 0.525806, acc 0.84\n",
      "current_step:  233\n",
      "2017-08-22T05:30:28.145634: step 234, loss 0.953266, acc 0.68\n",
      "current_step:  234\n",
      "2017-08-22T05:30:28.272057: step 235, loss 0.764797, acc 0.74\n",
      "current_step:  235\n",
      "2017-08-22T05:30:28.398158: step 236, loss 0.508223, acc 0.9\n",
      "current_step:  236\n",
      "2017-08-22T05:30:28.524918: step 237, loss 0.958949, acc 0.76\n",
      "current_step:  237\n",
      "2017-08-22T05:30:28.652884: step 238, loss 0.719753, acc 0.8\n",
      "current_step:  238\n",
      "2017-08-22T05:30:28.780845: step 239, loss 0.652671, acc 0.76\n",
      "current_step:  239\n",
      "2017-08-22T05:30:28.907144: step 240, loss 0.768924, acc 0.72\n",
      "current_step:  240\n",
      "2017-08-22T05:30:29.032767: step 241, loss 0.580904, acc 0.8\n",
      "current_step:  241\n",
      "2017-08-22T05:30:29.158898: step 242, loss 0.822866, acc 0.7\n",
      "current_step:  242\n",
      "2017-08-22T05:30:29.284641: step 243, loss 0.788577, acc 0.8\n",
      "current_step:  243\n",
      "2017-08-22T05:30:29.409743: step 244, loss 0.482063, acc 0.84\n",
      "current_step:  244\n",
      "2017-08-22T05:30:29.535863: step 245, loss 0.632753, acc 0.8\n",
      "current_step:  245\n",
      "2017-08-22T05:30:29.661484: step 246, loss 0.648829, acc 0.74\n",
      "current_step:  246\n",
      "2017-08-22T05:30:29.787922: step 247, loss 0.683285, acc 0.76\n",
      "current_step:  247\n",
      "2017-08-22T05:30:29.913632: step 248, loss 0.639837, acc 0.78\n",
      "current_step:  248\n",
      "2017-08-22T05:30:30.039476: step 249, loss 0.690187, acc 0.74\n",
      "current_step:  249\n",
      "2017-08-22T05:30:30.165274: step 250, loss 0.755083, acc 0.74\n",
      "current_step:  250\n",
      "2017-08-22T05:30:30.289925: step 251, loss 0.536837, acc 0.82\n",
      "current_step:  251\n",
      "2017-08-22T05:30:30.415857: step 252, loss 0.584815, acc 0.86\n",
      "current_step:  252\n",
      "2017-08-22T05:30:30.542247: step 253, loss 0.724535, acc 0.76\n",
      "current_step:  253\n",
      "2017-08-22T05:30:30.668123: step 254, loss 0.707106, acc 0.76\n",
      "current_step:  254\n",
      "2017-08-22T05:30:30.794448: step 255, loss 0.744259, acc 0.76\n",
      "current_step:  255\n",
      "2017-08-22T05:30:30.920569: step 256, loss 0.593263, acc 0.8\n",
      "current_step:  256\n",
      "2017-08-22T05:30:31.046693: step 257, loss 0.471385, acc 0.88\n",
      "current_step:  257\n",
      "2017-08-22T05:30:31.173536: step 258, loss 0.588639, acc 0.82\n",
      "current_step:  258\n",
      "2017-08-22T05:30:31.299565: step 259, loss 0.661642, acc 0.74\n",
      "current_step:  259\n",
      "2017-08-22T05:30:31.424860: step 260, loss 0.701775, acc 0.76\n",
      "current_step:  260\n",
      "2017-08-22T05:30:31.551992: step 261, loss 0.757328, acc 0.76\n",
      "current_step:  261\n",
      "2017-08-22T05:30:31.679998: step 262, loss 0.579912, acc 0.78\n",
      "current_step:  262\n",
      "2017-08-22T05:30:31.807451: step 263, loss 0.530073, acc 0.82\n",
      "current_step:  263\n",
      "2017-08-22T05:30:31.943119: step 264, loss 0.736207, acc 0.74\n",
      "current_step:  264\n",
      "2017-08-22T05:30:32.079937: step 265, loss 0.931228, acc 0.64\n",
      "current_step:  265\n",
      "2017-08-22T05:30:32.206673: step 266, loss 0.678832, acc 0.7\n",
      "current_step:  266\n",
      "2017-08-22T05:30:32.332788: step 267, loss 0.628487, acc 0.76\n",
      "current_step:  267\n",
      "2017-08-22T05:30:32.458046: step 268, loss 0.597204, acc 0.84\n",
      "current_step:  268\n",
      "2017-08-22T05:30:32.585076: step 269, loss 0.731962, acc 0.78\n",
      "current_step:  269\n",
      "2017-08-22T05:30:32.710452: step 270, loss 0.704528, acc 0.74\n",
      "current_step:  270\n",
      "2017-08-22T05:30:32.835530: step 271, loss 0.619496, acc 0.76\n",
      "current_step:  271\n",
      "2017-08-22T05:30:32.961163: step 272, loss 0.716967, acc 0.8\n",
      "current_step:  272\n",
      "2017-08-22T05:30:33.087205: step 273, loss 0.55287, acc 0.88\n",
      "current_step:  273\n",
      "2017-08-22T05:30:33.212276: step 274, loss 0.570385, acc 0.86\n",
      "current_step:  274\n",
      "2017-08-22T05:30:33.339024: step 275, loss 0.675665, acc 0.8\n",
      "current_step:  275\n",
      "2017-08-22T05:30:33.464235: step 276, loss 0.657395, acc 0.74\n",
      "current_step:  276\n",
      "2017-08-22T05:30:33.593317: step 277, loss 0.635768, acc 0.76\n",
      "current_step:  277\n",
      "2017-08-22T05:30:33.720322: step 278, loss 0.518328, acc 0.8\n",
      "current_step:  278\n",
      "2017-08-22T05:30:33.847188: step 279, loss 0.665279, acc 0.74\n",
      "current_step:  279\n",
      "2017-08-22T05:30:33.973416: step 280, loss 0.760422, acc 0.7\n",
      "current_step:  280\n",
      "2017-08-22T05:30:34.099237: step 281, loss 0.664302, acc 0.76\n",
      "current_step:  281\n",
      "2017-08-22T05:30:34.225352: step 282, loss 0.729203, acc 0.76\n",
      "current_step:  282\n",
      "2017-08-22T05:30:34.351753: step 283, loss 0.578874, acc 0.82\n",
      "current_step:  283\n",
      "2017-08-22T05:30:34.476507: step 284, loss 0.737264, acc 0.76\n",
      "current_step:  284\n",
      "2017-08-22T05:30:34.602419: step 285, loss 0.715652, acc 0.74\n",
      "current_step:  285\n",
      "2017-08-22T05:30:34.728988: step 286, loss 0.715568, acc 0.86\n",
      "current_step:  286\n",
      "2017-08-22T05:30:34.855217: step 287, loss 0.827631, acc 0.64\n",
      "current_step:  287\n",
      "2017-08-22T05:30:34.979765: step 288, loss 0.864809, acc 0.76\n",
      "current_step:  288\n",
      "2017-08-22T05:30:35.104969: step 289, loss 0.706074, acc 0.82\n",
      "current_step:  289\n",
      "2017-08-22T05:30:35.229936: step 290, loss 0.487687, acc 0.88\n",
      "current_step:  290\n",
      "2017-08-22T05:30:35.356183: step 291, loss 0.747388, acc 0.78\n",
      "current_step:  291\n",
      "2017-08-22T05:30:35.481539: step 292, loss 0.653872, acc 0.74\n",
      "current_step:  292\n",
      "2017-08-22T05:30:35.608680: step 293, loss 0.733101, acc 0.68\n",
      "current_step:  293\n",
      "2017-08-22T05:30:35.734391: step 294, loss 0.760057, acc 0.78\n",
      "current_step:  294\n",
      "2017-08-22T05:30:35.860529: step 295, loss 0.677622, acc 0.8\n",
      "current_step:  295\n",
      "2017-08-22T05:30:35.985549: step 296, loss 0.616687, acc 0.82\n",
      "current_step:  296\n",
      "2017-08-22T05:30:36.110736: step 297, loss 0.619297, acc 0.82\n",
      "current_step:  297\n",
      "2017-08-22T05:30:36.235071: step 298, loss 0.62997, acc 0.8\n",
      "current_step:  298\n",
      "2017-08-22T05:30:36.360765: step 299, loss 0.862279, acc 0.72\n",
      "current_step:  299\n",
      "2017-08-22T05:30:36.485808: step 300, loss 0.672282, acc 0.76\n",
      "current_step:  300\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:30:36.928370: step 300, loss 0.997326, acc 0.654804\n",
      "\n",
      "2017-08-22T05:30:37.052705: step 301, loss 0.853332, acc 0.72\n",
      "current_step:  301\n",
      "2017-08-22T05:30:37.178506: step 302, loss 0.658681, acc 0.76\n",
      "current_step:  302\n",
      "2017-08-22T05:30:37.304300: step 303, loss 0.960282, acc 0.62\n",
      "current_step:  303\n",
      "2017-08-22T05:30:37.430035: step 304, loss 0.620957, acc 0.88\n",
      "current_step:  304\n",
      "2017-08-22T05:30:37.556153: step 305, loss 0.659679, acc 0.78\n",
      "current_step:  305\n",
      "2017-08-22T05:30:37.585085: step 306, loss 0.776399, acc 0.666667\n",
      "current_step:  306\n",
      "2017-08-22T05:30:37.712988: step 307, loss 0.404058, acc 0.98\n",
      "current_step:  307\n",
      "2017-08-22T05:30:37.839448: step 308, loss 0.400548, acc 0.96\n",
      "current_step:  308\n",
      "2017-08-22T05:30:37.965038: step 309, loss 0.565862, acc 0.82\n",
      "current_step:  309\n",
      "2017-08-22T05:30:38.090483: step 310, loss 0.745355, acc 0.74\n",
      "current_step:  310\n",
      "2017-08-22T05:30:38.217875: step 311, loss 0.52768, acc 0.8\n",
      "current_step:  311\n",
      "2017-08-22T05:30:38.342388: step 312, loss 0.514969, acc 0.82\n",
      "current_step:  312\n",
      "2017-08-22T05:30:38.467441: step 313, loss 0.47458, acc 0.88\n",
      "current_step:  313\n",
      "2017-08-22T05:30:38.594598: step 314, loss 0.674286, acc 0.8\n",
      "current_step:  314\n",
      "2017-08-22T05:30:38.721911: step 315, loss 0.586725, acc 0.84\n",
      "current_step:  315\n",
      "2017-08-22T05:30:38.847059: step 316, loss 0.511719, acc 0.84\n",
      "current_step:  316\n",
      "2017-08-22T05:30:38.974082: step 317, loss 0.397067, acc 0.94\n",
      "current_step:  317\n",
      "2017-08-22T05:30:39.099288: step 318, loss 0.469332, acc 0.88\n",
      "current_step:  318\n",
      "2017-08-22T05:30:39.225292: step 319, loss 0.497511, acc 0.86\n",
      "current_step:  319\n",
      "2017-08-22T05:30:39.351795: step 320, loss 0.49735, acc 0.88\n",
      "current_step:  320\n",
      "2017-08-22T05:30:39.478282: step 321, loss 0.543284, acc 0.8\n",
      "current_step:  321\n",
      "2017-08-22T05:30:39.604403: step 322, loss 0.484474, acc 0.8\n",
      "current_step:  322\n",
      "2017-08-22T05:30:39.730583: step 323, loss 0.529217, acc 0.82\n",
      "current_step:  323\n",
      "2017-08-22T05:30:39.855677: step 324, loss 0.455333, acc 0.84\n",
      "current_step:  324\n",
      "2017-08-22T05:30:39.980827: step 325, loss 0.628128, acc 0.78\n",
      "current_step:  325\n",
      "2017-08-22T05:30:40.106446: step 326, loss 0.815913, acc 0.66\n",
      "current_step:  326\n",
      "2017-08-22T05:30:40.233433: step 327, loss 0.485263, acc 0.88\n",
      "current_step:  327\n",
      "2017-08-22T05:30:40.358281: step 328, loss 0.439211, acc 0.8\n",
      "current_step:  328\n",
      "2017-08-22T05:30:40.484321: step 329, loss 0.588602, acc 0.82\n",
      "current_step:  329\n",
      "2017-08-22T05:30:40.609667: step 330, loss 0.722008, acc 0.68\n",
      "current_step:  330\n",
      "2017-08-22T05:30:40.735621: step 331, loss 0.567235, acc 0.8\n",
      "current_step:  331\n",
      "2017-08-22T05:30:40.862156: step 332, loss 0.678844, acc 0.82\n",
      "current_step:  332\n",
      "2017-08-22T05:30:40.987851: step 333, loss 0.504149, acc 0.88\n",
      "current_step:  333\n",
      "2017-08-22T05:30:41.113866: step 334, loss 0.422966, acc 0.9\n",
      "current_step:  334\n",
      "2017-08-22T05:30:41.239653: step 335, loss 0.423287, acc 0.92\n",
      "current_step:  335\n",
      "2017-08-22T05:30:41.365275: step 336, loss 0.473159, acc 0.9\n",
      "current_step:  336\n",
      "2017-08-22T05:30:41.490367: step 337, loss 0.502756, acc 0.8\n",
      "current_step:  337\n",
      "2017-08-22T05:30:41.616082: step 338, loss 0.43067, acc 0.88\n",
      "current_step:  338\n",
      "2017-08-22T05:30:41.742989: step 339, loss 0.507818, acc 0.84\n",
      "current_step:  339\n",
      "2017-08-22T05:30:41.869109: step 340, loss 0.71388, acc 0.76\n",
      "current_step:  340\n",
      "2017-08-22T05:30:41.996031: step 341, loss 0.519757, acc 0.86\n",
      "current_step:  341\n",
      "2017-08-22T05:30:42.121943: step 342, loss 0.480956, acc 0.82\n",
      "current_step:  342\n",
      "2017-08-22T05:30:42.248059: step 343, loss 0.697476, acc 0.76\n",
      "current_step:  343\n",
      "2017-08-22T05:30:42.373629: step 344, loss 0.560858, acc 0.86\n",
      "current_step:  344\n",
      "2017-08-22T05:30:42.499059: step 345, loss 0.361497, acc 0.92\n",
      "current_step:  345\n",
      "2017-08-22T05:30:42.624740: step 346, loss 0.472951, acc 0.9\n",
      "current_step:  346\n",
      "2017-08-22T05:30:42.751787: step 347, loss 0.697854, acc 0.82\n",
      "current_step:  347\n",
      "2017-08-22T05:30:42.876744: step 348, loss 0.485386, acc 0.9\n",
      "current_step:  348\n",
      "2017-08-22T05:30:43.002386: step 349, loss 0.529495, acc 0.9\n",
      "current_step:  349\n",
      "2017-08-22T05:30:43.127480: step 350, loss 0.564308, acc 0.86\n",
      "current_step:  350\n",
      "2017-08-22T05:30:43.253005: step 351, loss 0.510595, acc 0.84\n",
      "current_step:  351\n",
      "2017-08-22T05:30:43.379058: step 352, loss 0.647091, acc 0.76\n",
      "current_step:  352\n",
      "2017-08-22T05:30:43.504716: step 353, loss 0.387759, acc 0.9\n",
      "current_step:  353\n",
      "2017-08-22T05:30:43.630833: step 354, loss 0.531996, acc 0.9\n",
      "current_step:  354\n",
      "2017-08-22T05:30:43.758046: step 355, loss 0.512313, acc 0.8\n",
      "current_step:  355\n",
      "2017-08-22T05:30:43.882998: step 356, loss 0.498245, acc 0.82\n",
      "current_step:  356\n",
      "2017-08-22T05:30:44.008593: step 357, loss 0.478148, acc 0.82\n",
      "current_step:  357\n",
      "2017-08-22T05:30:44.132974: step 358, loss 0.422118, acc 0.86\n",
      "current_step:  358\n",
      "2017-08-22T05:30:44.259411: step 359, loss 0.461533, acc 0.82\n",
      "current_step:  359\n",
      "2017-08-22T05:30:44.385283: step 360, loss 0.560026, acc 0.8\n",
      "current_step:  360\n",
      "2017-08-22T05:30:44.511186: step 361, loss 0.471457, acc 0.84\n",
      "current_step:  361\n",
      "2017-08-22T05:30:44.638035: step 362, loss 0.371453, acc 0.94\n",
      "current_step:  362\n",
      "2017-08-22T05:30:44.765288: step 363, loss 0.440771, acc 0.86\n",
      "current_step:  363\n",
      "2017-08-22T05:30:44.891267: step 364, loss 0.574773, acc 0.88\n",
      "current_step:  364\n",
      "2017-08-22T05:30:45.017807: step 365, loss 0.486444, acc 0.84\n",
      "current_step:  365\n",
      "2017-08-22T05:30:45.143331: step 366, loss 0.411693, acc 0.94\n",
      "current_step:  366\n",
      "2017-08-22T05:30:45.269076: step 367, loss 0.669636, acc 0.82\n",
      "current_step:  367\n",
      "2017-08-22T05:30:45.395889: step 368, loss 0.510073, acc 0.9\n",
      "current_step:  368\n",
      "2017-08-22T05:30:45.522615: step 369, loss 0.474892, acc 0.86\n",
      "current_step:  369\n",
      "2017-08-22T05:30:45.649748: step 370, loss 0.599106, acc 0.82\n",
      "current_step:  370\n",
      "2017-08-22T05:30:45.777941: step 371, loss 0.55713, acc 0.86\n",
      "current_step:  371\n",
      "2017-08-22T05:30:45.903826: step 372, loss 0.435348, acc 0.86\n",
      "current_step:  372\n",
      "2017-08-22T05:30:46.029346: step 373, loss 0.561321, acc 0.84\n",
      "current_step:  373\n",
      "2017-08-22T05:30:46.155770: step 374, loss 0.593467, acc 0.74\n",
      "current_step:  374\n",
      "2017-08-22T05:30:46.282471: step 375, loss 0.566473, acc 0.82\n",
      "current_step:  375\n",
      "2017-08-22T05:30:46.408802: step 376, loss 0.501613, acc 0.86\n",
      "current_step:  376\n",
      "2017-08-22T05:30:46.534363: step 377, loss 0.479863, acc 0.86\n",
      "current_step:  377\n",
      "2017-08-22T05:30:46.660113: step 378, loss 0.534463, acc 0.84\n",
      "current_step:  378\n",
      "2017-08-22T05:30:46.785800: step 379, loss 0.517461, acc 0.84\n",
      "current_step:  379\n",
      "2017-08-22T05:30:46.910485: step 380, loss 0.466054, acc 0.88\n",
      "current_step:  380\n",
      "2017-08-22T05:30:47.035335: step 381, loss 0.68704, acc 0.7\n",
      "current_step:  381\n",
      "2017-08-22T05:30:47.161768: step 382, loss 0.378148, acc 0.94\n",
      "current_step:  382\n",
      "2017-08-22T05:30:47.286766: step 383, loss 0.587391, acc 0.78\n",
      "current_step:  383\n",
      "2017-08-22T05:30:47.412289: step 384, loss 0.462902, acc 0.84\n",
      "current_step:  384\n",
      "2017-08-22T05:30:47.537907: step 385, loss 0.590795, acc 0.8\n",
      "current_step:  385\n",
      "2017-08-22T05:30:47.665602: step 386, loss 0.555222, acc 0.78\n",
      "current_step:  386\n",
      "2017-08-22T05:30:47.792541: step 387, loss 0.508676, acc 0.82\n",
      "current_step:  387\n",
      "2017-08-22T05:30:47.917581: step 388, loss 0.406811, acc 0.84\n",
      "current_step:  388\n",
      "2017-08-22T05:30:48.043307: step 389, loss 0.699582, acc 0.78\n",
      "current_step:  389\n",
      "2017-08-22T05:30:48.168933: step 390, loss 0.633856, acc 0.8\n",
      "current_step:  390\n",
      "2017-08-22T05:30:48.295524: step 391, loss 0.5702, acc 0.86\n",
      "current_step:  391\n",
      "2017-08-22T05:30:48.421561: step 392, loss 0.720885, acc 0.76\n",
      "current_step:  392\n",
      "2017-08-22T05:30:48.548221: step 393, loss 0.645585, acc 0.86\n",
      "current_step:  393\n",
      "2017-08-22T05:30:48.675903: step 394, loss 0.553471, acc 0.84\n",
      "current_step:  394\n",
      "2017-08-22T05:30:48.803850: step 395, loss 0.50304, acc 0.84\n",
      "current_step:  395\n",
      "2017-08-22T05:30:48.929240: step 396, loss 0.625811, acc 0.78\n",
      "current_step:  396\n",
      "2017-08-22T05:30:49.054828: step 397, loss 0.491038, acc 0.86\n",
      "current_step:  397\n",
      "2017-08-22T05:30:49.180697: step 398, loss 0.515694, acc 0.86\n",
      "current_step:  398\n",
      "2017-08-22T05:30:49.308348: step 399, loss 0.51204, acc 0.8\n",
      "current_step:  399\n",
      "2017-08-22T05:30:49.436153: step 400, loss 0.588609, acc 0.8\n",
      "current_step:  400\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:30:49.881324: step 400, loss 0.923188, acc 0.672598\n",
      "\n",
      "2017-08-22T05:30:50.007039: step 401, loss 0.591709, acc 0.8\n",
      "current_step:  401\n",
      "2017-08-22T05:30:50.132806: step 402, loss 0.412302, acc 0.92\n",
      "current_step:  402\n",
      "2017-08-22T05:30:50.258554: step 403, loss 0.555098, acc 0.78\n",
      "current_step:  403\n",
      "2017-08-22T05:30:50.384274: step 404, loss 0.601494, acc 0.8\n",
      "current_step:  404\n",
      "2017-08-22T05:30:50.509061: step 405, loss 0.700811, acc 0.74\n",
      "current_step:  405\n",
      "2017-08-22T05:30:50.636543: step 406, loss 0.621517, acc 0.76\n",
      "current_step:  406\n",
      "2017-08-22T05:30:50.762349: step 407, loss 0.432376, acc 0.9\n",
      "current_step:  407\n",
      "2017-08-22T05:30:50.789784: step 408, loss 0.492952, acc 0.777778\n",
      "current_step:  408\n",
      "2017-08-22T05:30:50.916756: step 409, loss 0.346031, acc 0.96\n",
      "current_step:  409\n",
      "2017-08-22T05:30:51.041135: step 410, loss 0.399702, acc 0.9\n",
      "current_step:  410\n",
      "2017-08-22T05:30:51.166512: step 411, loss 0.459489, acc 0.9\n",
      "current_step:  411\n",
      "2017-08-22T05:30:51.292828: step 412, loss 0.397017, acc 0.92\n",
      "current_step:  412\n",
      "2017-08-22T05:30:51.419331: step 413, loss 0.377401, acc 0.96\n",
      "current_step:  413\n",
      "2017-08-22T05:30:51.544174: step 414, loss 0.582172, acc 0.86\n",
      "current_step:  414\n",
      "2017-08-22T05:30:51.672251: step 415, loss 0.350517, acc 0.96\n",
      "current_step:  415\n",
      "2017-08-22T05:30:51.798176: step 416, loss 0.376213, acc 0.88\n",
      "current_step:  416\n",
      "2017-08-22T05:30:51.924509: step 417, loss 0.546627, acc 0.88\n",
      "current_step:  417\n",
      "2017-08-22T05:30:52.050889: step 418, loss 0.421418, acc 0.9\n",
      "current_step:  418\n",
      "2017-08-22T05:30:52.177650: step 419, loss 0.464884, acc 0.88\n",
      "current_step:  419\n",
      "2017-08-22T05:30:52.302704: step 420, loss 0.324651, acc 0.92\n",
      "current_step:  420\n",
      "2017-08-22T05:30:52.427413: step 421, loss 0.360415, acc 0.96\n",
      "current_step:  421\n",
      "2017-08-22T05:30:52.553197: step 422, loss 0.37655, acc 0.94\n",
      "current_step:  422\n",
      "2017-08-22T05:30:52.680635: step 423, loss 0.447444, acc 0.88\n",
      "current_step:  423\n",
      "2017-08-22T05:30:52.806470: step 424, loss 0.511101, acc 0.86\n",
      "current_step:  424\n",
      "2017-08-22T05:30:52.931586: step 425, loss 0.434032, acc 0.88\n",
      "current_step:  425\n",
      "2017-08-22T05:30:53.057143: step 426, loss 0.326228, acc 0.96\n",
      "current_step:  426\n",
      "2017-08-22T05:30:53.182329: step 427, loss 0.328527, acc 0.9\n",
      "current_step:  427\n",
      "2017-08-22T05:30:53.307812: step 428, loss 0.261561, acc 0.94\n",
      "current_step:  428\n",
      "2017-08-22T05:30:53.433319: step 429, loss 0.378361, acc 0.88\n",
      "current_step:  429\n",
      "2017-08-22T05:30:53.558519: step 430, loss 0.334408, acc 0.88\n",
      "current_step:  430\n",
      "2017-08-22T05:30:53.686327: step 431, loss 0.339645, acc 0.9\n",
      "current_step:  431\n",
      "2017-08-22T05:30:53.812793: step 432, loss 0.333946, acc 0.88\n",
      "current_step:  432\n",
      "2017-08-22T05:30:53.937573: step 433, loss 0.211809, acc 0.94\n",
      "current_step:  433\n",
      "2017-08-22T05:30:54.061812: step 434, loss 0.520278, acc 0.84\n",
      "current_step:  434\n",
      "2017-08-22T05:30:54.187076: step 435, loss 0.442307, acc 0.88\n",
      "current_step:  435\n",
      "2017-08-22T05:30:54.313089: step 436, loss 0.531242, acc 0.86\n",
      "current_step:  436\n",
      "2017-08-22T05:30:54.439306: step 437, loss 0.392325, acc 0.9\n",
      "current_step:  437\n",
      "2017-08-22T05:30:54.568207: step 438, loss 0.509255, acc 0.86\n",
      "current_step:  438\n",
      "2017-08-22T05:30:54.697663: step 439, loss 0.411993, acc 0.92\n",
      "current_step:  439\n",
      "2017-08-22T05:30:54.825164: step 440, loss 0.462393, acc 0.9\n",
      "current_step:  440\n",
      "2017-08-22T05:30:54.951052: step 441, loss 0.459215, acc 0.84\n",
      "current_step:  441\n",
      "2017-08-22T05:30:55.077322: step 442, loss 0.329816, acc 0.94\n",
      "current_step:  442\n",
      "2017-08-22T05:30:55.202864: step 443, loss 0.33057, acc 0.96\n",
      "current_step:  443\n",
      "2017-08-22T05:30:55.329740: step 444, loss 0.350507, acc 0.92\n",
      "current_step:  444\n",
      "2017-08-22T05:30:55.455515: step 445, loss 0.443023, acc 0.9\n",
      "current_step:  445\n",
      "2017-08-22T05:30:55.581298: step 446, loss 0.51159, acc 0.86\n",
      "current_step:  446\n",
      "2017-08-22T05:30:55.709158: step 447, loss 0.457508, acc 0.88\n",
      "current_step:  447\n",
      "2017-08-22T05:30:55.835915: step 448, loss 0.418129, acc 0.92\n",
      "current_step:  448\n",
      "2017-08-22T05:30:55.962985: step 449, loss 0.402799, acc 0.9\n",
      "current_step:  449\n",
      "2017-08-22T05:30:56.087933: step 450, loss 0.451375, acc 0.86\n",
      "current_step:  450\n",
      "2017-08-22T05:30:56.212673: step 451, loss 0.456933, acc 0.9\n",
      "current_step:  451\n",
      "2017-08-22T05:30:56.337838: step 452, loss 0.490086, acc 0.86\n",
      "current_step:  452\n",
      "2017-08-22T05:30:56.463237: step 453, loss 0.559669, acc 0.8\n",
      "current_step:  453\n",
      "2017-08-22T05:30:56.590028: step 454, loss 0.253033, acc 0.98\n",
      "current_step:  454\n",
      "2017-08-22T05:30:56.717077: step 455, loss 0.350571, acc 0.94\n",
      "current_step:  455\n",
      "2017-08-22T05:30:56.843251: step 456, loss 0.437511, acc 0.92\n",
      "current_step:  456\n",
      "2017-08-22T05:30:56.969641: step 457, loss 0.411873, acc 0.92\n",
      "current_step:  457\n",
      "2017-08-22T05:30:57.095895: step 458, loss 0.442523, acc 0.86\n",
      "current_step:  458\n",
      "2017-08-22T05:30:57.221321: step 459, loss 0.399375, acc 0.9\n",
      "current_step:  459\n",
      "2017-08-22T05:30:57.348190: step 460, loss 0.347718, acc 0.94\n",
      "current_step:  460\n",
      "2017-08-22T05:30:57.474804: step 461, loss 0.34466, acc 0.96\n",
      "current_step:  461\n",
      "2017-08-22T05:30:57.600873: step 462, loss 0.35254, acc 0.92\n",
      "current_step:  462\n",
      "2017-08-22T05:30:57.729135: step 463, loss 0.403235, acc 0.88\n",
      "current_step:  463\n",
      "2017-08-22T05:30:57.854580: step 464, loss 0.497976, acc 0.86\n",
      "current_step:  464\n",
      "2017-08-22T05:30:57.980163: step 465, loss 0.375423, acc 0.9\n",
      "current_step:  465\n",
      "2017-08-22T05:30:58.105788: step 466, loss 0.407235, acc 0.94\n",
      "current_step:  466\n",
      "2017-08-22T05:30:58.231656: step 467, loss 0.288922, acc 1\n",
      "current_step:  467\n",
      "2017-08-22T05:30:58.356638: step 468, loss 0.373896, acc 0.92\n",
      "current_step:  468\n",
      "2017-08-22T05:30:58.483668: step 469, loss 0.362061, acc 0.92\n",
      "current_step:  469\n",
      "2017-08-22T05:30:58.611156: step 470, loss 0.408031, acc 0.88\n",
      "current_step:  470\n",
      "2017-08-22T05:30:58.738186: step 471, loss 0.253213, acc 0.98\n",
      "current_step:  471\n",
      "2017-08-22T05:30:58.863694: step 472, loss 0.314318, acc 0.96\n",
      "current_step:  472\n",
      "2017-08-22T05:30:58.988334: step 473, loss 0.375182, acc 0.88\n",
      "current_step:  473\n",
      "2017-08-22T05:30:59.114031: step 474, loss 0.39556, acc 0.88\n",
      "current_step:  474\n",
      "2017-08-22T05:30:59.240132: step 475, loss 0.401944, acc 0.84\n",
      "current_step:  475\n",
      "2017-08-22T05:30:59.364873: step 476, loss 0.315299, acc 0.92\n",
      "current_step:  476\n",
      "2017-08-22T05:30:59.490439: step 477, loss 0.601416, acc 0.76\n",
      "current_step:  477\n",
      "2017-08-22T05:30:59.616394: step 478, loss 0.37631, acc 0.92\n",
      "current_step:  478\n",
      "2017-08-22T05:30:59.743425: step 479, loss 0.552185, acc 0.82\n",
      "current_step:  479\n",
      "2017-08-22T05:30:59.870176: step 480, loss 0.365486, acc 0.94\n",
      "current_step:  480\n",
      "2017-08-22T05:30:59.995615: step 481, loss 0.317582, acc 0.92\n",
      "current_step:  481\n",
      "2017-08-22T05:31:00.121510: step 482, loss 0.577057, acc 0.8\n",
      "current_step:  482\n",
      "2017-08-22T05:31:00.247986: step 483, loss 0.419247, acc 0.92\n",
      "current_step:  483\n",
      "2017-08-22T05:31:00.374342: step 484, loss 0.420672, acc 0.88\n",
      "current_step:  484\n",
      "2017-08-22T05:31:00.502050: step 485, loss 0.482701, acc 0.84\n",
      "current_step:  485\n",
      "2017-08-22T05:31:00.629465: step 486, loss 0.410026, acc 0.86\n",
      "current_step:  486\n",
      "2017-08-22T05:31:00.757326: step 487, loss 0.471216, acc 0.84\n",
      "current_step:  487\n",
      "2017-08-22T05:31:00.883706: step 488, loss 0.377445, acc 0.9\n",
      "current_step:  488\n",
      "2017-08-22T05:31:01.010275: step 489, loss 0.353518, acc 0.92\n",
      "current_step:  489\n",
      "2017-08-22T05:31:01.135123: step 490, loss 0.295939, acc 0.94\n",
      "current_step:  490\n",
      "2017-08-22T05:31:01.262425: step 491, loss 0.491689, acc 0.8\n",
      "current_step:  491\n",
      "2017-08-22T05:31:01.388122: step 492, loss 0.30565, acc 0.94\n",
      "current_step:  492\n",
      "2017-08-22T05:31:01.514425: step 493, loss 0.3966, acc 0.9\n",
      "current_step:  493\n",
      "2017-08-22T05:31:01.640953: step 494, loss 0.506795, acc 0.8\n",
      "current_step:  494\n",
      "2017-08-22T05:31:01.767387: step 495, loss 0.461963, acc 0.9\n",
      "current_step:  495\n",
      "2017-08-22T05:31:01.893077: step 496, loss 0.531866, acc 0.86\n",
      "current_step:  496\n",
      "2017-08-22T05:31:02.018508: step 497, loss 0.528345, acc 0.8\n",
      "current_step:  497\n",
      "2017-08-22T05:31:02.144337: step 498, loss 0.480802, acc 0.82\n",
      "current_step:  498\n",
      "2017-08-22T05:31:02.270705: step 499, loss 0.620991, acc 0.78\n",
      "current_step:  499\n",
      "2017-08-22T05:31:02.395753: step 500, loss 0.324131, acc 0.94\n",
      "current_step:  500\n",
      "\n",
      "Predicting annotation for test data:\n",
      "2017-08-22T05:31:02.837092: step 500, loss 0.937919, acc 0.679715\n",
      "\n",
      "Saved model checkpoint to /home/vslchu/w266/project/code/runs/20170822_0529_UTC/checkpoints/model-500\n",
      "\n",
      "2017-08-22T05:31:03.033658: step 501, loss 0.321197, acc 0.96\n",
      "current_step:  501\n",
      "2017-08-22T05:31:03.158996: step 502, loss 0.375722, acc 0.92\n",
      "current_step:  502\n",
      "2017-08-22T05:31:03.284669: step 503, loss 0.32491, acc 0.94\n",
      "current_step:  503\n",
      "2017-08-22T05:31:03.410706: step 504, loss 0.262529, acc 0.94\n",
      "current_step:  504\n",
      "2017-08-22T05:31:03.536335: step 505, loss 0.358598, acc 0.92\n",
      "current_step:  505\n",
      "2017-08-22T05:31:03.662624: step 506, loss 0.456396, acc 0.88\n",
      "current_step:  506\n",
      "2017-08-22T05:31:03.789889: step 507, loss 0.517348, acc 0.8\n",
      "current_step:  507\n",
      "2017-08-22T05:31:03.914515: step 508, loss 0.413327, acc 0.86\n",
      "current_step:  508\n",
      "2017-08-22T05:31:04.041347: step 509, loss 0.498811, acc 0.82\n",
      "current_step:  509\n",
      "2017-08-22T05:31:04.069969: step 510, loss 0.40475, acc 0.888889\n",
      "current_step:  510\n",
      "\n",
      "Ran 510 batches during training and created 5 rounds of predictions\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 0):\n",
      "F1 Score = 0.548070\n",
      "Precision Score = 0.548366\n",
      "Recall Score = 0.601423\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 1):\n",
      "F1 Score = 0.583467\n",
      "Precision Score = 0.629873\n",
      "Recall Score = 0.631673\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 2):\n",
      "F1 Score = 0.627387\n",
      "Precision Score = 0.616502\n",
      "Recall Score = 0.644128\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 3):\n",
      "F1 Score = 0.627673\n",
      "Precision Score = 0.619534\n",
      "Recall Score = 0.661922\n",
      "\n",
      "Performance Evaluation of CNN Model (i = 4):\n",
      "F1 Score = 0.651030\n",
      "Precision Score = 0.640877\n",
      "Recall Score = 0.672598\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Word-level Data Processor v3 with stopwords but without non-alpha words\n",
    "############################################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_orig_train, y_orig_test, vocab_processor = \\\n",
    "    load_text_data(params.data_dir, 3, remove_non_alpha = True)\n",
    "test_preds = run_cnn(x_train, y_train, x_test, y_test, vocab_processor)\n",
    "test_eval = eval_preds(test_preds, y_orig_test)\n",
    "\n",
    "x_train = None\n",
    "x_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_orig_train = None\n",
    "y_orig_test = None\n",
    "vocab_processor = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
